{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# YOLOX检测案例实现\n",
    "## 1.算法原理介绍\n",
    "YOLOX是旷视科技在2021年提出的目标检测算法，在YOLO系列的基础上进行了经验性改进和提升，主要的改进体现在三个方面：decoupled head、anchor-free和advanced label assigning strategy（SimOTA）。\n",
    "\n",
    "论文：YOLOX: Exceeding YOLO Series in 2021\n",
    "\n",
    "论文地址：https://arxiv.org/abs/2107.08430\n",
    "\n",
    "YOLOX的整体网络结构以YOLOv3+Darknet作为基线搭建，整体架构如下图所示，主要包括三个部分：CSPDarknet、FPN和Yolo Head。\n",
    "<!-- <img src=\"attachment:image-2.png\" width=\"80%\"> -->\n",
    "<img src=\"./images/1.png\" width=\"80%\">\n",
    "<!-- ![image-2.png](attachment:image-2.png) -->\n",
    "\n",
    "CSPDarknet是YoloX的主干特征提取网络，输入的图片首先在CSPDarknet里面进行特征提取，提取到的特征可以称为特征层，是输入图片的特征集合。在主干部分，获取了三个特征层便于进行下一步网络的构建，这三个特征层可以称为有效特征层。\n",
    "\n",
    "FPN是YoloX的加强特征提取网络，在主干部分获得的三个有效特征层会在这一部分进行特征融合，特征融合的目的是结合不同尺度的特征信息。在FPN部分，已经获得的有效特征层被用于继续提取特征。在YoloX里面同样使用了YoloV4中用到的Panet的结构，不仅会对特征进行上采样实现特征融合，还会对特征再次进行下采样实现特征融合。\n",
    "\n",
    "Yolo Head是YoloX的分类器与回归器，通过CSPDarknet和FPN已经可以获得了三个加强过的有效特征层。每一个特征层都有宽、高和通道数，此时可以将特征图看作一个又一个特征点的集合，每一个特征点都有通道数个特征。Yolo Head实际上所做的工作就是对特征点进行判断，判断特征点是否有物体与其对应。以前版本的Yolo所用的解耦头是一起的，也就是分类和回归在一个1×1卷积里实现，YoloX认为这给网络的识别带来了不利影响。在YoloX中，Yolo Head被分为了两部分，分别实现，最后预测的时候整合在一起。\n",
    "\n",
    "因此，整个YoloX网络所作的工作就是**特征提取-特征加强-预测特征点对应的物体情况**。\n",
    "\n",
    "\n",
    "### 1.1 主干部分\n",
    "YOLOX的主干特征提取网络为CSPDarknet，有以下几个特点：\n",
    "\n",
    "1.使用了**残差网络Residual**，CSPDarknet中的残差卷积可以分为两个部分，主干部分是一次1×1的卷积和一次3×3的卷积，残差边部分不做任何处理，直接将主干的输入与输出结合。整个YoloX的主干部分都由残差卷积构成，残差网络的特点是容易优化，并且能够通过增加相当的深度来提高准确率。其内部的残差块使用了跳跃连接，缓解了在深度神经网络中增加深度带来的梯度消失问题。\n",
    "\n",
    "2.使用**CSPnet网络结构**，CSPnet结构并不算复杂，就是将原来的残差块的堆叠进行了一个拆分，拆成左右两部分：主干部分继续进行原来的残差块的堆叠，另一部分则像一个残差边一样，经过少量处理直接连接到最后。因此可以认为CSP中存在一个大的残差边。\n",
    "\n",
    "3.使用了**Focus网络结构**，这个网络结构是在YoloV5里面使用到比较有趣的网络结构，具体操作是在一张图片中每隔一个像素拿到一个值，这时获得了四个独立的特征层，然后将四个独立的特征层进行堆叠，此时宽高信息就集中到了通道信息，输入通道扩充了四倍。Focus结构如下图所示，拼接起来的特征层相对于原先的三通道变成了十二个通道。\n",
    "<!-- ![image-3.png](attachment:image-3.png) -->\n",
    "<!-- <img src=\"attachment:image-5.png\" width=\"30%\"> -->\n",
    "<img src=\"./images/1.1.3(1).png\" width=\"30%\">\n",
    "<img src=\"./images/1.1.3(2).png\" width=\"30%\">\n",
    "\n",
    "4.使用了**SiLU激活函数**，SiLU是Sigmoid和ReLU的改进版。SiLU具备无上界有下界、平滑、非单调的特性。SiLU在深层模型上的效果优于 ReLU，可以看作是平滑的ReLU激活函数。\n",
    "\n",
    "$$f(x) = x·sigmoid(x)$$\n",
    "\n",
    "5.使用了**SPP结构**，通过不同池化核大小的最大池化进行特征提取，提高网络的感受野。在YoloV4中，SPP用在FPN里，而在YoloX中，SPP模块被用在了主干特征提取网络中。\n",
    "<!-- <img src=\"attachment:image-6.png\" width=\"40%\"> -->\n",
    "<img src=\"./images/1.1.5.png\" width=\"30%\">\n",
    "\n",
    "### 1.2 构建FPN特征金字塔\n",
    "在特征提取部分，YoloX提取多特征层进行目标检测，一共提取三个特征层。\n",
    "三个特征层位于主干部分CSPdarknet的不同位置，分别位于中间层，中下层，底层，当输入图片的尺寸为640×640×3时，三个特征层的shape分别为feat1 = 80×80×256、feat2 = 40×40×512、feat3 = 20×20×1024。\n",
    "\n",
    "在获得三个有效特征层之后，进行FPN层的构建，构建方式如下：\n",
    "\n",
    "1.feat3 = 20×20×1024特征层进行一次1×1卷积后获得P5，对P5执行上采样操作后与feat2 = 40×40×512特征层进行结合，然后使用CSPLayer进行特征提取获得P5_upsample，此时得到的特征层尺寸为40×40×512。\n",
    "\n",
    "2.P5_upsample = 40×40×512的特征层进行一次1×1卷积后获得P4，对P4执行上采样操作后与feat1 = 80×80×256特征层结合，然后使用CSPLayer进行特征提取获得P3_out，此时得到的特征层尺寸为80×80×256。\n",
    "\n",
    "3.P3_out = 80×80×256特征层进行一次3×3卷积后，执行下采样操作并与P4堆叠，然后使用CSPLayer进行特征提取获得P4_out，此时得到的特征层尺寸为40×40×512。\n",
    "\n",
    "4.P4_out = 40×40×512特征层进行一次3×3卷积后，执行下采样操作并与P5堆叠，然后使用CSPLayer进行特征提取获得P5_out，此时得到的特征层尺寸为20×20×1024。\n",
    "\n",
    "特征金字塔可以将**不同shape的特征层进行特征融合**，有利于提取出更好的特征。\n",
    "\n",
    "\n",
    "### 1.3 利用Yolo Head获得预测结果\n",
    "通过FPN特征金字塔，获得了三个加强特征，其shape分别为20×20×1024、40×40×512、80×80×256，将这些加强特征层传入Yolo Head获得预测结果。\n",
    "\n",
    "YoloX中的Yolo Head与之前版本的Yolo Head不同，以前版本的Yolo所用的解耦头是一起的，即分类和回归在一个1×1卷积里实现，YoloX认为这给网络的识别带来了不利影响。于是在YoloX中，Yolo Head被分为了两部分分别实现，最后预测的时候才整合在一起，如下图所示。\n",
    "<!-- <img src=\"attachment:image-4.png\" width=\"30%\"> -->\n",
    "<img src=\"./images/1.3.png\" width=\"50%\">\n",
    "<!-- ![image-4.png](attachment:image-4.png) -->\n",
    "\n",
    "对于每一个特征层，可以获得三个预测结果，分别是：\n",
    "\n",
    "1）Reg(H×W×4)用于判断每一个特征点的回归参数，对回归参数进行调整后可以获得预测框；\n",
    "\n",
    "2）Obj(H×W×1)用于判断每一个特征点是否包含物体；\n",
    "\n",
    "3）Cls(H×W×C)用于判断每一个特征点所包含物体的种类（C表示类别数）。\n",
    "\n",
    "将三个预测结果进行堆叠，每个特征层获得的结果为Out(H×W×(4+1+C))，前四个参数用于判断每一个特征点的回归参数，回归参数调整后可以获得预测框。第五个参数用于判断每一个特征点是否包含物体，最后C个参数用于判断每一个特征点所包含的物体种类。\n",
    "\n",
    "\n",
    "## 2.数据集\n",
    "本案例使用的数据集是**COCO2017**，COCO的全称是Common Objects in Context，是微软团队提供的可以用来进行图像识别的数据集。目前最常用于图像检测定位，是一个新的图像识别、分割、和字幕数据集，其对于图像的标注信息不仅有类别、位置信息，还有对图像的语义文本描述。MS COCO数据集中的图像分为训练、验证和测试集，其中包括person、bicycle、car、motorbike、aeroplane、bus等80个类别。\n",
    "\n",
    "CoCo2017数据集包括train(118287张)、val(5000张)。\n",
    "\n",
    "数据集官网链接：https://cocodataset.org/#home\n",
    " \n",
    "数据集的文件目录结构如下所示：  \n",
    "\n",
    "        ├── dataset\n",
    "            ├── coco2017\n",
    "                ├── annotations\n",
    "                │   ├─ train.json\n",
    "                │   └─ val.json\n",
    "                ├─ train\n",
    "                │   ├─picture1.jpg\n",
    "                │   ├─ ...\n",
    "                │   └─picturen.jpg\n",
    "                └─ val\n",
    "                    ├─picture1.jpg\n",
    "                    ├─ ...\n",
    "                    └─picturen.jpg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.实现\n",
    "模型分为两个训练阶段，第一阶段使用数据增强，第二阶段不使用数据增强\n",
    "### 3.1导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import cv2\n",
    "import multiprocessing\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import stat\n",
    "import functools\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from tqdm import tqdm\n",
    "import colorsys\n",
    "\n",
    "import mindspore\n",
    "import mindspore.dataset as de\n",
    "import mindspore.nn as nn\n",
    "import mindspore.common.dtype as mstype\n",
    "from mindspore import load_checkpoint, load_param_into_net, save_checkpoint, Tensor, Parameter, ops, context, Model, DynamicLossScaleManager\n",
    "from mindspore.profiler.profiling import Profiler\n",
    "from mindspore.communication.management import init, get_rank, get_group_size\n",
    "from mindspore.context import ParallelMode\n",
    "from mindspore.common.parameter import ParameterTuple\n",
    "from mindspore.common import set_seed, initializer\n",
    "from mindspore.common.initializer import Initializer as MeInitializer\n",
    "from mindspore.train.callback import Callback, CheckpointConfig, ModelCheckpoint\n",
    "from mindspore.ops import composite as C\n",
    "from mindspore.ops import functional as F\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.ops.primitive import constexpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.2运行设置\n",
    "关键设置:\n",
    "1) device_targe,根据平台设置相应设备\n",
    "\n",
    "2) max_epoch,当前训练阶段的最大epoch, 设置为第一阶段训练最大epoch\n",
    "\n",
    "3) total_epoch, 两个阶段总共训练的epoch\n",
    "\n",
    "4) data_dir, coco格式数据集的位置(按照目录存放)\n",
    "\n",
    "5) per_batch_size, 训练、验证时的batch size\n",
    "\n",
    "6) num_classes, 数据集对应的种类数\n",
    "\n",
    "7) input_size, 输入模型的图像尺寸，设置为32的倍数\n",
    "\n",
    "8) no_aug_epochs, total_epoch - max_epoch\n",
    "\n",
    "9) log_path, 存放eval结果的路径\n",
    "\n",
    "10) run_eval, 是否在训练时启用eval\n",
    "\n",
    "11) eval_interval, eval的频率\n",
    "\n",
    "12) classes_path, 存放数据集类别名称的文件位置，需要pred时使用\n",
    "\n",
    "13) pred_input, 测试图片位置，需要pred时使用\n",
    "\n",
    "14) pred_output, 测试输出位置，需要pred时使用\n",
    "\n",
    "15) ckpt_path, 存放输出权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "# settings\n",
    "#------------------------#\n",
    "def parase_config():\n",
    "    parase = argparse.ArgumentParser(description=__doc__)\n",
    "\n",
    "    parase.add_argument('--backbone', default='yolox_darknet53', help='option for backbone, you can choose yolox_darknet53 or yolox_x')\n",
    "    parase.add_argument('--data_aug', default=True, help='stage one use data aug, stage two not use')\n",
    "    parase.add_argument('--device_target', default='Ascend', help='Ascend GPU CPU platform')\n",
    "    parase.add_argument('--outputs_dir', default='./')\n",
    "\n",
    "    #train opt\n",
    "    parase.add_argument('--save_graphs', default=False)\n",
    "    parase.add_argument('--lr_scheduler', default='yolox_warm_cos_lr')\n",
    "    parase.add_argument('--max_epoch', default=10, help='max epoch for one train stage')\n",
    "    parase.add_argument('--total_epoch', default=15, help='total epoch for all train stages')\n",
    "    parase.add_argument('--data_dir', default='test_coco', help='data with coco form')\n",
    "    parase.add_argument('--yolox_no_aug_ckpt', default='', help='last no data aug related')\n",
    "    parase.add_argument('--need_profiler', default=0)\n",
    "    parase.add_argument('--pretrained', default=None, help='pretrained backbon path')\n",
    "    parase.add_argument('--resume_yolox', default=None, help='resume weight path')\n",
    "    parase.add_argument('--flip_prob', default=0.5, help='related to data aug')\n",
    "    parase.add_argument('--hsv_prob', default=1.0, help='data aug related')\n",
    "    parase.add_argument('--per_batch_size', default=2, help='batch size')\n",
    "\n",
    "    #net config\n",
    "    parase.add_argument('--depth_wise', default=False)\n",
    "    parase.add_argument('--max_gt', default=120)\n",
    "    parase.add_argument('--num_classes', default=3, help='match the classes num your dataset owns')\n",
    "    parase.add_argument('--input_size', default=[640, 640])\n",
    "    parase.add_argument('--fpn_strides', default=[8, 16, 32])\n",
    "    parase.add_argument('--use_l1', default=False, help='use l1 loss when stage_2')\n",
    "    parase.add_argument('--use_syc_bn', default=True)\n",
    "    parase.add_argument('--updates', default=0.0)\n",
    "    parase.add_argument('--n_candidate_k', default=10, help='dynamic_k')\n",
    "\n",
    "    #optimizer\n",
    "    parase.add_argument('--lr', default=0.01, help='set 0.04 for yolox-x')\n",
    "    parase.add_argument('--min_lr_ratio', default=0.001)\n",
    "    parase.add_argument('--warmup_epochs', default=5)\n",
    "    parase.add_argument('--weight_decay', default=0.0005)\n",
    "    parase.add_argument('--momentum', default=0.9)\n",
    "    parase.add_argument('--no_aug_epochs', default=5, help='set equal to total_epoch - max_epoch')\n",
    "\n",
    "    #logger\n",
    "    parase.add_argument('--log_interval', default=30)\n",
    "    parase.add_argument('--ckpt_interval', default=10)\n",
    "    parase.add_argument('--is_save_on_master', default=1)\n",
    "    parase.add_argument('--ckpt_max_num', default=60)\n",
    "    parase.add_argument('--opt', default='Momentum')\n",
    "\n",
    "    #distributed\n",
    "    parase.add_argument('--is_distributed', default=0)\n",
    "    parase.add_argument('--rank', default=0)\n",
    "    parase.add_argument('--group_size', default=1)\n",
    "    parase.add_argument('--bind_cpu', default=True)\n",
    "    parase.add_argument('--device_num', default=1)\n",
    "\n",
    "    #model arts\n",
    "    parase.add_argument('--is_modelArts', default=0)\n",
    "    parase.add_argument('--enable_modelarts', default=False)\n",
    "    parase.add_argument('--need_modelarts_dataset_unzip',default=False)\n",
    "    parase.add_argument('--modelarts_dataset_unzip_name', default='coco2017')\n",
    "    parase.add_argument('--data_url', default='')\n",
    "    parase.add_argument('--train_url', default='')\n",
    "    parase.add_argument('--checkpoint_url', default='')\n",
    "    parase.add_argument('--data_path', default='')\n",
    "    parase.add_argument('--output_path', default='./')\n",
    "    parase.add_argument('--load_path', default='')\n",
    "\n",
    "    \n",
    "    parase.add_argument('--ckpt_path', default='./save_weights', help='save ckpt')\n",
    "\n",
    "    #eval\n",
    "    parase.add_argument('--log_path', default='./eval_logs')\n",
    "    parase.add_argument('--val_ckpt', default='')\n",
    "    parase.add_argument('--conf_thre', default=0.001)\n",
    "    parase.add_argument('--nms_thre', default=0.65)\n",
    "    parase.add_argument('--eval_interval', default=10)\n",
    "    parase.add_argument('--run_eval', default=False)\n",
    "\n",
    "    #pred\n",
    "    parase.add_argument('--pred_ckpt', default='')\n",
    "    parase.add_argument('--pred_conf_thre', default=0.01)\n",
    "    parase.add_argument('--pred_nms_thre', default=0.5)\n",
    "    parase.add_argument('--classes_path', default='test_coco/classes.txt')\n",
    "    parase.add_argument('--pred_input', default='test_coco/val2017')\n",
    "    parase.add_argument('--pred_output', default='./pred_output')\n",
    "\n",
    "    #modelarts\n",
    "    parase.add_argument('--is_modelart', default=False)\n",
    "    parase.add_argument('--result_path', default='')\n",
    "\n",
    "    #export opt\n",
    "    parase.add_argument('--file_format', default='MINDIR')\n",
    "    parase.add_argument('--export_bs', default=1)\n",
    "\n",
    "    args = parase.parse_args(args=[])\n",
    "\n",
    "    return args\n",
    "\n",
    "config = parase_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.3 logger相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "# logger\n",
    "#------------------------#\n",
    "class LOGGER(logging.Logger):\n",
    "    \"\"\"\n",
    "    Logger.\n",
    "\n",
    "    Args:\n",
    "         logger_name: String. Logger name.\n",
    "         rank: Integer. Rank id.\n",
    "    \"\"\"\n",
    "    def __init__(self, logger_name, rank=0):\n",
    "        super(LOGGER, self).__init__(logger_name)\n",
    "        self.rank = rank\n",
    "        if rank % 8 == 0:\n",
    "            console = logging.StreamHandler(sys.stdout)\n",
    "            console.setLevel(logging.INFO)\n",
    "            formatter = logging.Formatter('%(asctime)s:%(levelname)s:%(message)s')\n",
    "            console.setFormatter(formatter)\n",
    "            self.addHandler(console)\n",
    "\n",
    "    def setup_logging_file(self, log_dir, rank=0):\n",
    "        \"\"\"Setup logging file.\"\"\"\n",
    "        self.rank = rank\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir, exist_ok=True)\n",
    "        log_name = datetime.datetime.now().strftime('%Y-%m-%d_time_%H_%M_%S') + '_rank_{}.log'.format(rank)\n",
    "        self.log_fn = os.path.join(log_dir, log_name)\n",
    "        fh = logging.FileHandler(self.log_fn)\n",
    "        fh.setLevel(logging.INFO)\n",
    "        formatter = logging.Formatter('%(asctime)s:%(levelname)s:%(message)s')\n",
    "        fh.setFormatter(formatter)\n",
    "        self.addHandler(fh)\n",
    "\n",
    "    def info(self, msg, *args, **kwargs):\n",
    "        if self.isEnabledFor(logging.INFO):\n",
    "            self._log(logging.INFO, msg, args, **kwargs)\n",
    "\n",
    "    def save_args(self, args):\n",
    "        self.info('Args:')\n",
    "        args_dict = vars(args)\n",
    "        for key in args_dict.keys():\n",
    "            self.info('--> %s: %s', key, args_dict[key])\n",
    "        self.info('')\n",
    "\n",
    "    def important_info(self, msg, *args, **kwargs):\n",
    "        if self.isEnabledFor(logging.INFO) and self.rank == 0:\n",
    "            line_width = 2\n",
    "            important_msg = '\\n'\n",
    "            important_msg += ('*'*70 + '\\n')*line_width\n",
    "            important_msg += ('*'*line_width + '\\n')*2\n",
    "            important_msg += '*'*line_width + ' '*8 + msg + '\\n'\n",
    "            important_msg += ('*'*line_width + '\\n')*2\n",
    "            important_msg += ('*'*70 + '\\n')*line_width\n",
    "            self.info(important_msg, *args, **kwargs)\n",
    "\n",
    "def get_logger(path, rank):\n",
    "    \"\"\"Get Logger.\"\"\"\n",
    "    logger = LOGGER('yolox', rank)\n",
    "    logger.setup_logging_file(path, rank)\n",
    "    return logger\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.4 image transform相关\n",
    "进行图像预处理，包含Mosaic和MixUp所需函数， 获取SimOTA中的动态正样本。\n",
    "\n",
    "在SimOTA中，**不同目标设定不同的正样本数量(dynamic k)**，以旷视科技官方回答中的蚂蚁和西瓜为例子，传统的正样本分配方案常常为同一场景下的西瓜和蚂蚁分配同样的正样本数，那要么蚂蚁有很多低质量的正样本，要么西瓜仅仅只有一两个正样本，这样的结果对于哪个分配方式都是不合适的。\n",
    "\n",
    "动态的正样本设置的关键在于如何确定k，SimOTA具体的做法是首先计算每个目标Cost最低的10特征点，然后把这十个特征点对应的预测框与真实框的IOU加起来求得最终的k。\n",
    "\n",
    "因此，**SimOTA的过程**总结如下：\n",
    "\n",
    "1）计算每个真实框和当前特征点预测框的重合程度；\n",
    "\n",
    "2）计算将重合度最高的十个预测框与真实框的IOU加起来求得每个真实框的k，也就代表每个真实框有k个特征点与之对应；\n",
    "\n",
    "3）计算每个真实框和当前特征点预测框的种类预测准确度；\n",
    "\n",
    "4）判断真实框的中心是否落在了特征点的一定半径内；\n",
    "\n",
    "5）计算Cost代价矩阵；\n",
    "\n",
    "6）将Cost最低的k个点作为该真实框的正样本。\n",
    "\n",
    "**Cost代价矩阵的目的是自适应的找到当前特征点应该去拟合的真实框，重合度越高越需要拟合，分类越准越需要拟合，在一定半径内越需要拟合。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "# image transform\n",
    "#------------------------#\n",
    "def get_aug_params(value, center=0):\n",
    "    if isinstance(value, float):\n",
    "        min_v = center - value\n",
    "        max_v = center + value\n",
    "    elif len(value) == 2:\n",
    "        min_v = value[0]\n",
    "        max_v = value[1]\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Affine params should be either a sequence containing two values\\\n",
    "             or single float values. Got {}\".format(value)\n",
    "        )\n",
    "    return random.uniform(min_v, max_v)\n",
    "\n",
    "def get_affine_matrix(\n",
    "        target_size,\n",
    "        degrees=10,\n",
    "        translate=0.1,\n",
    "        scales=0.1,\n",
    "        shear=10,\n",
    "):\n",
    "    twidth, theight = target_size\n",
    "\n",
    "    # Rotation and Scale\n",
    "    angle = get_aug_params(degrees)\n",
    "    scale = get_aug_params(scales, center=1.0)\n",
    "\n",
    "    if scale <= 0.0:\n",
    "        raise ValueError(\"Argument scale should be positive\")\n",
    "\n",
    "    R = cv2.getRotationMatrix2D(angle=angle, center=(0, 0), scale=scale)\n",
    "\n",
    "    M = np.ones([2, 3])\n",
    "    # Shear\n",
    "    shear_x = math.tan(get_aug_params(shear) * math.pi / 180)\n",
    "    shear_y = math.tan(get_aug_params(shear) * math.pi / 180)\n",
    "\n",
    "    M[0] = R[0] + shear_y * R[1]\n",
    "    M[1] = R[1] + shear_x * R[0]\n",
    "\n",
    "    # Translation\n",
    "    translation_x = get_aug_params(translate) * twidth  # x translation (pixels)\n",
    "    translation_y = get_aug_params(translate) * theight  # y translation (pixels)\n",
    "\n",
    "    M[0, 2] = translation_x\n",
    "    M[1, 2] = translation_y\n",
    "\n",
    "    return M, scale\n",
    "\n",
    "def apply_affine_to_bboxes(targets, target_size, M, scale):\n",
    "    num_gts = len(targets)\n",
    "\n",
    "    # warp corner points\n",
    "    twidth, theight = target_size\n",
    "    corner_points = np.ones((4 * num_gts, 3))\n",
    "    corner_points[:, :2] = targets[:, [0, 1, 2, 3, 0, 3, 2, 1]].reshape(\n",
    "        4 * num_gts, 2\n",
    "    )  # x1y1, x2y2, x1y2, x2y1\n",
    "    corner_points = corner_points @ M.T  # apply affine transform\n",
    "    corner_points = corner_points.reshape(num_gts, 8)\n",
    "\n",
    "    # create new boxes\n",
    "    corner_xs = corner_points[:, 0::2]\n",
    "    corner_ys = corner_points[:, 1::2]\n",
    "    new_bboxes = (\n",
    "        np.concatenate((corner_xs.min(1), corner_ys.min(1), corner_xs.max(1), corner_ys.max(1))).reshape(4, num_gts).T)\n",
    "\n",
    "    # clip boxes\n",
    "    new_bboxes[:, 0::2] = new_bboxes[:, 0::2].clip(0, twidth)\n",
    "    new_bboxes[:, 1::2] = new_bboxes[:, 1::2].clip(0, theight)\n",
    "\n",
    "    targets[:, :4] = new_bboxes\n",
    "\n",
    "    return targets\n",
    "\n",
    "def random_affine(\n",
    "        img,\n",
    "        targets=(),\n",
    "        target_size=(640, 640),\n",
    "        degrees=10,\n",
    "        translate=0.1,\n",
    "        scales=0.1,\n",
    "        shear=10,\n",
    "):\n",
    "    M, scale = get_affine_matrix(target_size, degrees, translate, scales, shear)\n",
    "\n",
    "    img = cv2.warpAffine(img, M, dsize=target_size, borderValue=(114, 114, 114))\n",
    "\n",
    "    # Transform label coordinates\n",
    "    target_length = len(targets)\n",
    "    if target_length:\n",
    "        targets = apply_affine_to_bboxes(targets, target_size, M, scale)\n",
    "    return img, targets\n",
    "\n",
    "def box_candidates(box1, box2, wh_thr=2, ar_thr=20, area_thr=0.2):\n",
    "    # box1(4,n), box2(4,n)\n",
    "    # Compute candidate boxes which include following 5 things:\n",
    "    # box1 before augment, box2 after augment, wh_thr (pixels), aspect_ratio_thr, area_ratio\n",
    "    w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\n",
    "    w2, h2 = box2[2] - box2[0], box2[3] - box2[1]\n",
    "    ar = np.maximum(w2 / (h2 + 1e-16), h2 / (w2 + 1e-16))  # aspect ratio\n",
    "    return (w2 > wh_thr) & (h2 > wh_thr) & (w2 * h2 / (w1 * h1 + 1e-16) > area_thr) & (ar < ar_thr)  # candidates\n",
    "\n",
    "def augment_hsv(img, hgain=0.015, sgain=0.7, vgain=0.4):\n",
    "    \"\"\" hsv augment \"\"\"\n",
    "    r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  # random gains\n",
    "    hue, sat, val = cv2.split(cv2.cvtColor(img, cv2.COLOR_BGR2HSV))\n",
    "    dtype = img.dtype\n",
    "\n",
    "    x = np.arange(0, 256, dtype=np.int16)\n",
    "    lut_hue = ((x * r[0]) % 180).astype(dtype)\n",
    "    lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\n",
    "    lut_val = np.clip(x * r[2], 0, 255).astype(dtype)\n",
    "\n",
    "    img_hsv = cv2.merge(\n",
    "        (cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val))\n",
    "    ).astype(dtype)\n",
    "    cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR, dst=img)\n",
    "\n",
    "def _mirror(image, boxes, prob=0.5):\n",
    "    _, width, _ = image.shape\n",
    "    if random.random() < prob:\n",
    "        image = image[:, ::-1]\n",
    "        boxes[:, 0::2] = width - boxes[:, 2::-2]\n",
    "    return image, boxes\n",
    "\n",
    "def preproc(img, input_size, swap=(2, 0, 1)):\n",
    "    \"\"\" padding image and transpose dim \"\"\"\n",
    "    if len(img.shape) == 3:\n",
    "        padded_img = np.ones((input_size[0], input_size[1], 3), dtype=np.uint8) * 114\n",
    "    else:\n",
    "        padded_img = np.ones(input_size, dtype=np.uint8) * 114\n",
    "    r = min(input_size[0] / img.shape[0], input_size[1] / img.shape[1])\n",
    "    resized_img = cv2.resize(\n",
    "        img,\n",
    "        (int(img.shape[1] * r), int(img.shape[0] * r)),\n",
    "        interpolation=cv2.INTER_LINEAR,\n",
    "    ).astype(np.uint8)\n",
    "    padded_img[: int(img.shape[0] * r), : int(img.shape[1] * r)] = resized_img\n",
    "\n",
    "    padded_img = padded_img.transpose(swap)\n",
    "    padded_img = np.ascontiguousarray(padded_img, dtype=np.float32)\n",
    "    return padded_img, r\n",
    "\n",
    "class TrainTransform:\n",
    "    \"\"\" image transform for training \"\"\"\n",
    "\n",
    "    def __init__(self, max_labels=50, flip_prob=0.5, hsv_prob=1.0, config=None):\n",
    "        if config:\n",
    "            self.max_labels = config.max_gt\n",
    "            self.flip_prob = config.flip_prob\n",
    "            self.hsv_prob = config.hsv_prob\n",
    "            self.strides = config.fpn_strides\n",
    "            self.input_size = config.input_size\n",
    "        else:\n",
    "            self.hsv_prob = 1.0\n",
    "            self.flip_prob = 0.5\n",
    "            self.max_labels = max_labels\n",
    "            self.strides = [8, 16, 32]\n",
    "            self.input_size = (640, 640)\n",
    "        self.grid_size = [(self.input_size[0] / x) * (self.input_size[1] / x) for x in\n",
    "                          self.strides]\n",
    "        self.num_total_anchor = int(sum(self.grid_size))\n",
    "\n",
    "    def __call__(self, image, targets, input_dim):\n",
    "        \"\"\" Tran transform call \"\"\"\n",
    "        boxes = targets[:, :4]\n",
    "        labels = targets[:, 4]\n",
    "        if not boxes.size:\n",
    "            targets = np.zeros((self.max_labels, 5), dtype=np.float32)\n",
    "            image, r_o = preproc(image, input_dim)\n",
    "            is_in_boxes_all = np.zeros((self.max_labels, self.num_total_anchor)).astype(np.bool_)\n",
    "            is_in_boxes_and_center = np.zeros((self.max_labels, self.num_total_anchor)).astype(np.bool_)\n",
    "            return image, targets, is_in_boxes_all, is_in_boxes_and_center\n",
    "        image_o = image.copy()\n",
    "        targets_o = targets.copy()\n",
    "        boxes_o = targets_o[:, :4]\n",
    "        labels_o = targets_o[:, 4]\n",
    "        boxes_o = xyxy2cxcywh(boxes_o)\n",
    "\n",
    "        if random.random() < self.hsv_prob:\n",
    "            augment_hsv(image)\n",
    "        image_t, boxes = _mirror(image, boxes, self.flip_prob)\n",
    "        image_t, r_ = preproc(image_t, input_dim)\n",
    "        boxes = xyxy2cxcywh(boxes)\n",
    "        boxes *= r_\n",
    "\n",
    "        mask_b = np.minimum(boxes[:, 2], boxes[:, 3]) > 1\n",
    "        boxes_t = boxes[mask_b]\n",
    "        labels_t = labels[mask_b]\n",
    "\n",
    "        if not boxes_t.size:\n",
    "            image_t, r_o = preproc(image_o, input_dim)\n",
    "            boxes_o *= r_o\n",
    "            boxes_t = boxes_o\n",
    "            labels_t = labels_o\n",
    "\n",
    "        labels_t = np.expand_dims(labels_t, 1)\n",
    "\n",
    "        targets_t = np.hstack((labels_t, boxes_t))\n",
    "        padded_labels = np.zeros((self.max_labels, 5))\n",
    "        true_labels = len(targets_t)\n",
    "\n",
    "        padded_labels[range(len(targets_t))[: self.max_labels]] = targets_t[: self.max_labels]\n",
    "        padded_labels = np.ascontiguousarray(padded_labels, dtype=np.float32)\n",
    "        gt_bboxes_per_image = padded_labels[:, 1:5]\n",
    "        # is_in_boxes_all [gt_max, 8400]\n",
    "        is_in_boxes_all, is_in_boxes_and_center = self.get_in_boxes_info(gt_bboxes_per_image, true_labels)\n",
    "        # is_in_boxes_all [gt_max, 8400]\n",
    "        is_in_boxes_all = is_in_boxes_all.any(1).reshape((-1, 1)) * is_in_boxes_all.any(0).reshape((1, -1))\n",
    "        return image_t, padded_labels, is_in_boxes_all, is_in_boxes_and_center\n",
    "\n",
    "    def get_grid(self):\n",
    "        \"\"\" get grid in each image \"\"\"\n",
    "        grid_size_x = []\n",
    "        grid_size_y = []\n",
    "        x_shifts = []  # (1, 6400) (1,1600) (1, 400) -->(1, 8400)\n",
    "        y_shifts = []  # (1, 6400) (1,1600) (1, 400)\n",
    "        expanded_strides = []  # (1, 6400) (1,1600) (1, 400)\n",
    "        for _stride in self.strides:\n",
    "            grid_size_x.append(int(self.input_size[0] / _stride))\n",
    "            grid_size_y.append(int(self.input_size[1] / _stride))\n",
    "        for i in range(len(grid_size_x)):\n",
    "            xv, yv = np.meshgrid(np.arange(0, grid_size_y[i]), np.arange(0, grid_size_x[i]))\n",
    "            grid = np.stack((xv, yv), 2).reshape(1, 1, grid_size_x[i], grid_size_y[i], 2)\n",
    "            grid = grid.reshape(1, -1, 2)\n",
    "            x_shifts.append(grid[:, :, 0])\n",
    "            y_shifts.append(grid[:, :, 1])\n",
    "            this_stride = np.zeros((1, grid.shape[1]))\n",
    "            this_stride.fill(self.strides[i])\n",
    "            this_stride = this_stride.astype(np.float32)\n",
    "            expanded_strides.append(this_stride)\n",
    "        x_shifts = np.concatenate(x_shifts, axis=1)\n",
    "        y_shifts = np.concatenate(y_shifts, axis=1)\n",
    "        expanded_strides = np.concatenate(expanded_strides, axis=1)\n",
    "        return x_shifts, y_shifts, expanded_strides\n",
    "\n",
    "    def get_in_boxes_info(self, gt_bboxes_per_image, true_labels):\n",
    "        \"\"\" get the pre in-center and in-box info for each image \"\"\"\n",
    "        x_shifts, y_shifts, expanded_strides = self.get_grid()\n",
    "        num_total_anchor = x_shifts.shape[1]\n",
    "        expanded_strides = expanded_strides[0]\n",
    "        x_shifts_per_image = x_shifts[0] * expanded_strides\n",
    "        y_shifts_per_image = y_shifts[0] * expanded_strides\n",
    "\n",
    "        x_centers_per_image = np.expand_dims((x_shifts_per_image + 0.5 * expanded_strides), axis=0)\n",
    "        x_centers_per_image = np.repeat(x_centers_per_image, self.max_labels, axis=0)\n",
    "\n",
    "        y_centers_per_image = np.expand_dims((y_shifts_per_image + 0.5 * expanded_strides), axis=0)\n",
    "        y_centers_per_image = np.repeat(y_centers_per_image, self.max_labels, axis=0)\n",
    "\n",
    "        gt_bboxes_per_image_l = np.expand_dims((gt_bboxes_per_image[:, 0] - 0.5 * gt_bboxes_per_image[:, 2]), axis=1)\n",
    "        gt_bboxes_per_image_l = np.repeat(gt_bboxes_per_image_l, num_total_anchor, axis=1)\n",
    "\n",
    "        gt_bboxes_per_image_r = np.expand_dims((gt_bboxes_per_image[:, 0] + 0.5 * gt_bboxes_per_image[:, 2]), axis=1)\n",
    "        gt_bboxes_per_image_r = np.repeat(gt_bboxes_per_image_r, num_total_anchor, axis=1)\n",
    "\n",
    "        gt_bboxes_per_image_t = np.expand_dims((gt_bboxes_per_image[:, 1] - 0.5 * gt_bboxes_per_image[:, 3]), axis=1)\n",
    "        gt_bboxes_per_image_t = np.repeat(gt_bboxes_per_image_t, num_total_anchor, axis=1)\n",
    "\n",
    "        gt_bboxes_per_image_b = np.expand_dims((gt_bboxes_per_image[:, 1] + 0.5 * gt_bboxes_per_image[:, 3]), axis=1)\n",
    "        gt_bboxes_per_image_b = np.repeat(gt_bboxes_per_image_b, num_total_anchor, axis=1)\n",
    "\n",
    "        b_l = x_centers_per_image - gt_bboxes_per_image_l\n",
    "        b_r = gt_bboxes_per_image_r - x_centers_per_image\n",
    "        b_t = y_centers_per_image - gt_bboxes_per_image_t\n",
    "        b_b = gt_bboxes_per_image_b - y_centers_per_image\n",
    "\n",
    "        bbox_deltas = np.stack([b_l, b_t, b_r, b_b], 2)\n",
    "        is_in_boxes = bbox_deltas.min(axis=-1) > 0.0\n",
    "        is_in_boxes[true_labels:, ...] = False\n",
    "\n",
    "        center_radius = 2.5\n",
    "        gt_bboxes_per_image_l = np.repeat(np.expand_dims((gt_bboxes_per_image[:, 0]), 1), num_total_anchor, 1) - \\\n",
    "                                center_radius * np.expand_dims(expanded_strides, 0)\n",
    "\n",
    "        gt_bboxes_per_image_r = np.repeat(np.expand_dims((gt_bboxes_per_image[:, 0]), 1), num_total_anchor, 1) + \\\n",
    "                                center_radius * np.expand_dims(expanded_strides, 0)\n",
    "\n",
    "        gt_bboxes_per_image_t = np.repeat(np.expand_dims((gt_bboxes_per_image[:, 1]), 1), num_total_anchor, 1) - \\\n",
    "                                center_radius * np.expand_dims(expanded_strides, 0)\n",
    "\n",
    "        gt_bboxes_per_image_b = np.repeat(np.expand_dims((gt_bboxes_per_image[:, 1]), 1), num_total_anchor, 1) + \\\n",
    "                                center_radius * np.expand_dims(expanded_strides, 0)\n",
    "\n",
    "        c_l = x_centers_per_image - gt_bboxes_per_image_l\n",
    "        c_r = gt_bboxes_per_image_r - x_centers_per_image\n",
    "        c_t = y_centers_per_image - gt_bboxes_per_image_t\n",
    "        c_b = gt_bboxes_per_image_b - y_centers_per_image\n",
    "\n",
    "        center_deltas = np.stack([c_l, c_r, c_t, c_b], 2)\n",
    "        is_in_centers = center_deltas.min(axis=-1) > 0.0\n",
    "        is_in_centers[true_labels:, ...] = False  # padding gts are set False\n",
    "\n",
    "        is_in_boxes_all = is_in_boxes | is_in_centers\n",
    "        is_in_boxes_and_center = is_in_boxes & is_in_centers\n",
    "        return is_in_boxes_all, is_in_boxes_and_center\n",
    "\n",
    "class ValTransform:\n",
    "    \"\"\" image transform for val \"\"\"\n",
    "\n",
    "    def __init__(self, swap=(2, 0, 1), legacy=False):\n",
    "        self.swap = swap\n",
    "        self.legacy = legacy\n",
    "        self.mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n",
    "        self.std = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\n",
    "\n",
    "    def __call__(self, img, input_size):\n",
    "        img, _ = preproc(img, input_size, self.swap)\n",
    "        if self.legacy:\n",
    "            img = img[::-1, :, :].copy() / 255.0\n",
    "            img = (img - self.mean) / self.std\n",
    "        return img, np.zeros((1, 5))\n",
    "\n",
    "def xyxy2cxcywh(bboxes):\n",
    "    bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 0]\n",
    "    bboxes[:, 3] = bboxes[:, 3] - bboxes[:, 1]\n",
    "    bboxes[:, 0] = bboxes[:, 0] + bboxes[:, 2] * 0.5\n",
    "    bboxes[:, 1] = bboxes[:, 1] + bboxes[:, 3] * 0.5\n",
    "    return bboxes\n",
    "\n",
    "def xyxy2xywh(bboxes):\n",
    "    bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 0]\n",
    "    bboxes[:, 3] = bboxes[:, 3] - bboxes[:, 1]\n",
    "    return bboxes\n",
    "\n",
    "def statistic_normalize_img(img, statistic_norm):\n",
    "    \"\"\"Statistic normalize images.\"\"\"\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    img = img / 255.\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    if statistic_norm:\n",
    "        img = (img - mean) / std\n",
    "    return np.transpose(img, (2, 0, 1)).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.5 数据集创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "# dataset\n",
    "#------------------------#\n",
    "min_keypoints_per_image = 10\n",
    "\n",
    "def _has_only_empty_bbox(anno):\n",
    "    return all(any(o <= 1 for o in obj[\"bbox\"][2:]) for obj in anno)\n",
    "\n",
    "def _count_visible_keypoints(anno):\n",
    "    return sum(sum(1 for v in ann[\"keypoints\"][2::3] if v > 0) for ann in anno)\n",
    "\n",
    "def has_valid_annotation(anno):\n",
    "    \"\"\"Check annotation file.\"\"\"\n",
    "    # if it's empty, there is no annotation\n",
    "    if not anno:\n",
    "        return False\n",
    "    # if all boxes have close to zero area, there is no annotation\n",
    "    if _has_only_empty_bbox(anno):\n",
    "        return False\n",
    "    # keypoints task have a slight different criteria for considering\n",
    "    # if an annotation is valid\n",
    "    if \"keypoints\" not in anno[0]:\n",
    "        return True\n",
    "    # for keypoint detection tasks, only consider valid images those\n",
    "    # containing at least min_keypoints_per_image\n",
    "    if _count_visible_keypoints(anno) >= min_keypoints_per_image:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_mosaic_coordinate(mosaic_image, mosaic_index, xc, yc, w, h, input_h, input_w):\n",
    "    \"\"\" Get mosaic coordinate \"\"\"\n",
    "    # index0 to top left part of image\n",
    "    if mosaic_index == 0:\n",
    "        x1, y1, x2, y2 = max(xc - w, 0), max(yc - h, 0), xc, yc\n",
    "        small_coord = w - (x2 - x1), h - (y2 - y1), w, h\n",
    "    # index1 to top right part of image\n",
    "    elif mosaic_index == 1:\n",
    "        x1, y1, x2, y2 = xc, max(yc - h, 0), min(xc + w, input_w * 2), yc\n",
    "        small_coord = 0, h - (y2 - y1), min(w, x2 - x1), h\n",
    "    # index2 to bottom left part of image\n",
    "    elif mosaic_index == 2:\n",
    "        x1, y1, x2, y2 = max(xc - w, 0), yc, xc, min(input_h * 2, yc + h)\n",
    "        small_coord = w - (x2 - x1), 0, w, min(y2 - y1, h)\n",
    "    # index2 to bottom right part of image\n",
    "    elif mosaic_index == 3:\n",
    "        x1, y1, x2, y2 = xc, yc, min(xc + w, input_w * 2), min(input_h * 2, yc + h)  # noqa\n",
    "        small_coord = 0, 0, min(w, x2 - x1), min(y2 - y1, h)\n",
    "    return (x1, y1, x2, y2), small_coord\n",
    "\n",
    "def adjust_box_anns(bbox, scale_ratio, padw, padh, w_max, h_max):\n",
    "    bbox[:, 0::2] = np.clip(bbox[:, 0::2] * scale_ratio + padw, 0, w_max)\n",
    "    bbox[:, 1::2] = np.clip(bbox[:, 1::2] * scale_ratio + padh, 0, h_max)\n",
    "    return bbox\n",
    "\n",
    "class COCOYoloXDataset:\n",
    "    \"\"\" YoloX Dataset for COCO \"\"\"\n",
    "\n",
    "    def __init__(self, root, ann_file, remove_images_without_annotations=True,\n",
    "                 filter_crowd_anno=True, is_training=True, mosaic=True, img_size=(640, 640),\n",
    "                 preproc=None, input_dim=(640, 640), mosaic_prob=1.0, enable_mosaic=True, eable_mixup=True,\n",
    "                 mixup_prob=1.0):\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.img_ids = list(self.coco.imgs.keys())\n",
    "        self.filter_crowd_anno = filter_crowd_anno\n",
    "        self.is_training = is_training\n",
    "        self.root = root\n",
    "        self.mosaic = mosaic\n",
    "        self.img_size = img_size\n",
    "        self.preproc = preproc\n",
    "        self.input_dim = input_dim\n",
    "        self.mosaic_prob = mosaic_prob\n",
    "        self.enable_mosaic = enable_mosaic\n",
    "        self.degrees = 10.0\n",
    "        self.translate = 0.1\n",
    "        self.scale = (0.5, 1.5)\n",
    "        self.mixup_scale = (0.5, 1.5)\n",
    "        self.shear = 2.0\n",
    "        self.perspective = 0.0\n",
    "        self.mixup_prob = mixup_prob\n",
    "        self.enable_mixup = eable_mixup\n",
    "\n",
    "        if remove_images_without_annotations:\n",
    "            img_ids = []\n",
    "            for img_id in self.img_ids:\n",
    "                ann_ids = self.coco.getAnnIds(imgIds=img_id, iscrowd=None)\n",
    "                anno = self.coco.loadAnns(ann_ids)\n",
    "                if has_valid_annotation(anno):\n",
    "                    img_ids.append(img_id)\n",
    "            self.img_ids = img_ids\n",
    "        self.categories = {cat[\"id\"]: cat[\"name\"] for cat in self.coco.cats.values()}\n",
    "        self.cat_ids_to_continuous_ids = {v: i for i, v in enumerate(self.coco.getCatIds())}\n",
    "        self.continuous_ids_cat_ids = {v: k for k, v in self.cat_ids_to_continuous_ids.items()}\n",
    "\n",
    "    def pull_item(self, index):\n",
    "        \"\"\"\n",
    "        pull image and label\n",
    "        \"\"\"\n",
    "        res, img_info, _ = self.load_anno_from_ids(index)\n",
    "        img = self.load_resized_img(index)\n",
    "        return img, res.copy(), img_info, np.array([self.img_ids[index]])\n",
    "\n",
    "    def mosaic_proc(self, idx):\n",
    "        \"\"\" Mosaic data augment \"\"\"\n",
    "        if self.enable_mosaic and random.random() < self.mosaic_prob:\n",
    "            mosaic_labels = []\n",
    "            input_dim = self.input_dim\n",
    "            input_h, input_w = input_dim[0], input_dim[1]\n",
    "            yc = int(random.uniform(0.5 * input_h, 1.5 * input_h))\n",
    "            xc = int(random.uniform(0.5 * input_w, 1.5 * input_w))\n",
    "            # 3 additional image indices\n",
    "            indices = [idx] + [random.randint(0, len(self.img_ids) - 1) for _ in range(3)]\n",
    "            for i_mosaic, index in enumerate(indices):\n",
    "                img, _labels, _, _ = self.pull_item(index)\n",
    "                h0, w0 = img.shape[:2]  # orig hw\n",
    "                scale = min(1. * input_h / h0, 1. * input_w / w0)\n",
    "                img = cv2.resize(\n",
    "                    img, (int(w0 * scale), int(h0 * scale)), interpolation=cv2.INTER_LINEAR\n",
    "                )\n",
    "                # generate output mosaic image\n",
    "                (h, w, c) = img.shape[:3]\n",
    "                if i_mosaic == 0:\n",
    "                    mosaic_img = np.full((input_h * 2, input_w * 2, c), 114, dtype=np.uint8)\n",
    "                # suffix l means large image, while s means small image in mosaic aug.\n",
    "                (l_x1, l_y1, l_x2, l_y2), (s_x1, s_y1, s_x2, s_y2) = get_mosaic_coordinate(\n",
    "                    mosaic_img, i_mosaic, xc, yc, w, h, input_h, input_w\n",
    "                )\n",
    "\n",
    "                mosaic_img[l_y1:l_y2, l_x1:l_x2] = img[s_y1:s_y2, s_x1:s_x2]\n",
    "                padw, padh = l_x1 - s_x1, l_y1 - s_y1\n",
    "\n",
    "                labels = _labels.copy()\n",
    "                # Normalized xywh to pixel xyxy format\n",
    "                if _labels.size > 0:\n",
    "                    labels[:, 0] = scale * _labels[:, 0] + padw\n",
    "                    labels[:, 1] = scale * _labels[:, 1] + padh\n",
    "                    labels[:, 2] = scale * _labels[:, 2] + padw\n",
    "                    labels[:, 3] = scale * _labels[:, 3] + padh\n",
    "                mosaic_labels.append(labels)\n",
    "\n",
    "            if mosaic_labels:\n",
    "                mosaic_labels = np.concatenate(mosaic_labels, 0)\n",
    "                np.clip(mosaic_labels[:, 0], 0, 2 * input_w, out=mosaic_labels[:, 0])\n",
    "                np.clip(mosaic_labels[:, 1], 0, 2 * input_h, out=mosaic_labels[:, 1])\n",
    "                np.clip(mosaic_labels[:, 2], 0, 2 * input_w, out=mosaic_labels[:, 2])\n",
    "                np.clip(mosaic_labels[:, 3], 0, 2 * input_h, out=mosaic_labels[:, 3])\n",
    "\n",
    "            mosaic_img, mosaic_labels = random_affine(\n",
    "                mosaic_img,\n",
    "                mosaic_labels,\n",
    "                target_size=(input_w, input_h),\n",
    "                degrees=self.degrees,\n",
    "                translate=self.translate,\n",
    "                scales=self.scale,\n",
    "                shear=self.shear,\n",
    "            )\n",
    "\n",
    "            if (\n",
    "                    self.enable_mixup\n",
    "                    and not mosaic_labels.size == 0\n",
    "                    and random.random() < self.mixup_prob\n",
    "            ):\n",
    "                mosaic_img, mosaic_labels = self.mixup(mosaic_img, mosaic_labels, self.input_dim)\n",
    "            mix_img, padded_labels, pre_fg_mask, is_inbox_and_incenter = self.preproc(mosaic_img, mosaic_labels,\n",
    "                                                                                      self.input_dim)\n",
    "            # -----------------------------------------------------------------\n",
    "            # img_info and img_id are not used for training.\n",
    "            # They are also hard to be specified on a mosaic image.\n",
    "            # -----------------------------------------------------------------\n",
    "            return mix_img, padded_labels, pre_fg_mask, is_inbox_and_incenter\n",
    "        img, label, _, _ = self.pull_item(idx)\n",
    "        img, label, pre_fg_mask, is_inbox_and_incenter = self.preproc(img, label, self.input_dim)\n",
    "        return img, label, pre_fg_mask, is_inbox_and_incenter\n",
    "\n",
    "    def mixup(self, origin_img, origin_labels, input_dim):\n",
    "        \"\"\" Mixup data augment \"\"\"\n",
    "        jit_factor = random.uniform(*self.mixup_scale)\n",
    "        FLIP = random.uniform(0, 1) > 0.5\n",
    "        cp_labels = np.empty(0)\n",
    "        while not cp_labels.size:\n",
    "            cp_index = random.randint(0, self.__len__() - 1)\n",
    "            cp_labels, _, _ = self.load_anno_from_ids(cp_index)\n",
    "        img, cp_labels, _, _ = self.pull_item(cp_index)\n",
    "\n",
    "        if len(img.shape) == 3:\n",
    "            cp_img = np.ones((input_dim[0], input_dim[1], 3), dtype=np.uint8) * 114\n",
    "        else:\n",
    "            cp_img = np.ones(input_dim, dtype=np.uint8) * 114\n",
    "\n",
    "        cp_scale_ratio = min(input_dim[0] / img.shape[0], input_dim[1] / img.shape[1])\n",
    "        resized_img = cv2.resize(\n",
    "            img,\n",
    "            (int(img.shape[1] * cp_scale_ratio), int(img.shape[0] * cp_scale_ratio)),\n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "        )\n",
    "\n",
    "        cp_img[: int(img.shape[0] * cp_scale_ratio), : int(img.shape[1] * cp_scale_ratio)] = resized_img\n",
    "\n",
    "        cp_img = cv2.resize(\n",
    "            cp_img,\n",
    "            (int(cp_img.shape[1] * jit_factor), int(cp_img.shape[0] * jit_factor)),\n",
    "        )\n",
    "        cp_scale_ratio *= jit_factor\n",
    "\n",
    "        if FLIP:\n",
    "            cp_img = cp_img[:, ::-1, :]\n",
    "\n",
    "        origin_h, origin_w = cp_img.shape[:2]\n",
    "        target_h, target_w = origin_img.shape[:2]\n",
    "        padded_img = np.zeros(\n",
    "            (max(origin_h, target_h), max(origin_w, target_w), 3), dtype=np.uint8\n",
    "        )\n",
    "        padded_img[:origin_h, :origin_w] = cp_img\n",
    "\n",
    "        x_offset, y_offset = 0, 0\n",
    "        if padded_img.shape[0] > target_h:\n",
    "            y_offset = random.randint(0, padded_img.shape[0] - target_h - 1)\n",
    "        if padded_img.shape[1] > target_w:\n",
    "            x_offset = random.randint(0, padded_img.shape[1] - target_w - 1)\n",
    "        padded_cropped_img = padded_img[y_offset: y_offset + target_h, x_offset: x_offset + target_w]\n",
    "\n",
    "        cp_bboxes_origin_np = adjust_box_anns(\n",
    "            cp_labels[:, :4].copy(), cp_scale_ratio, 0, 0, origin_w, origin_h\n",
    "        )\n",
    "        if FLIP:\n",
    "            cp_bboxes_origin_np[:, 0::2] = (origin_w - cp_bboxes_origin_np[:, 0::2][:, ::-1])\n",
    "        cp_bboxes_transformed_np = cp_bboxes_origin_np.copy()\n",
    "        cp_bboxes_transformed_np[:, 0::2] = np.clip(\n",
    "            cp_bboxes_transformed_np[:, 0::2] - x_offset, 0, target_w\n",
    "        )\n",
    "        cp_bboxes_transformed_np[:, 1::2] = np.clip(\n",
    "            cp_bboxes_transformed_np[:, 1::2] - y_offset, 0, target_h\n",
    "        )\n",
    "        keep_list = box_candidates(cp_bboxes_origin_np.T, cp_bboxes_transformed_np.T, 5)\n",
    "\n",
    "        if keep_list.sum() >= 1.0:\n",
    "            cls_labels = cp_labels[keep_list, 4:5].copy()\n",
    "            box_labels = cp_bboxes_transformed_np[keep_list]\n",
    "            labels = np.hstack((box_labels, cls_labels))\n",
    "            origin_labels = np.vstack((origin_labels, labels))\n",
    "            origin_img = origin_img.astype(np.float32)\n",
    "            origin_img = 0.5 * origin_img + 0.5 * padded_cropped_img.astype(np.float32)\n",
    "\n",
    "        return origin_img.astype(np.uint8), origin_labels\n",
    "\n",
    "    def load_anno_from_ids(self, index):\n",
    "        \"\"\"\n",
    "        load annotations via ids\n",
    "        \"\"\"\n",
    "        img_id = self.img_ids[index]\n",
    "        im_ann = self.coco.loadImgs(img_id)[0]\n",
    "        width = im_ann[\"width\"]\n",
    "        height = im_ann[\"height\"]\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        annotations = self.coco.loadAnns(ann_ids)\n",
    "        objs = []\n",
    "        for obj in annotations:\n",
    "            x1 = np.max((0, obj[\"bbox\"][0]))\n",
    "            y1 = np.max((0, obj[\"bbox\"][1]))\n",
    "            x2 = np.min((width, x1 + np.max((0, obj[\"bbox\"][2]))))\n",
    "            y2 = np.min((height, y1 + np.max((0, obj[\"bbox\"][3]))))\n",
    "            if obj[\"area\"] > 0 and x2 >= x1 and y2 >= y1:\n",
    "                obj[\"clean_bbox\"] = [x1, y1, x2, y2]\n",
    "                objs.append(obj)\n",
    "        nums_objs = len(objs)\n",
    "        res = np.zeros((nums_objs, 5))\n",
    "\n",
    "        for ix, obj in enumerate(objs):\n",
    "            cls = self.cat_ids_to_continuous_ids[obj[\"category_id\"]]\n",
    "            res[ix, 0:4] = obj[\"clean_bbox\"]\n",
    "            res[ix, 4] = cls\n",
    "        r = min(self.img_size[0] / height, self.img_size[1] / width)\n",
    "        res[:, :4] *= r\n",
    "        img_info = (height, width)\n",
    "        resize_info = (int(height * r), int(width * r))\n",
    "        return res, img_info, resize_info\n",
    "\n",
    "    def load_resized_img(self, index):\n",
    "        \"\"\"\n",
    "        resize to fix size\n",
    "        \"\"\"\n",
    "        img_id = self.img_ids[index]\n",
    "        img_path = self.coco.loadImgs(img_id)[0][\"file_name\"]\n",
    "        img_path = os.path.join(self.root, img_path)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = np.array(img)\n",
    "        r = min(self.img_size[0] / img.shape[0], self.img_size[1] / img.shape[1])\n",
    "        resize_img = cv2.resize(\n",
    "            img,\n",
    "            (int(img.shape[1] * r), int(img.shape[0] * r)),\n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "        ).astype(np.uint8)\n",
    "        return resize_img\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.is_training:\n",
    "            img, labels, pre_fg_mask, is_inbox_and_incenter = self.mosaic_proc(index)\n",
    "            return img, labels, pre_fg_mask, is_inbox_and_incenter\n",
    "        img, _, img_info, img_id = self.pull_item(index)\n",
    "        if self.preproc is not None:\n",
    "            img, _ = self.preproc(img, self.input_dim)\n",
    "            img = img.astype(np.float32)\n",
    "        return img, img_info, img_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "def create_yolox_dataset(image_dir, anno_path, batch_size, device_num, rank,\n",
    "                         data_aug=True, is_training=True):\n",
    "    \"\"\" create yolox dataset \"\"\"\n",
    "    cv2.setNumThreads(0)\n",
    "    if is_training:\n",
    "        filter_crowd = False\n",
    "        remove_empty_anno = False\n",
    "    else:\n",
    "        filter_crowd = False\n",
    "        remove_empty_anno = False\n",
    "    img_size = config.input_size\n",
    "    input_dim = img_size\n",
    "    if is_training:\n",
    "\n",
    "        yolo_dataset = COCOYoloXDataset(root=image_dir, ann_file=anno_path, filter_crowd_anno=filter_crowd,\n",
    "                                        remove_images_without_annotations=remove_empty_anno, is_training=is_training,\n",
    "                                        mosaic=data_aug, eable_mixup=data_aug, enable_mosaic=data_aug,\n",
    "                                        preproc=TrainTransform(config=config), img_size=img_size, input_dim=input_dim)\n",
    "    else:\n",
    "        yolo_dataset = COCOYoloXDataset(\n",
    "            root=image_dir, ann_file=anno_path, filter_crowd_anno=filter_crowd,\n",
    "            remove_images_without_annotations=remove_empty_anno, is_training=is_training, mosaic=False,\n",
    "            eable_mixup=False,\n",
    "            img_size=img_size, input_dim=input_dim, preproc=ValTransform(legacy=False)\n",
    "        )\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    num_parallel_workers = int(cores / device_num)\n",
    "    if is_training:\n",
    "        dataset_column_names = [\"image\", \"labels\", \"pre_fg_mask\", \"is_inbox_and_inCenter\"]\n",
    "        ds = de.GeneratorDataset(yolo_dataset, column_names=dataset_column_names,\n",
    "                                 num_parallel_workers=min(8, num_parallel_workers),\n",
    "                                 python_multiprocessing=True,\n",
    "                                 shard_id=rank, num_shards=device_num, shuffle=True)\n",
    "        ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    else:  # for val\n",
    "        ds = de.GeneratorDataset(yolo_dataset, column_names=[\"image\", \"image_shape\", \"img_id\"],\n",
    "                                 num_parallel_workers=min(8, num_parallel_workers), shuffle=False)\n",
    "        ds = ds.batch(batch_size, drop_remainder=False)\n",
    "    ds = ds.repeat(1)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.6 模型构建\n",
    "模型结构图如图所示:\n",
    "<!-- ![image.png](attachment:image.png) -->\n",
    "<img src=\"./images/3.6.png\" width=\"80%\">\n",
    "#### 3.6.1 基础模块\n",
    "模型基础组件，BaseConv, DWConv卷积块, Bottleneck、SPPBottleneck、CSPLayer以及Focus, 在后续模型构建中使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "# network blocks\n",
    "#------------------------#\n",
    "class SiLU(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(SiLU, self).__init__()\n",
    "        self.silu = nn.Sigmoid()\n",
    "\n",
    "    def construct(self, x):\n",
    "        return x * self.silu(x)\n",
    "\n",
    "def get_activation(name=\"silu\"):\n",
    "    \"\"\" get the activation function \"\"\"\n",
    "    if name == \"silu\":\n",
    "        module = SiLU()\n",
    "    elif name == \"relu\":\n",
    "        module = nn.ReLU()\n",
    "    elif name == \"lrelu\":\n",
    "        module = nn.LeakyReLU(0.1)\n",
    "    else:\n",
    "        raise AttributeError(\"Unsupported activate type: {}\".format(name))\n",
    "\n",
    "    return module\n",
    "\n",
    "class BaseConv(nn.Cell):\n",
    "    \"\"\"\n",
    "    A conv2d  -> BatchNorm  -> silu/leaky relu block\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, in_channels, out_channels, ksize, stride, groups=1, bias=False, act=\"silu\"):\n",
    "        super(BaseConv, self).__init__()\n",
    "        # same padding\n",
    "        pad = (ksize - 1) // 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=ksize,\n",
    "            stride=stride,\n",
    "            padding=pad,\n",
    "            pad_mode=\"pad\",\n",
    "            group=groups,\n",
    "            has_bias=bias\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = get_activation(act)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.act(self.bn(self.conv(x)))\n",
    "        return x\n",
    "\n",
    "def use_syc_bn(network):\n",
    "    \"\"\"Use synchronized batchnorm layer\"\"\"\n",
    "    for _, cell in network.cells_and_names():\n",
    "        if isinstance(cell, BaseConv):\n",
    "            out_channels = cell.bn.num_features\n",
    "            cell.bn = nn.SyncBatchNorm(out_channels)\n",
    "\n",
    "class DWConv(nn.Cell):\n",
    "    \"\"\"Depthwise Conv + Point Conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, ksize, stride=1, act=\"silu\"):\n",
    "        super(DWConv, self).__init__()\n",
    "        self.dconv = BaseConv(\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            ksize=ksize,\n",
    "            stride=stride,\n",
    "            groups=in_channels,\n",
    "            act=act\n",
    "        )\n",
    "        self.pconv = BaseConv(\n",
    "            in_channels, out_channels, ksize=1, stride=1, groups=1, act=act,\n",
    "        )\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.dconv(x)\n",
    "        return self.pconv(x)\n",
    "\n",
    "class Bottleneck(nn.Cell):\n",
    "    \"\"\" Standard bottleneck \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            shortcut=True,\n",
    "            expansion=0.5,\n",
    "            depthwise=False,\n",
    "            act=\"silu\"\n",
    "    ):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        hidden_channels = int(out_channels * expansion)\n",
    "        Conv = DWConv if depthwise else BaseConv\n",
    "        self.conv1 = BaseConv(in_channels, hidden_channels, 1, stride=1, act=act)\n",
    "        self.conv2 = Conv(hidden_channels, out_channels, 3, stride=1, act=act)\n",
    "        self.use_add = shortcut and in_channels == out_channels\n",
    "\n",
    "    def construct(self, x):\n",
    "        y = self.conv2(self.conv1(x))\n",
    "        if self.use_add:\n",
    "            y = y + x\n",
    "        return y\n",
    "\n",
    "class ResLayer(nn.Cell):\n",
    "    \"Residual layer with `in_channels` inputs.\"\n",
    "\n",
    "    def __init__(self, in_channels: int):\n",
    "        super().__init__()\n",
    "        mid_channels = in_channels // 2\n",
    "        self.layer1 = BaseConv(\n",
    "            in_channels, mid_channels, ksize=1, stride=1, act=\"lrelu\"\n",
    "        )\n",
    "        self.layer2 = BaseConv(\n",
    "            mid_channels, in_channels, ksize=3, stride=1, act=\"lrelu\"\n",
    "        )\n",
    "\n",
    "    def construct(self, x):\n",
    "        out = self.layer2(self.layer1(x))\n",
    "        return x + out\n",
    "\n",
    "class SPPBottleneck(nn.Cell):\n",
    "    \"\"\"Spatial pyramid pooling layer used in YOLOv3-SPP \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, in_channels, out_channels, kernel_sizes=(5, 9, 13), activation=\"silu\"\n",
    "    ):\n",
    "        super(SPPBottleneck, self).__init__()\n",
    "        hidden_channels = in_channels // 2\n",
    "        self.conv1 = BaseConv(in_channels, hidden_channels, 1, stride=1, act=activation)\n",
    "        self.m = nn.CellList(\n",
    "            [\n",
    "                nn.MaxPool2d(kernel_size=ks, stride=1)\n",
    "                for ks in kernel_sizes\n",
    "            ]\n",
    "        )\n",
    "        self.pad0 = ops.Pad(((0, 0), (0, 0), (kernel_sizes[0] // 2, kernel_sizes[0] // 2),\n",
    "                             (kernel_sizes[0] // 2, kernel_sizes[0] // 2)))\n",
    "        self.pad1 = ops.Pad(((0, 0), (0, 0), (kernel_sizes[1] // 2, kernel_sizes[1] // 2),\n",
    "                             (kernel_sizes[1] // 2, kernel_sizes[1] // 2)))\n",
    "        self.pad2 = ops.Pad(((0, 0), (0, 0), (kernel_sizes[2] // 2, kernel_sizes[2] // 2),\n",
    "                             (kernel_sizes[2] // 2, kernel_sizes[2] // 2)))\n",
    "        conv2_channels = hidden_channels * (len(kernel_sizes) + 1)\n",
    "        self.conv2 = BaseConv(conv2_channels, out_channels, 1, stride=1, act=activation)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        op = ops.Concat(axis=1)\n",
    "        x1 = self.m[0](self.pad0(x))\n",
    "        x2 = self.m[1](self.pad1(x))\n",
    "        x3 = self.m[2](self.pad2(x))\n",
    "        x = op((x, x1, x2, x3))\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class CSPLayer(nn.Cell):\n",
    "    \"\"\"C3 in yolov5, CSP Bottleneck with 3 convolutions\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            n=1,\n",
    "            shortcut=True,\n",
    "            expansion=0.5,\n",
    "            depthwise=False,\n",
    "            act=\"silu\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): input channels.\n",
    "            out_channels (int): output channels.\n",
    "            n (int): number of Bottlenecks. Default value: 1.\n",
    "        \"\"\"\n",
    "        # ch_in, ch_out, number, shortcut, groups, expansion\n",
    "        super().__init__()\n",
    "        hidden_channels = int(out_channels * expansion)  # hidden channels\n",
    "        self.conv1 = BaseConv(in_channels, hidden_channels, 1, stride=1, act=act)\n",
    "        self.conv2 = BaseConv(in_channels, hidden_channels, 1, stride=1, act=act)\n",
    "        self.conv3 = BaseConv(2 * hidden_channels, out_channels, 1, stride=1, act=act)\n",
    "        module_list = [\n",
    "            Bottleneck(\n",
    "                hidden_channels, hidden_channels, shortcut, 1.0, depthwise, act=act\n",
    "            )\n",
    "            for _ in range(n)\n",
    "        ]\n",
    "        self.m = nn.SequentialCell(module_list)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x_1 = self.conv1(x)\n",
    "        x_2 = self.conv2(x)\n",
    "        x_1 = self.m(x_1)\n",
    "        op = ops.Concat(axis=1)\n",
    "        x = op((x_1, x_2))\n",
    "        return self.conv3(x)\n",
    "\n",
    "class Focus(nn.Cell):\n",
    "    \"\"\"Focus width and height information into channel space.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, ksize=1, stride=1, act=\"silu\"):\n",
    "        super().__init__()\n",
    "        self.conv = BaseConv(in_channels * 4, out_channels, ksize, stride, act=act)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\" Focus forward \"\"\"\n",
    "        # shape of x (b,c,w,h) -> y(b,4c,w/2,h/2)\n",
    "        patch_top_left = x[..., ::2, ::2]\n",
    "        patch_top_right = x[..., ::2, 1::2]\n",
    "        patch_bot_left = x[..., 1::2, ::2]\n",
    "        patch_bot_right = x[..., 1::2, 1::2]\n",
    "        op = ops.Concat(axis=1)\n",
    "        x = op(\n",
    "            (patch_top_left,\n",
    "             patch_bot_left,\n",
    "             patch_top_right,\n",
    "             patch_bot_right)\n",
    "        )\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3.6.2 Darknet\n",
    "两种darknet结构，本代码默认使用Darknet(用于YOLOFPN)，需要使用CSPDarknet(用于YOLOPAFPN)需将backbone设置修改为yolox_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "# darknet\n",
    "#------------------------#\n",
    "class Darknet(nn.Cell):\n",
    "    \"\"\" Darknet for yolox-darknet53 \"\"\"\n",
    "    # number of block from dark2 to dark5.\n",
    "    depth2block = {21: [1, 2, 2, 1], 53: [2, 8, 8, 4]}\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            depth,\n",
    "            in_channels=3,\n",
    "            stem_out_channels=32,\n",
    "            out_features=(\"dark3\", \"dark4\", \"dark5\"),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            depth (int): depth of darknet used in model, usually use [21, 53] for this param.\n",
    "            in_channels (int): number of input channels, for example, use 3 for RGB image.\n",
    "            stem_out_channels (int): number of output channels of darknet stem.\n",
    "                It decides channels of darknet layer2 to layer5.\n",
    "            out_features (Tuple[str]): desired output layer name.\n",
    "        \"\"\"\n",
    "        super(Darknet, self).__init__()\n",
    "        assert out_features, \"please provide output features of Darknet\"\n",
    "        self.out_features = out_features\n",
    "        self.stem = nn.SequentialCell(\n",
    "            BaseConv(in_channels=in_channels, out_channels=stem_out_channels, ksize=3, stride=1, act=\"lrelu\"),\n",
    "            *self.make_group_layer(stem_out_channels, num_blocks=1, stride=2),\n",
    "        )\n",
    "        in_channels = stem_out_channels * 2\n",
    "\n",
    "        num_blocks = Darknet.depth2block[depth]\n",
    "        # create darknet with `stem_out_channels` and `num_blocks` layers.\n",
    "        # to make model structure more clear, we don't use `for` statement in python.\n",
    "        self.dark2 = nn.SequentialCell(\n",
    "            *self.make_group_layer(in_channels=in_channels, num_blocks=num_blocks[0], stride=2)\n",
    "        )\n",
    "        in_channels *= 2  # 128\n",
    "        self.dark3 = nn.SequentialCell(\n",
    "            *self.make_group_layer(in_channels=in_channels, num_blocks=num_blocks[1], stride=2)\n",
    "        )\n",
    "        in_channels *= 2  # 256\n",
    "        self.dark4 = nn.SequentialCell(\n",
    "            *self.make_group_layer(in_channels=in_channels, num_blocks=num_blocks[2], stride=2)\n",
    "        )\n",
    "        in_channels *= 2  # 512\n",
    "        self.dark5 = nn.SequentialCell(\n",
    "            *self.make_group_layer(in_channels=in_channels, num_blocks=num_blocks[3], stride=2),\n",
    "            *self.make_spp_block([in_channels, in_channels * 2], in_channels * 2),\n",
    "        )\n",
    "\n",
    "    def make_group_layer(self, in_channels: int, num_blocks: int, stride: int = 1):\n",
    "        \"starts with conv layer then has `num_blocks` `ResLayer`\"\n",
    "        return [\n",
    "            BaseConv(in_channels, in_channels * 2, ksize=3, stride=stride, act=\"lrelu\"),\n",
    "            *[(ResLayer(in_channels * 2)) for _ in range(num_blocks)],\n",
    "        ]\n",
    "\n",
    "    def make_spp_block(self, filters_list, in_filters):\n",
    "        \"\"\" spatial pyramid pooling block\"\"\"\n",
    "        m = nn.SequentialCell(\n",
    "            *[\n",
    "                BaseConv(in_filters, filters_list[0], 1, stride=1, act=\"lrelu\"),\n",
    "                BaseConv(filters_list[0], filters_list[1], 3, stride=1, act=\"lrelu\"),\n",
    "                SPPBottleneck(\n",
    "                    in_channels=filters_list[1],\n",
    "                    out_channels=filters_list[0],\n",
    "                    activation=\"lrelu\",\n",
    "                ),\n",
    "                BaseConv(filters_list[0], filters_list[1], 3, stride=1, act=\"lrelu\"),\n",
    "                BaseConv(filters_list[1], filters_list[0], 1, stride=1, act=\"lrelu\"),\n",
    "            ]\n",
    "        )\n",
    "        return m\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\" forward \"\"\"\n",
    "        outputs = {}\n",
    "        x = self.stem(x)\n",
    "        outputs[\"stem\"] = x\n",
    "        x = self.dark2(x)\n",
    "        outputs[\"dark2\"] = x\n",
    "        x = self.dark3(x)\n",
    "        outputs[\"dark3\"] = x\n",
    "        x = self.dark4(x)\n",
    "        outputs[\"dark4\"] = x\n",
    "        x = self.dark5(x)\n",
    "        outputs[\"dark5\"] = x\n",
    "        return outputs[\"dark3\"], outputs[\"dark4\"], outputs[\"dark5\"]\n",
    "\n",
    "class CSPDarknet(nn.Cell):\n",
    "    \"\"\" Darknet with CSP block for yolox-s m l x\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dep_mul,\n",
    "            wid_mul,\n",
    "            out_features=(\"dark3\", \"dark4\", \"dark5\"),\n",
    "            depthwise=False,\n",
    "            act=\"silu\"\n",
    "    ):\n",
    "        super(CSPDarknet, self).__init__()\n",
    "        assert out_features, \"please provide output features of Darknet\"\n",
    "        self.out_features = out_features\n",
    "        Conv = DWConv if depthwise else BaseConv\n",
    "        base_channels = int(wid_mul * 64)\n",
    "        base_depth = max(round(dep_mul * 3), 1)\n",
    "\n",
    "        # stem\n",
    "        self.stem = Focus(3, base_channels, ksize=3, act=act)\n",
    "\n",
    "        # dark2\n",
    "        self.dark2 = nn.SequentialCell(\n",
    "            Conv(base_channels, base_channels * 2, 3, 2, act=act),\n",
    "            CSPLayer(\n",
    "                base_channels * 2,\n",
    "                base_channels * 2,\n",
    "                n=base_depth,\n",
    "                depthwise=depthwise,\n",
    "                act=act,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # dark3\n",
    "        self.dark3 = nn.SequentialCell(\n",
    "            Conv(base_channels * 2, base_channels * 4, 3, 2, act=act),\n",
    "            CSPLayer(\n",
    "                base_channels * 4,\n",
    "                base_channels * 4,\n",
    "                n=base_depth * 3,\n",
    "                depthwise=depthwise,\n",
    "                act=act,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # dark4\n",
    "        self.dark4 = nn.SequentialCell(\n",
    "            Conv(base_channels * 4, base_channels * 8, 3, 2, act=act),\n",
    "            CSPLayer(\n",
    "                base_channels * 8,\n",
    "                base_channels * 8,\n",
    "                n=base_depth * 3,\n",
    "                depthwise=depthwise,\n",
    "                act=act,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # dark5\n",
    "        self.dark5 = nn.SequentialCell(\n",
    "            Conv(base_channels * 8, base_channels * 16, 3, 2, act=act),\n",
    "            SPPBottleneck(base_channels * 16, base_channels * 16, activation=act),\n",
    "            CSPLayer(\n",
    "                base_channels * 16,\n",
    "                base_channels * 16,\n",
    "                n=base_depth,\n",
    "                shortcut=False,\n",
    "                depthwise=depthwise,\n",
    "                act=act,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\" forward \"\"\"\n",
    "        outputs = {}\n",
    "        x = self.stem(x)\n",
    "        outputs[\"stem\"] = x\n",
    "        x = self.dark2(x)\n",
    "        outputs[\"dark2\"] = x\n",
    "        x = self.dark3(x)\n",
    "        outputs[\"dark3\"] = x\n",
    "        x = self.dark4(x)\n",
    "        outputs[\"dark4\"] = x\n",
    "        x = self.dark5(x)\n",
    "        outputs[\"dark5\"] = x\n",
    "        return outputs[\"dark3\"], outputs[\"dark4\"], outputs[\"dark5\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3.6.3 backbon+neck\n",
    "两种结构，如下图所示：\n",
    "1) YOLOFPN，采用Darknet为backbone，使用yolov3 baseline的Neck结构，都采用FPN结构进行融合\n",
    "\n",
    "2) YOLOPAFPN, 在FPN基础上引入PAN结构\n",
    "<!-- <img src=\"attachment:image.png\" width=\"40%\"> -->\n",
    "<img src=\"./images/3.6.3.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "# YOLOFPN\n",
    "#------------------------#\n",
    "class YOLOFPN(nn.Cell):\n",
    "    \"\"\"\n",
    "    YOLOFPN module, Darknet53 is the default backbone of this model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_w, input_h, depth=53, in_features=None):\n",
    "        super(YOLOFPN, self).__init__()\n",
    "        if in_features is None:\n",
    "            in_features = [\"dark3\", \"dark4\", \"dark5\"]\n",
    "        self.backbone = Darknet(depth)\n",
    "        self.in_features = in_features\n",
    "\n",
    "        # out 1\n",
    "        self.out1_cbl = self._make_cbl(512, 256, 1)\n",
    "        self.out1 = self._make_embedding([256, 512], 512 + 256)\n",
    "\n",
    "        # out 2\n",
    "        self.out2_cbl = self._make_cbl(256, 128, 1)\n",
    "        self.out2 = self._make_embedding([128, 256], 256 + 128)\n",
    "        # upsample\n",
    "        self.upsample0 = P.ResizeNearestNeighbor((input_h // 16, input_w // 16))\n",
    "        self.upsample1 = P.ResizeNearestNeighbor((input_h // 8, input_w // 8))\n",
    "\n",
    "    def _make_cbl(self, _in, _out, ks):\n",
    "        \"\"\" make cbl layer \"\"\"\n",
    "        return BaseConv(_in, _out, ks, stride=1, act=\"lrelu\")\n",
    "\n",
    "    def _make_embedding(self, filters_list, in_filters):\n",
    "        \"\"\" make embedding \"\"\"\n",
    "        m = nn.SequentialCell(\n",
    "            *[\n",
    "                self._make_cbl(in_filters, filters_list[0], 1),\n",
    "                self._make_cbl(filters_list[0], filters_list[1], 3),\n",
    "                self._make_cbl(filters_list[1], filters_list[0], 1),\n",
    "                self._make_cbl(filters_list[0], filters_list[1], 3),\n",
    "                self._make_cbl(filters_list[1], filters_list[0], 1),\n",
    "            ]\n",
    "        )\n",
    "        return m\n",
    "\n",
    "    def construct(self, inputs):\n",
    "        \"\"\" forward \"\"\"\n",
    "        out_features = self.backbone(inputs)\n",
    "        x2, x1, x0 = out_features\n",
    "\n",
    "        #  yolo branch 1\n",
    "        x1_in = self.out1_cbl(x0)\n",
    "        x1_in = self.upsample0(x1_in)\n",
    "        x1_in = P.Concat(axis=1)([x1_in, x1])\n",
    "        out_dark4 = self.out1(x1_in)\n",
    "\n",
    "        #  yolo branch 2\n",
    "        x2_in = self.out2_cbl(out_dark4)\n",
    "        x2_in = self.upsample1(x2_in)\n",
    "        x2_in = P.Concat(axis=1)([x2_in, x2])\n",
    "        out_dark3 = self.out2(x2_in)\n",
    "        outputs = (out_dark3, out_dark4, x0)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "#------------------------#\n",
    "# YOLOPAFPN\n",
    "#------------------------#\n",
    "class YOLOPAFPN(nn.Cell):\n",
    "    \"\"\"\n",
    "    YOLOv3 model. Darknet 53 is the default backbone of this model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_w,\n",
    "            input_h,\n",
    "            depth=1.0,\n",
    "            width=1.0,\n",
    "            in_features=(\"dark3\", \"dark4\", \"dark5\"),\n",
    "            in_channels=None,\n",
    "            depthwise=False,\n",
    "            act=\"silu\"\n",
    "    ):\n",
    "        super(YOLOPAFPN, self).__init__()\n",
    "        if in_channels is None:\n",
    "            in_channels = [256, 512, 1024]\n",
    "        self.input_w = input_w\n",
    "        self.input_h = input_h\n",
    "        self.backbone = CSPDarknet(depth, width, depthwise=depthwise, act=act)\n",
    "        self.in_features = in_features\n",
    "        self.in_channels = in_channels\n",
    "        Conv = DWConv if depthwise else BaseConv\n",
    "\n",
    "        self.upsample0 = P.ResizeNearestNeighbor((input_h // 16, input_w // 16))\n",
    "        self.upsample1 = P.ResizeNearestNeighbor((input_h // 8, input_w // 8))\n",
    "        self.lateral_conv0 = BaseConv(int(in_channels[2] * width), int(in_channels[1] * width), 1, 1, act=act)\n",
    "        self.C3_p4 = CSPLayer(\n",
    "            int(2 * in_channels[1] * width),\n",
    "            int(in_channels[1] * width),\n",
    "            round(3 * depth),\n",
    "            False,\n",
    "            depthwise=depthwise,\n",
    "            act=act\n",
    "        )\n",
    "        self.reduce_conv1 = BaseConv(\n",
    "            int(in_channels[1] * width), int(in_channels[0] * width), 1, 1, act=act\n",
    "        )\n",
    "        self.C3_p3 = CSPLayer(\n",
    "            int(2 * in_channels[0] * width),\n",
    "            int(in_channels[0] * width),\n",
    "            round(3 * depth),\n",
    "            False,\n",
    "            depthwise=depthwise,\n",
    "            act=act,\n",
    "        )\n",
    "        # bottom-up conv\n",
    "        self.bu_conv2 = Conv(\n",
    "            int(in_channels[0] * width), int(in_channels[0] * width), 3, 2, act=act\n",
    "        )\n",
    "        self.C3_n3 = CSPLayer(\n",
    "            int(2 * in_channels[0] * width),\n",
    "            int(in_channels[1] * width),\n",
    "            round(3 * depth),\n",
    "            False,\n",
    "            depthwise=depthwise,\n",
    "            act=act,\n",
    "        )\n",
    "\n",
    "        # bottom-up conv\n",
    "        self.bu_conv1 = Conv(\n",
    "            int(in_channels[1] * width), int(in_channels[1] * width), 3, 2, act=act\n",
    "        )\n",
    "        self.C3_n4 = CSPLayer(\n",
    "            int(2 * in_channels[1] * width),\n",
    "            int(in_channels[2] * width),\n",
    "            round(3 * depth),\n",
    "            False,\n",
    "            depthwise=depthwise,\n",
    "            act=act,\n",
    "        )\n",
    "        self.concat = P.Concat(axis=1)\n",
    "\n",
    "    def construct(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: input images.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor]: FPN feature.\n",
    "        \"\"\"\n",
    "\n",
    "        x2, x1, x0 = self.backbone(inputs)\n",
    "        fpn_out0 = self.lateral_conv0(x0)  # 1024->512  /32\n",
    "        f_out0 = self.upsample0(fpn_out0)  # 512    /16\n",
    "        f_out0 = self.concat((f_out0, x1))  # 512->1024    /16\n",
    "        f_out0 = self.C3_p4(f_out0)  # 1024->512  /16\n",
    "\n",
    "        fpn_out1 = self.reduce_conv1(f_out0)  # 512->256  /16\n",
    "        f_out1 = self.upsample1(fpn_out1)  # 256  /8\n",
    "        f_out1 = self.concat((f_out1, x2))  # 256->512  /8\n",
    "        pan_out2 = self.C3_p3(f_out1)  # 512->256  /16\n",
    "\n",
    "        p_out1 = self.bu_conv2(pan_out2)  # 256->256  /16\n",
    "        p_out1 = self.concat((p_out1, fpn_out1))  # 256->512  /16\n",
    "        pan_out1 = self.C3_n3(p_out1)  # 512->512/16\n",
    "\n",
    "        p_out0 = self.bu_conv1(pan_out1)  # 512->512/32\n",
    "        p_out0 = self.concat((p_out0, fpn_out0))  # 512->1024/32\n",
    "        pan_out0 = self.C3_n4(p_out0)  # 1024->1024/32\n",
    "\n",
    "        return pan_out2, pan_out1, pan_out0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.7 bbox iou计算相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "# bbox iou\n",
    "#------------------------#\n",
    "@constexpr\n",
    "def raise_bbox_error():\n",
    "    raise IndexError(\"Index error, shape of input must be 4!\")\n",
    "\n",
    "def bboxes_iou(bboxes_a, bboxes_b, xyxy=True):\n",
    "    \"\"\"\n",
    "    calculate iou\n",
    "    Args:\n",
    "        bboxes_a:\n",
    "        bboxes_b:\n",
    "        xyxy:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    if bboxes_a.shape[1] != 4 or bboxes_b.shape[1] != 4:\n",
    "        raise_bbox_error()\n",
    "\n",
    "    if xyxy:\n",
    "        tl = P.Maximum()(bboxes_a[:, None, :2], bboxes_b[:, :2])\n",
    "\n",
    "        br = P.Minimum()(bboxes_a[:, None, 2:], bboxes_b[:, 2:])\n",
    "\n",
    "        area_a = bboxes_a[:, 2:] - bboxes_a[:, :2]\n",
    "        area_a = (area_a[:, 0:1] * area_a[:, 1:2]).squeeze(-1)\n",
    "\n",
    "        area_b = bboxes_b[:, 2:] - bboxes_b[:, :2]\n",
    "        area_b = (area_b[:, 0:1] * area_b[:, 1:2]).squeeze(-1)\n",
    "\n",
    "    else:\n",
    "        tl = P.Maximum()(\n",
    "            (bboxes_a[:, None, :2] - bboxes_a[:, None, 2:] / 2),\n",
    "            (bboxes_b[:, :2] - bboxes_b[:, 2:] / 2),\n",
    "        )\n",
    "        br = P.Minimum()(\n",
    "            (bboxes_a[:, None, :2] + bboxes_a[:, None, 2:] / 2),\n",
    "            (bboxes_b[:, :2] + bboxes_b[:, 2:] / 2),\n",
    "        )\n",
    "        area_a = (bboxes_a[:, 2:3] * bboxes_a[:, 3:4]).squeeze(-1)\n",
    "        area_b = (bboxes_b[:, 2:3] * bboxes_b[:, 3:4]).squeeze(-1)\n",
    "    en = (tl < br).astype(tl.dtype)\n",
    "    en = (en[..., 0:1] * en[..., 1:2]).squeeze(-1)\n",
    "    area_i = tl - br\n",
    "    area_i = (area_i[:, :, 0:1] * area_i[:, :, 1:2]).squeeze(-1) * en\n",
    "    return area_i / (area_a[:, None] + area_b - area_i)\n",
    "\n",
    "def batch_bboxes_iou(batch_bboxes_a, batch_bboxes_b, xyxy=True):\n",
    "    \"\"\"\n",
    "        calculate iou for one batch\n",
    "    Args:\n",
    "        batch_bboxes_a:\n",
    "        batch_bboxes_b:\n",
    "        xyxy:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    if batch_bboxes_a.shape[-1] != 4 or batch_bboxes_b.shape[-1] != 4:\n",
    "        raise_bbox_error()\n",
    "    ious = []\n",
    "    for i in range(len(batch_bboxes_a)):\n",
    "        if xyxy:\n",
    "            iou = bboxes_iou(batch_bboxes_a[i], batch_bboxes_b[i], True)\n",
    "        else:\n",
    "            iou = bboxes_iou(batch_bboxes_a[i], batch_bboxes_b[i], False)\n",
    "        iou = P.ExpandDims()(iou, 0)\n",
    "        ious.append(iou)\n",
    "    ious = P.Concat(axis=0)(ious)\n",
    "    return ious\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.8 模型、Loss相关\n",
    "1) DetectionBlock为完整的yolox结构，用于声明后续训练声明网络结构\n",
    "\n",
    "2) yololoss\n",
    "\n",
    "3) ema指数移动平均，对模型权重进行加权平均，使其更加鲁棒\n",
    "\n",
    "### 3.8.1 网络损失函数\n",
    "和网络的预测结果一样，YOLOX网络的损失函数也由三个部分组成，分别是Reg部分、Obj部分和Cls部分。Reg部分是特征点的回归参数判断，Obj部分是特征点是否包含物体判断，Cls部分是特征点包含的物体的种类。\n",
    "\n",
    "在YoloX中，物体的真实框落在哪些特征点内就由该特征点来预测。\n",
    "\n",
    "对于每一个真实框需要求取所有特征点与它的空间位置情况，作为正样本的特征点需要满足以下几个特点：\n",
    "\n",
    "1）特征点落在物体的真实框内；\n",
    "\n",
    "2）特征点距离物体中心尽量要在一定半径内。\n",
    "\n",
    "满足这两点保证了**属于正样本的特征点会落在物体真实框内部，特征点中心与物体真实框中心要相近**。但是这两个条件仅用作正样本的初步筛选，在YoloX中，使用了**SimOTA方法进行动态的正样本数量分配**。\n",
    "\n",
    "在YoloX中，会计算一个Cost代价矩阵，代表每个真实框和每个特征点之间的代价关系，Cost代价矩阵由三个部分组成：\n",
    "\n",
    "1）每个真实框和当前特征点预测框的重合程度；\n",
    "\n",
    "2）每个真实框和当前特征点预测框的种类预测准确度；\n",
    "\n",
    "3）每个真实框的中心是否落在了特征点的一定半径内。\n",
    "\n",
    "**Cost代价矩阵的目的是自适应的找到当前特征点应该去拟合的真实框，重合度越高越需要拟合，分类越准越需要拟合，在一定半径内越需要拟合。**\n",
    "\n",
    "在SimOTA中，**不同目标设定不同的正样本数量(dynamic k)**，以旷视科技官方回答中的蚂蚁和西瓜为例子，传统的正样本分配方案常常为同一场景下的西瓜和蚂蚁分配同样的正样本数，那要么蚂蚁有很多低质量的正样本，要么西瓜仅仅只有一两个正样本，这样的结果对于哪个分配方式都是不合适的。\n",
    "\n",
    "动态的正样本设置的关键在于如何确定k，SimOTA具体的做法是首先计算每个目标Cost最低的10特征点，然后把这十个特征点对应的预测框与真实框的IOU加起来求得最终的k。\n",
    "\n",
    "因此，**SimOTA的过程**总结如下：\n",
    "\n",
    "1）计算每个真实框和当前特征点预测框的重合程度；\n",
    "\n",
    "2）计算将重合度最高的十个预测框与真实框的IOU加起来求得每个真实框的k，也就代表每个真实框有k个特征点与之对应；\n",
    "\n",
    "3）计算每个真实框和当前特征点预测框的种类预测准确度；\n",
    "\n",
    "4）判断真实框的中心是否落在了特征点的一定半径内；\n",
    "\n",
    "5）计算Cost代价矩阵；\n",
    "\n",
    "6）将Cost最低的k个点作为该真实框的正样本。\n",
    "\n",
    "\n",
    "由前文所述可知，YoloX的损失由三个部分组成：\n",
    "\n",
    "1.Reg部分，由SimOTA可以知道每个真实框对应的特征点，获取到每个框对应的特征点后，取出该特征点的预测框，利用真实框和预测框计算IOU损失，作为Reg部分的Loss组成。\n",
    "\n",
    "2.Obj部分，由SimOTA可知道每个真实框对应的特征点，所有真实框对应的特征点都是正样本，剩余的特征点均为负样本，根据正负样本和特征点的是否包含物体的预测结果计算交叉熵损失，作为Obj部分的Loss组成。\n",
    "\n",
    "3.Cls部分，由SimOTA可知道每个真实框对应的特征点，获取到每个框对应的特征点后，取出该特征点的种类预测结果，根据真实框的种类和特征点的种类预测结果计算交叉熵损失，作为Cls部分的Loss组成。\n",
    "\n",
    "其中Cls和Obj部分采用的都是二值交叉熵损失（BCELoss），Reg部分采用的是IoULoss。值得注意的是，Cls和Reg部分只计算正样本的损失，而Obj既计算正样本也计算负样本的损失。\n",
    "<!-- <img src=\"attachment:image.png\" width=\"20%\"> -->\n",
    "<img src=\"./images/3.8.1.png\" width=\"20%\">\n",
    "\n",
    "其中：\n",
    "\n",
    "Lcls代表分类损失，Lreg代表定位损失，Lobj代表obj损失，λ代表定位损失的平衡系数，源码中设置是5.0，Npos代表被分为正样的Anchor Point数。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "# yolox model\n",
    "#------------------------#\n",
    "class DetectionPerFPN(nn.Cell):\n",
    "    \"\"\" head  \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, scale, in_channels=None, act=\"silu\", width=1.0):\n",
    "        super(DetectionPerFPN, self).__init__()\n",
    "        if in_channels is None:\n",
    "            in_channels = [1024, 512, 256]\n",
    "        self.scale = scale\n",
    "        self.num_classes = num_classes\n",
    "        Conv = BaseConv\n",
    "        if scale == 's':\n",
    "            self.stem = BaseConv(in_channels=int(in_channels[0] * width), out_channels=int(256 * width), ksize=1,\n",
    "                                 stride=1, act=act)\n",
    "        elif scale == 'm':\n",
    "            self.stem = BaseConv(in_channels=int(in_channels[1] * width), out_channels=int(256 * width), ksize=1,\n",
    "                                 stride=1, act=act)\n",
    "        elif scale == 'l':\n",
    "            self.stem = BaseConv(in_channels=int(in_channels[2] * width), out_channels=int(256 * width), ksize=1,\n",
    "                                 stride=1, act=act)\n",
    "        else:\n",
    "            raise KeyError(\"Invalid scale value for DetectionBlock\")\n",
    "\n",
    "        self.cls_convs = nn.SequentialCell(\n",
    "            [\n",
    "                Conv(\n",
    "                    in_channels=int(256 * width),\n",
    "                    out_channels=int(256 * width),\n",
    "                    ksize=3,\n",
    "                    stride=1,\n",
    "                    act=act,\n",
    "                ),\n",
    "                Conv(\n",
    "                    in_channels=int(256 * width),\n",
    "                    out_channels=int(256 * width),\n",
    "                    ksize=3,\n",
    "                    stride=1,\n",
    "                    act=act,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.reg_convs = nn.SequentialCell(\n",
    "            [\n",
    "                Conv(\n",
    "                    in_channels=int(256 * width),\n",
    "                    out_channels=int(256 * width),\n",
    "                    ksize=3,\n",
    "                    stride=1,\n",
    "                    act=act,\n",
    "                ),\n",
    "                Conv(\n",
    "                    in_channels=int(256 * width),\n",
    "                    out_channels=int(256 * width),\n",
    "                    ksize=3,\n",
    "                    stride=1,\n",
    "                    act=act,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.cls_preds = nn.Conv2d(in_channels=int(256 * width), out_channels=self.num_classes, kernel_size=1, stride=1,\n",
    "                                   pad_mode=\"pad\", has_bias=True)\n",
    "\n",
    "        self.reg_preds = nn.Conv2d(in_channels=int(256 * width), out_channels=4, kernel_size=1, stride=1,\n",
    "                                   pad_mode=\"pad\",\n",
    "                                   has_bias=True)\n",
    "\n",
    "        self.obj_preds = nn.Conv2d(in_channels=int(256 * width), out_channels=1, kernel_size=1, stride=1,\n",
    "                                   pad_mode=\"pad\",\n",
    "                                   has_bias=True)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\" forward \"\"\"\n",
    "        x = self.stem(x)\n",
    "        cls_x = x\n",
    "        reg_x = x\n",
    "        cls_feat = self.cls_convs(cls_x)\n",
    "\n",
    "        cls_output = self.cls_preds(cls_feat)\n",
    "\n",
    "        reg_feat = self.reg_convs(reg_x)\n",
    "        reg_output = self.reg_preds(reg_feat)\n",
    "        obj_output = self.obj_preds(reg_feat)\n",
    "\n",
    "        return cls_output, reg_output, obj_output\n",
    "\n",
    "class DetectionBlock(nn.Cell):\n",
    "    \"\"\" connect yolox backbone and head \"\"\"\n",
    "\n",
    "    def __init__(self, config, backbone=\"yolopafpn\"):\n",
    "        super(DetectionBlock, self).__init__()\n",
    "        self.num_classes = config.num_classes\n",
    "        self.attr_num = self.num_classes + 5\n",
    "        self.depthwise = config.depth_wise\n",
    "        self.strides = Tensor([8, 16, 32], mindspore.float32)\n",
    "        self.input_size = config.input_size\n",
    "\n",
    "        # network\n",
    "        if backbone == \"yolopafpn\":\n",
    "            self.backbone = YOLOPAFPN(depth=1.33, width=1.25, input_w=self.input_size[1], input_h=self.input_size[0])\n",
    "            self.head_inchannels = [1024, 512, 256]\n",
    "            self.activation = \"silu\"\n",
    "            self.width = 1.25\n",
    "        else:\n",
    "            self.backbone = YOLOFPN(input_w=self.input_size[1], input_h=self.input_size[0])\n",
    "            self.head_inchannels = [512, 256, 128]\n",
    "            self.activation = \"lrelu\"\n",
    "            self.width = 1.0\n",
    "\n",
    "        self.head_l = DetectionPerFPN(in_channels=self.head_inchannels, num_classes=self.num_classes, scale='l',\n",
    "                                      act=self.activation, width=self.width)\n",
    "        self.head_m = DetectionPerFPN(in_channels=self.head_inchannels, num_classes=self.num_classes, scale='m',\n",
    "                                      act=self.activation, width=self.width)\n",
    "        self.head_s = DetectionPerFPN(in_channels=self.head_inchannels, num_classes=self.num_classes, scale='s',\n",
    "                                      act=self.activation, width=self.width)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\" forward \"\"\"\n",
    "        outputs = []\n",
    "        x_l, x_m, x_s = self.backbone(x)\n",
    "        cls_output_l, reg_output_l, obj_output_l = self.head_l(x_l)  # (bs, 80, 80, 80)(bs, 4, 80, 80)(bs, 1, 80, 80)\n",
    "        cls_output_m, reg_output_m, obj_output_m = self.head_m(x_m)  # (bs, 80, 40, 40)(bs, 4, 40, 40)(bs, 1, 40, 40)\n",
    "        cls_output_s, reg_output_s, obj_output_s = self.head_s(x_s)  # (bs, 80, 20, 20)(bs, 4, 20, 20)(bs, 1, 20, 20)\n",
    "        if self.training:\n",
    "            output_l = P.Concat(axis=1)((reg_output_l, obj_output_l, cls_output_l))  # (bs, 85, 80, 80)\n",
    "            output_m = P.Concat(axis=1)((reg_output_m, obj_output_m, cls_output_m))  # (bs, 85, 40, 40)\n",
    "            output_s = P.Concat(axis=1)((reg_output_s, obj_output_s, cls_output_s))  # (bs, 85, 20, 20)\n",
    "\n",
    "            output_l = self.mapping_to_img(output_l, stride=self.strides[0])  # (bs, 6400, 85)x_c, y_c, w, h\n",
    "            output_m = self.mapping_to_img(output_m, stride=self.strides[1])  # (bs, 1600, 85)x_c, y_c, w, h\n",
    "            output_s = self.mapping_to_img(output_s, stride=self.strides[2])  # (bs,  400, 85)x_c, y_c, w, h\n",
    "\n",
    "        else:\n",
    "\n",
    "            output_l = P.Concat(axis=1)(\n",
    "                (reg_output_l, P.Sigmoid()(obj_output_l), P.Sigmoid()(cls_output_l)))  # bs, 85, 80, 80\n",
    "\n",
    "            output_m = P.Concat(axis=1)(\n",
    "                (reg_output_m, P.Sigmoid()(obj_output_m), P.Sigmoid()(cls_output_m)))  # bs, 85, 40, 40\n",
    "\n",
    "            output_s = P.Concat(axis=1)(\n",
    "                (reg_output_s, P.Sigmoid()(obj_output_s), P.Sigmoid()(cls_output_s)))  # bs, 85, 20, 20\n",
    "            output_l = self.mapping_to_img(output_l, stride=self.strides[0])  # (bs, 6400, 85)x_c, y_c, w, h\n",
    "            output_m = self.mapping_to_img(output_m, stride=self.strides[1])  # (bs, 1600, 85)x_c, y_c, w, h\n",
    "            output_s = self.mapping_to_img(output_s, stride=self.strides[2])  # (bs,  400, 85)x_c, y_c, w, h\n",
    "        outputs.append(output_l)\n",
    "        outputs.append(output_m)\n",
    "        outputs.append(output_s)\n",
    "        return P.Concat(axis=1)(outputs)  # batch_size, 8400, 85\n",
    "\n",
    "    def mapping_to_img(self, output, stride):\n",
    "        \"\"\" map to origin image scale for each fpn \"\"\"\n",
    "        batch_size = P.Shape()(output)[0]\n",
    "        n_ch = self.attr_num\n",
    "        grid_size = P.Shape()(output)[2:4]\n",
    "        range_x = range(grid_size[1])\n",
    "        range_y = range(grid_size[0])\n",
    "        stride = P.Cast()(stride, output.dtype)\n",
    "        grid_x = P.Cast()(F.tuple_to_array(range_x), output.dtype)\n",
    "        grid_y = P.Cast()(F.tuple_to_array(range_y), output.dtype)\n",
    "        grid_y = P.ExpandDims()(grid_y, 1)\n",
    "        grid_x = P.ExpandDims()(grid_x, 0)\n",
    "        yv = P.Tile()(grid_y, (1, grid_size[1]))\n",
    "        xv = P.Tile()(grid_x, (grid_size[0], 1))\n",
    "        grid = P.Stack(axis=2)([xv, yv])  # (80, 80, 2)\n",
    "        grid = P.Reshape()(grid, (1, 1, grid_size[0], grid_size[1], 2))  # (1,1,80,80,2)\n",
    "        output = P.Reshape()(output,\n",
    "                             (batch_size, n_ch, grid_size[0], grid_size[1]))  # bs, 6400, 85-->(bs,85,80,80)\n",
    "        output = P.Transpose()(output, (0, 2, 1, 3))  # (bs,85,80,80)-->(bs,80,85,80)\n",
    "        output = P.Transpose()(output, (0, 1, 3, 2))  # (bs,80,85,80)--->(bs, 80, 80, 85)\n",
    "        output = P.Reshape()(output, (batch_size, 1 * grid_size[0] * grid_size[1], -1))  # bs, 6400, 85\n",
    "        grid = P.Reshape()(grid, (1, -1, 2))  # grid(1, 6400, 2)\n",
    "\n",
    "        # reconstruct\n",
    "        output_xy = output[..., :2]\n",
    "        output_xy = (output_xy + grid) * stride\n",
    "        output_wh = output[..., 2:4]\n",
    "        output_wh = P.Exp()(output_wh) * stride\n",
    "        output_other = output[..., 4:]\n",
    "        output_t = P.Concat(axis=-1)([output_xy, output_wh, output_other])\n",
    "        return output_t  # bs, 6400, 85           grid(1, 6400, 2)\n",
    "\n",
    "#------------------------#\n",
    "# yolox Loss\n",
    "#------------------------#\n",
    "class YOLOLossCell(nn.Cell):\n",
    "    \"\"\" yolox with loss cell \"\"\"\n",
    "\n",
    "    def __init__(self, network=None, config=None):\n",
    "        super(YOLOLossCell, self).__init__()\n",
    "        self.network = network\n",
    "        self.n_candidate_k = config.n_candidate_k\n",
    "        self.on_value = Tensor(1.0, mindspore.float32)\n",
    "        self.off_value = Tensor(0.0, mindspore.float32)\n",
    "        self.depth = config.num_classes\n",
    "\n",
    "        self.unsqueeze = P.ExpandDims()\n",
    "        self.reshape = P.Reshape()\n",
    "        self.one_hot = P.OneHot()\n",
    "        self.zeros = P.ZerosLike()\n",
    "        self.sort_ascending = P.Sort(descending=False)\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        self.l1_loss = nn.L1Loss(reduction=\"none\")\n",
    "        self.batch_iter = Tensor(np.arange(0, config.per_batch_size * config.max_gt), mindspore.int32)\n",
    "        self.strides = config.fpn_strides\n",
    "        self.grids = [(config.input_size[0] // _stride) * (config.input_size[1] // _stride) for _stride in\n",
    "                      config.fpn_strides]\n",
    "        self.use_l1 = config.use_l1\n",
    "\n",
    "    def construct(self, img, labels=None, pre_fg_mask=None, is_inbox_and_incenter=None):\n",
    "        \"\"\" forward with loss return \"\"\"\n",
    "        batch_size = P.Shape()(img)[0]\n",
    "        gt_max = P.Shape()(labels)[1]\n",
    "        outputs = self.network(img)  # batch_size, 8400, 85\n",
    "        total_num_anchors = P.Shape()(outputs)[1]\n",
    "        bbox_preds = outputs[:, :, :4]  # batch_size, 8400, 4\n",
    "\n",
    "        obj_preds = outputs[:, :, 4:5]  # batch_size, 8400, 1\n",
    "        cls_preds = outputs[:, :, 5:]  # (batch_size, 8400, 80)\n",
    "\n",
    "        # process label\n",
    "        bbox_true = labels[:, :, 1:]  # (batch_size, gt_max, 4)\n",
    "\n",
    "        gt_classes = F.cast(labels[:, :, 0:1].squeeze(-1), mindspore.int32)\n",
    "        pair_wise_ious = batch_bboxes_iou(bbox_true, bbox_preds, xyxy=False)\n",
    "        pair_wise_ious = pair_wise_ious * pre_fg_mask\n",
    "        pair_wise_iou_loss = -P.Log()(pair_wise_ious + 1e-8) * pre_fg_mask\n",
    "        gt_classes_ = self.one_hot(gt_classes, self.depth, self.on_value, self.off_value)\n",
    "        gt_classes_expaned = ops.repeat_elements(self.unsqueeze(gt_classes_, 2), rep=total_num_anchors, axis=2)\n",
    "        gt_classes_expaned = F.stop_gradient(gt_classes_expaned)\n",
    "\n",
    "        cls_preds_ = P.Sigmoid()(ops.repeat_elements(self.unsqueeze(cls_preds, 1), rep=gt_max, axis=1)) * \\\n",
    "                     P.Sigmoid()(\n",
    "                         ops.repeat_elements(self.unsqueeze(obj_preds, 1), rep=gt_max, axis=1)\n",
    "                     )\n",
    "        pair_wise_cls_loss = P.ReduceSum()(\n",
    "            P.BinaryCrossEntropy(reduction=\"none\")(P.Sqrt()(cls_preds_), gt_classes_expaned, None), -1)\n",
    "        pair_wise_cls_loss = pair_wise_cls_loss * pre_fg_mask\n",
    "        cost = pair_wise_cls_loss + 3.0 * pair_wise_iou_loss\n",
    "        punishment_cost = 1000.0 * (1.0 - F.cast(is_inbox_and_incenter, mindspore.float32))\n",
    "        cost = F.cast(cost + punishment_cost, mindspore.float16)\n",
    "        # dynamic k matching\n",
    "        ious_in_boxes_matrix = pair_wise_ious  # (batch_size, gt_max, 8400)\n",
    "        ious_in_boxes_matrix = F.cast(pre_fg_mask * ious_in_boxes_matrix, mindspore.float16)\n",
    "        topk_ious, _ = P.TopK(sorted=True)(ious_in_boxes_matrix, self.n_candidate_k)\n",
    "\n",
    "        dynamic_ks = P.ReduceSum()(topk_ious, 2).astype(mindspore.int32).clip(xmin=1, xmax=total_num_anchors - 1,\n",
    "                                                                              dtype=mindspore.int32)\n",
    "\n",
    "        # (1, batch_size * gt_max, 2)\n",
    "        dynamic_ks_indices = P.Stack(axis=1)((self.batch_iter, dynamic_ks.reshape((-1,))))\n",
    "\n",
    "        dynamic_ks_indices = F.stop_gradient(dynamic_ks_indices)\n",
    "\n",
    "        values, _ = P.TopK(sorted=True)(-cost, self.n_candidate_k)  # b_s , 50, 8400\n",
    "        values = P.Reshape()(-values, (-1, self.n_candidate_k))\n",
    "        max_neg_score = self.unsqueeze(P.GatherNd()(values, dynamic_ks_indices).reshape(batch_size, -1), 2)\n",
    "        pos_mask = F.cast(cost < max_neg_score, mindspore.float32)  # (batch_size, gt_num, 8400)\n",
    "        pos_mask = pre_fg_mask * pos_mask\n",
    "        # ----dynamic_k---- END-----------------------------------------------------------------------------------------\n",
    "        cost_t = cost * pos_mask + (1.0 - pos_mask) * 2000.\n",
    "        min_index, _ = P.ArgMinWithValue(axis=1)(cost_t)\n",
    "        ret_posk = P.Transpose()(nn.OneHot(depth=gt_max, axis=-1)(min_index), (0, 2, 1))\n",
    "        pos_mask = pos_mask * ret_posk\n",
    "        pos_mask = F.stop_gradient(pos_mask)\n",
    "        # AA problem--------------END ----------------------------------------------------------------------------------\n",
    "\n",
    "        # calculate target ---------------------------------------------------------------------------------------------\n",
    "        # Cast precision\n",
    "        pos_mask = F.cast(pos_mask, mindspore.float16)\n",
    "        bbox_true = F.cast(bbox_true, mindspore.float16)\n",
    "        gt_classes_ = F.cast(gt_classes_, mindspore.float16)\n",
    "\n",
    "        reg_target = P.BatchMatMul(transpose_a=True)(pos_mask, bbox_true)  # (batch_size, 8400, 4)\n",
    "        pred_ious_this_matching = self.unsqueeze(P.ReduceSum()((ious_in_boxes_matrix * pos_mask), 1), -1)\n",
    "        cls_target = P.BatchMatMul(transpose_a=True)(pos_mask, gt_classes_)\n",
    "\n",
    "        cls_target = cls_target * pred_ious_this_matching\n",
    "        obj_target = P.ReduceMax()(pos_mask, 1)  # (batch_size, 8400)\n",
    "\n",
    "        # calculate l1_target\n",
    "        reg_target = F.stop_gradient(reg_target)\n",
    "        cls_target = F.stop_gradient(cls_target)\n",
    "        obj_target = F.stop_gradient(obj_target)\n",
    "        bbox_preds = F.cast(bbox_preds, mindspore.float32)\n",
    "        reg_target = F.cast(reg_target, mindspore.float32)\n",
    "        obj_preds = F.cast(obj_preds, mindspore.float32)\n",
    "        obj_target = F.cast(obj_target, mindspore.float32)\n",
    "        cls_preds = F.cast(cls_preds, mindspore.float32)\n",
    "        cls_target = F.cast(cls_target, mindspore.float32)\n",
    "        loss_l1 = 0.0\n",
    "        if self.use_l1:\n",
    "            l1_target = self.get_l1_format(reg_target)\n",
    "            l1_preds = self.get_l1_format(bbox_preds)\n",
    "            l1_target = F.stop_gradient(l1_target)\n",
    "            l1_target = F.cast(l1_target, mindspore.float32)\n",
    "            l1_preds = F.cast(l1_preds, mindspore.float32)\n",
    "            loss_l1 = P.ReduceSum()(self.l1_loss(l1_preds, l1_target), -1) * obj_target\n",
    "            loss_l1 = P.ReduceSum()(loss_l1)\n",
    "        # calculate target -----------END-------------------------------------------------------------------------------\n",
    "        loss_iou = IOUloss()(P.Reshape()(bbox_preds, (-1, 4)), reg_target).reshape(batch_size, -1) * obj_target\n",
    "        loss_iou = P.ReduceSum()(loss_iou)\n",
    "        loss_obj = self.bce_loss(P.Reshape()(obj_preds, (-1, 1)), P.Reshape()(obj_target, (-1, 1)))\n",
    "        loss_obj = P.ReduceSum()(loss_obj)\n",
    "\n",
    "        loss_cls = P.ReduceSum()(self.bce_loss(cls_preds, cls_target), -1) * obj_target\n",
    "        loss_cls = P.ReduceSum()(loss_cls)\n",
    "        loss_all = (5 * loss_iou + loss_cls + loss_obj + loss_l1) / (P.ReduceSum()(obj_target) + 1e-3)\n",
    "        return loss_all\n",
    "\n",
    "    def get_l1_format_single(self, reg_target, stride, eps):\n",
    "        \"\"\" calculate L1 loss related \"\"\"\n",
    "        reg_target = reg_target / stride\n",
    "        reg_target_xy = reg_target[:, :, :2]\n",
    "        reg_target_wh = reg_target[:, :, 2:]\n",
    "        reg_target_wh = P.Log()(reg_target_wh + eps)\n",
    "        return P.Concat(-1)((reg_target_xy, reg_target_wh))\n",
    "\n",
    "    def get_l1_format(self, reg_target, eps=1e-8):\n",
    "        \"\"\" calculate L1 loss related \"\"\"\n",
    "        reg_target_l = reg_target[:, 0:self.grids[0], :]  # (bs, 6400, 4)\n",
    "        reg_target_m = reg_target[:, self.grids[0]:self.grids[1] + self.grids[0], :]  # (bs, 1600, 4)\n",
    "        reg_target_s = reg_target[:, -self.grids[2]:, :]  # (bs, 400, 4)\n",
    "\n",
    "        reg_target_l = self.get_l1_format_single(reg_target_l, self.strides[0], eps)\n",
    "        reg_target_m = self.get_l1_format_single(reg_target_m, self.strides[1], eps)\n",
    "        reg_target_s = self.get_l1_format_single(reg_target_s, self.strides[2], eps)\n",
    "\n",
    "        l1_target = P.Concat(axis=1)([reg_target_l, reg_target_m, reg_target_s])\n",
    "        return l1_target\n",
    "\n",
    "class IOUloss(nn.Cell):\n",
    "    \"\"\" Iou loss \"\"\"\n",
    "\n",
    "    def __init__(self, reduction=\"none\"):\n",
    "        super(IOUloss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.reshape = P.Reshape()\n",
    "\n",
    "    def construct(self, pred, target):\n",
    "        \"\"\" forward \"\"\"\n",
    "        pred = self.reshape(pred, (-1, 4))\n",
    "        target = self.reshape(target, (-1, 4))\n",
    "        tl = P.Maximum()(pred[:, :2] - pred[:, 2:] / 2, target[:, :2] - target[:, 2:] / 2)\n",
    "        br = P.Minimum()(pred[:, :2] + pred[:, 2:] / 2, target[:, :2] + target[:, 2:] / 2)\n",
    "        area_p = (pred[:, 2:3] * pred[:, 3:4]).squeeze(-1)\n",
    "        area_g = (target[:, 2:3] * target[:, 3:4]).squeeze(-1)\n",
    "        en = F.cast((tl < br), tl.dtype)\n",
    "        en = (en[:, 0:1] * en[:, 1:2]).squeeze(-1)\n",
    "        area_i = br - tl\n",
    "        area_i = (area_i[:, 0:1] * area_i[:, 1:2]).squeeze(-1) * en\n",
    "        area_u = area_p + area_g - area_i\n",
    "\n",
    "        iou = area_i / (area_u + 1e-16)\n",
    "        loss = 1 - iou * iou\n",
    "        if self.reduction == \"mean\":\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            loss = loss.sum()\n",
    "        return loss\n",
    "\n",
    "grad_scale = C.MultitypeFuncGraph(\"grad_scale\")\n",
    "reciprocal = P.Reciprocal()\n",
    "\n",
    "@grad_scale.register(\"Tensor\", \"Tensor\")\n",
    "def tensor_grad_scale(scale, grad):\n",
    "    return grad * reciprocal(scale)\n",
    "\n",
    "_grad_overflow = C.MultitypeFuncGraph(\"_grad_overflow\")\n",
    "grad_overflow = P.FloatStatus()\n",
    "\n",
    "@_grad_overflow.register(\"Tensor\")\n",
    "def _tensor_grad_overflow(grad):\n",
    "    return grad_overflow(grad)\n",
    "\n",
    "#------------------------#\n",
    "#  ema\n",
    "#------------------------#\n",
    "class TrainOneStepWithEMA(nn.TrainOneStepWithLossScaleCell):\n",
    "    \"\"\" Train one step with ema model \"\"\"\n",
    "\n",
    "    def __init__(self, network, optimizer, scale_sense, ema=True, decay=0.9998, updates=0, moving_name=None,\n",
    "                 ema_moving_weight=None):\n",
    "        super(TrainOneStepWithEMA, self).__init__(network, optimizer, scale_sense)\n",
    "        self.ema = ema\n",
    "        self.moving_name = moving_name\n",
    "        self.ema_moving_weight = ema_moving_weight\n",
    "        if self.ema:\n",
    "            self.ema_weight = self.weights.clone(\"ema\", init='same')\n",
    "            self.decay = decay\n",
    "            self.updates = Parameter(Tensor(updates, mindspore.float32))\n",
    "            self.assign = ops.Assign()\n",
    "            self.ema_moving_parameters()\n",
    "\n",
    "    def ema_moving_parameters(self):\n",
    "        self.moving_name = {}\n",
    "        moving_list = []\n",
    "        idx = 0\n",
    "        for key, param in self.network.parameters_and_names():\n",
    "            if \"moving_mean\" in key or \"moving_variance\" in key:\n",
    "                new_param = param.clone()\n",
    "                new_param.name = \"ema.\" + param.name\n",
    "                moving_list.append(new_param)\n",
    "                self.moving_name[\"ema.\" + key] = idx\n",
    "                idx += 1\n",
    "        self.ema_moving_weight = ParameterTuple(moving_list)\n",
    "\n",
    "    def ema_update(self):\n",
    "        \"\"\"Update EMA parameters.\"\"\"\n",
    "        if self.ema:\n",
    "            self.updates += 1\n",
    "            d = self.decay * (1 - ops.Exp()(-self.updates / 2000))\n",
    "            # update trainable parameters\n",
    "            for ema_v, weight in zip(self.ema_weight, self.weights):\n",
    "                tep_v = ema_v * d\n",
    "                self.assign(ema_v, (1.0 - d) * weight + tep_v)\n",
    "        return self.updates\n",
    "\n",
    "    # moving_parameter_update is executed inside the callback(EMACallBack)\n",
    "    def moving_parameter_update(self):\n",
    "        if self.ema:\n",
    "            d = (self.decay * (1 - ops.Exp()(-self.updates / 2000))).asnumpy().item()\n",
    "            # update moving mean and moving var\n",
    "            for key, param in self.network.parameters_and_names():\n",
    "                if \"moving_mean\" in key or \"moving_variance\" in key:\n",
    "                    idx = self.moving_name[\"ema.\" + key]\n",
    "                    moving_weight = param.asnumpy()\n",
    "                    tep_v = self.ema_moving_weight[idx] * d\n",
    "                    ema_value = (1.0 - d) * moving_weight + tep_v\n",
    "                    self.ema_moving_weight[idx] = ema_value\n",
    "\n",
    "    def construct(self, *inputs):\n",
    "        \"\"\" Forward \"\"\"\n",
    "        weights = self.weights\n",
    "        loss = self.network(*inputs)\n",
    "        scaling_sens = self.scale_sense\n",
    "\n",
    "        status, scaling_sens = self.start_overflow_check(loss, scaling_sens)\n",
    "\n",
    "        scaling_sens_filled = C.ones_like(loss) * F.cast(scaling_sens, F.dtype(loss))\n",
    "        grads = self.grad(self.network, weights)(*inputs, scaling_sens_filled)\n",
    "        grads = self.hyper_map(F.partial(grad_scale, scaling_sens), grads)\n",
    "        # apply grad reducer on grads\n",
    "        grads = self.grad_reducer(grads)\n",
    "        self.ema_update()\n",
    "\n",
    "        # get the overflow buffer\n",
    "        cond = self.get_overflow_status(status, grads)\n",
    "        overflow = self.process_loss_scale(cond)\n",
    "        # if there is no overflow, do optimize\n",
    "        if not overflow:\n",
    "            loss = F.depend(loss, self.optimizer(grads))\n",
    "        return loss, cond, scaling_sens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.9 设备函数\n",
    "针对平台设备的相关函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "# device adapter\n",
    "#------------------------#\n",
    "def local_adp_get_device_id():\n",
    "    device_id = os.getenv('DEVICE_ID', '0')\n",
    "    return int(device_id)\n",
    "\n",
    "def local_adp_get_device_num():\n",
    "    device_num = os.getenv('RANK_SIZE', '1')\n",
    "    return int(device_num)\n",
    "\n",
    "def local_adp_get_rank_id():\n",
    "    global_rank_id = os.getenv('RANK_ID', '0')\n",
    "    return int(global_rank_id)\n",
    "\n",
    "def local_adp_get_job_id():\n",
    "    return \"Local Job\"\n",
    "\n",
    "def moxing_adp_get_device_id():\n",
    "    device_id = os.getenv('DEVICE_ID', '0')\n",
    "    return int(device_id)\n",
    "\n",
    "def moxing_adp_get_device_num():\n",
    "    device_num = os.getenv('RANK_SIZE', '1')\n",
    "    return int(device_num)\n",
    "\n",
    "def moxing_adp_get_rank_id():\n",
    "    global_rank_id = os.getenv('RANK_ID', '0')\n",
    "    return int(global_rank_id)\n",
    "\n",
    "def moxing_adp_get_job_id():\n",
    "    job_id = os.getenv('JOB_ID')\n",
    "    job_id = job_id if job_id != \"\" else \"default\"\n",
    "    return job_id\n",
    "\n",
    "def sync_data(from_path, to_path):\n",
    "    \"\"\"\n",
    "    Download data from remote obs to local directory if the first url is remote url and the second one is local path\n",
    "    Upload data from local directory to remote obs in contrast.\n",
    "    \"\"\"\n",
    "    import moxing as mox\n",
    "    global _global_sync_count\n",
    "    sync_lock = \"/tmp/copy_sync.lock\" + str(_global_sync_count)\n",
    "    _global_sync_count += 1\n",
    "\n",
    "    # Each server contains 8 devices as most.\n",
    "    if get_device_id() % min(get_device_num(), 8) == 0 and not os.path.exists(sync_lock):\n",
    "        print(\"from path: \", from_path)\n",
    "        print(\"to path: \", to_path)\n",
    "        mox.file.copy_parallel(from_path, to_path)\n",
    "        print(\"===finish data synchronization===\")\n",
    "        try:\n",
    "            os.mknod(sync_lock)\n",
    "        except IOError:\n",
    "            pass\n",
    "        print(\"===save flag===\")\n",
    "\n",
    "    while True:\n",
    "        if os.path.exists(sync_lock):\n",
    "            break\n",
    "        time.sleep(1)\n",
    "\n",
    "    print(\"Finish sync data from {} to {}.\".format(from_path, to_path))\n",
    "\n",
    "def moxing_wrapper(pre_process=None, post_process=None):\n",
    "    \"\"\"\n",
    "    Moxing wrapper to download dataset and upload outputs.\n",
    "    \"\"\"\n",
    "    def wrapper(run_func):\n",
    "        @functools.wraps(run_func)\n",
    "        def wrapped_func(*args, **kwargs):\n",
    "            # Download data from data_url\n",
    "            if config.enable_modelarts:\n",
    "                if config.data_url:\n",
    "                    sync_data(config.data_url, config.data_path)\n",
    "                    print(\"Dataset downloaded: \", os.listdir(config.data_path))\n",
    "                if config.checkpoint_url:\n",
    "                    sync_data(config.checkpoint_url, config.load_path)\n",
    "                    print(\"Preload downloaded: \", os.listdir(config.load_path))\n",
    "                if config.train_url:\n",
    "                    sync_data(config.train_url, config.output_path)\n",
    "                    print(\"Workspace downloaded: \", os.listdir(config.output_path))\n",
    "\n",
    "                context.set_context(save_graphs_path=os.path.join(config.output_path, str(get_rank_id())))\n",
    "                config.device_num = get_device_num()\n",
    "                config.device_id = get_device_id()\n",
    "                if not os.path.exists(config.output_path):\n",
    "                    os.makedirs(config.output_path)\n",
    "\n",
    "                if pre_process:\n",
    "                    pre_process()\n",
    "\n",
    "            # Run the main function\n",
    "            run_func(*args, **kwargs)\n",
    "\n",
    "            # Upload data to train_url\n",
    "            if config.enable_modelarts:\n",
    "                if post_process:\n",
    "                    post_process()\n",
    "\n",
    "                if config.train_url:\n",
    "                    print(\"Start to copy output directory\")\n",
    "                    sync_data(config.output_path, config.train_url)\n",
    "        return wrapped_func\n",
    "    return wrapper\n",
    "\n",
    "if config.enable_modelarts:\n",
    "    get_device_id = moxing_adp_get_device_id\n",
    "    get_device_num = moxing_adp_get_device_num\n",
    "    get_rank_id = moxing_adp_get_rank_id\n",
    "    get_job_id = moxing_adp_get_job_id\n",
    "else:\n",
    "    get_device_id = local_adp_get_device_id\n",
    "    get_device_num = local_adp_get_device_num\n",
    "    get_rank_id = local_adp_get_rank_id\n",
    "    get_job_id = local_adp_get_job_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.10 训练与评估相关函数\n",
    "1) 针对学习率相关的函数\n",
    "2) 回调机制，保存训练纪录和验证纪录，更新EMA权重\n",
    "3) DetectionEngine与PredictionEngine分别作用于验证和测试模块\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "# lr and callback utils \n",
    "#------------------------#\n",
    "def linear_warmup_lr(current_step, warmup_steps, base_lr, init_lr):\n",
    "    \"\"\"Linear learning rate.\"\"\"\n",
    "    lr_inc = (float(base_lr) - float(init_lr)) / float(warmup_steps)\n",
    "    lr = float(init_lr) + lr_inc * current_step\n",
    "    return lr\n",
    "\n",
    "def warmup_step_lr(lr, lr_epochs, steps_per_epoch, warmup_epochs, max_epoch, gamma=0.1):\n",
    "    \"\"\"Warmup step learning rate.\"\"\"\n",
    "    base_lr = lr\n",
    "    warmup_init_lr = 0\n",
    "    total_steps = int(max_epoch * steps_per_epoch)\n",
    "    warmup_steps = int(warmup_epochs * steps_per_epoch)\n",
    "    milestones = lr_epochs\n",
    "    milestones_steps = []\n",
    "    for milestone in milestones:\n",
    "        milestones_step = milestone * steps_per_epoch\n",
    "        milestones_steps.append(milestones_step)\n",
    "\n",
    "    lr_each_step = []\n",
    "    lr = base_lr\n",
    "    milestones_steps_counter = Counter(milestones_steps)\n",
    "    for i in range(total_steps):\n",
    "        if i < warmup_steps:\n",
    "            lr = linear_warmup_lr(i + 1, warmup_steps, base_lr, warmup_init_lr)\n",
    "        else:\n",
    "            lr = lr * gamma ** milestones_steps_counter[i]\n",
    "        lr_each_step.append(lr)\n",
    "\n",
    "    return np.array(lr_each_step).astype(np.float32)\n",
    "\n",
    "def multi_step_lr(lr, milestones, steps_per_epoch, max_epoch, gamma=0.1):\n",
    "    return warmup_step_lr(lr, milestones, steps_per_epoch, 0, max_epoch, gamma=gamma)\n",
    "\n",
    "def step_lr(lr, epoch_size, steps_per_epoch, max_epoch, gamma=0.1):\n",
    "    lr_epochs = []\n",
    "    for i in range(1, max_epoch):\n",
    "        if i % epoch_size == 0:\n",
    "            lr_epochs.append(i)\n",
    "    return multi_step_lr(lr, lr_epochs, steps_per_epoch, max_epoch, gamma=gamma)\n",
    "\n",
    "def warmup_cosine_annealing_lr(lr, steps_per_epoch, warmup_epochs, max_epoch, t_max, eta_min=0):\n",
    "    \"\"\"Cosine annealing learning rate.\"\"\"\n",
    "    base_lr = lr\n",
    "    warmup_init_lr = 0\n",
    "    total_steps = int(max_epoch * steps_per_epoch)\n",
    "    warmup_steps = int(warmup_epochs * steps_per_epoch)\n",
    "\n",
    "    lr_each_step = []\n",
    "    for i in range(total_steps):\n",
    "        last_epoch = i // steps_per_epoch\n",
    "        if i < warmup_steps:\n",
    "            lr = linear_warmup_lr(i + 1, warmup_steps, base_lr, warmup_init_lr)\n",
    "        else:\n",
    "            lr = eta_min + (base_lr - eta_min) * (1. + math.cos(math.pi * last_epoch / t_max)) / 2\n",
    "        lr_each_step.append(lr)\n",
    "\n",
    "    return np.array(lr_each_step).astype(np.float32)\n",
    "\n",
    "def yolox_warm_cos_lr(\n",
    "        lr,\n",
    "        steps_per_epoch,\n",
    "        warmup_epochs,\n",
    "        max_epoch,\n",
    "        no_aug_epochs,\n",
    "        warmup_lr_start=0,\n",
    "        min_lr_ratio=0.05\n",
    "):\n",
    "    \"\"\"Cosine learning rate with warm up.\"\"\"\n",
    "    base_lr = lr\n",
    "    min_lr = lr * min_lr_ratio\n",
    "    total_iters = int(max_epoch * steps_per_epoch)\n",
    "    warmup_total_iters = int(warmup_epochs * steps_per_epoch)\n",
    "    no_aug_iter = no_aug_epochs * steps_per_epoch\n",
    "    lr_each_step = []\n",
    "    for i in range(total_iters):\n",
    "        if i < warmup_total_iters:\n",
    "            lr = (base_lr - warmup_lr_start) * pow(\n",
    "                (i + 1) / float(warmup_total_iters), 2\n",
    "            ) + warmup_lr_start\n",
    "        elif i >= total_iters - no_aug_iter:\n",
    "            lr = min_lr\n",
    "        else:\n",
    "            lr = min_lr + 0.5 * (base_lr - min_lr) * (1.0 + math.cos(\n",
    "                math.pi * (i - warmup_total_iters) / (total_iters - warmup_total_iters - no_aug_iter)))\n",
    "        lr_each_step.append(lr)\n",
    "    return np.array(lr_each_step).astype(np.float32)\n",
    "\n",
    "def warmup_cosine_annealing_lr_v2(lr, steps_per_epoch, warmup_epochs, max_epoch, t_max, eta_min=0):\n",
    "    \"\"\"Cosine annealing learning rate V2.\"\"\"\n",
    "    base_lr = lr\n",
    "    warmup_init_lr = 0\n",
    "    total_steps = int(max_epoch * steps_per_epoch)\n",
    "    warmup_steps = int(warmup_epochs * steps_per_epoch)\n",
    "\n",
    "    last_lr = 0\n",
    "    last_epoch_v1 = 0\n",
    "\n",
    "    t_max_v2 = int(max_epoch * 1 / 3)\n",
    "\n",
    "    lr_each_step = []\n",
    "    for i in range(total_steps):\n",
    "        last_epoch = i // steps_per_epoch\n",
    "        if i < warmup_steps:\n",
    "            lr = linear_warmup_lr(i + 1, warmup_steps, base_lr, warmup_init_lr)\n",
    "        else:\n",
    "            if i < total_steps * 2 / 3:\n",
    "                lr = eta_min + (base_lr - eta_min) * (1. + math.cos(math.pi * last_epoch / t_max)) / 2\n",
    "                last_lr = lr\n",
    "                last_epoch_v1 = last_epoch\n",
    "            else:\n",
    "                base_lr = last_lr\n",
    "                last_epoch = last_epoch - last_epoch_v1\n",
    "                lr = eta_min + (base_lr - eta_min) * (1. + math.cos(math.pi * last_epoch / t_max_v2)) / 2\n",
    "\n",
    "        lr_each_step.append(lr)\n",
    "    return np.array(lr_each_step).astype(np.float32)\n",
    "\n",
    "def warmup_cosine_annealing_lr_sample(lr, steps_per_epoch, warmup_epochs, max_epoch, t_max, eta_min=0):\n",
    "    \"\"\"Warmup cosine annealing learning rate.\"\"\"\n",
    "    start_sample_epoch = 60\n",
    "    step_sample = 2\n",
    "    tobe_sampled_epoch = 60\n",
    "    end_sampled_epoch = start_sample_epoch + step_sample * tobe_sampled_epoch\n",
    "    max_sampled_epoch = max_epoch + tobe_sampled_epoch\n",
    "    t_max = max_sampled_epoch\n",
    "\n",
    "    base_lr = lr\n",
    "    warmup_init_lr = 0\n",
    "    total_steps = int(max_epoch * steps_per_epoch)\n",
    "    total_sampled_steps = int(max_sampled_epoch * steps_per_epoch)\n",
    "    warmup_steps = int(warmup_epochs * steps_per_epoch)\n",
    "\n",
    "    lr_each_step = []\n",
    "\n",
    "    for i in range(total_sampled_steps):\n",
    "        last_epoch = i // steps_per_epoch\n",
    "        if last_epoch in range(start_sample_epoch, end_sampled_epoch, step_sample):\n",
    "            continue\n",
    "        if i < warmup_steps:\n",
    "            lr = linear_warmup_lr(i + 1, warmup_steps, base_lr, warmup_init_lr)\n",
    "        else:\n",
    "            lr = eta_min + (base_lr - eta_min) * (1. + math.cos(math.pi * last_epoch / t_max)) / 2\n",
    "        lr_each_step.append(lr)\n",
    "\n",
    "    assert total_steps == len(lr_each_step)\n",
    "    return np.array(lr_each_step).astype(np.float32)\n",
    "\n",
    "def yolox_no_aug_lr(base_lr, steps_per_epoch, max_epoch, min_lr_ratio=0.05):\n",
    "    total_iters = int(max_epoch * steps_per_epoch)\n",
    "    lr = base_lr * min_lr_ratio\n",
    "    lr_each_step = []\n",
    "    for _ in range(total_iters):\n",
    "        lr_each_step.append(lr)\n",
    "    return np.array(lr_each_step).astype(np.float32)\n",
    "\n",
    "def get_lr(args):\n",
    "    \"\"\"generate learning rate.\"\"\"\n",
    "    if args.lr_scheduler == 'exponential':\n",
    "        lr = warmup_step_lr(args.lr,\n",
    "                            args.lr_epochs,\n",
    "                            args.steps_per_epoch,\n",
    "                            args.warmup_epochs,\n",
    "                            args.max_epoch,\n",
    "                            gamma=args.lr_gamma,\n",
    "                            )\n",
    "    elif args.lr_scheduler == 'cosine_annealing':\n",
    "        lr = warmup_cosine_annealing_lr(args.lr,\n",
    "                                        args.steps_per_epoch,\n",
    "                                        args.warmup_epochs,\n",
    "                                        args.max_epoch,\n",
    "                                        args.t_max,\n",
    "                                        args.eta_min)\n",
    "    elif args.lr_scheduler == 'cosine_annealing_V2':\n",
    "        lr = warmup_cosine_annealing_lr_v2(args.lr,\n",
    "                                           args.steps_per_epoch,\n",
    "                                           args.warmup_epochs,\n",
    "                                           args.max_epoch,\n",
    "                                           args.t_max,\n",
    "                                           args.eta_min)\n",
    "    elif args.lr_scheduler == 'cosine_annealing_sample':\n",
    "        lr = warmup_cosine_annealing_lr_sample(args.lr,\n",
    "                                               args.steps_per_epoch,\n",
    "                                               args.warmup_epochs,\n",
    "                                               args.max_epoch,\n",
    "                                               args.t_max,\n",
    "                                               args.eta_min)\n",
    "    elif args.lr_scheduler == 'yolox_warm_cos_lr':\n",
    "        lr = yolox_warm_cos_lr(lr=args.lr,\n",
    "                               steps_per_epoch=args.steps_per_epoch,\n",
    "                               warmup_epochs=args.warmup_epochs,\n",
    "                               max_epoch=args.total_epoch,\n",
    "                               no_aug_epochs=args.no_aug_epochs,\n",
    "                               min_lr_ratio=args.min_lr_ratio)\n",
    "    elif args.lr_scheduler == 'no_aug_lr':\n",
    "        lr = yolox_no_aug_lr(\n",
    "            args.lr,\n",
    "            args.steps_per_epoch,\n",
    "            args.max_epoch,\n",
    "            min_lr_ratio=args.min_lr_ratio\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(args.lr_scheduler)\n",
    "    return lr\n",
    "\n",
    "def get_param_groups(network, weight_decay):\n",
    "    \"\"\"Param groups for optimizer.\"\"\"\n",
    "    decay_params = []\n",
    "    no_decay_params = []\n",
    "    for x in network.trainable_params():\n",
    "        parameter_name = x.name\n",
    "        if parameter_name.endswith('.bias'):\n",
    "            # all bias not using weight decay\n",
    "            no_decay_params.append(x)\n",
    "        elif parameter_name.endswith('.gamma'):\n",
    "            # bn weight bias not using weight decay, be carefully for now x not include BN\n",
    "            no_decay_params.append(x)\n",
    "        elif parameter_name.endswith('.beta'):\n",
    "            # bn weight bias not using weight decay, be carefully for now x not include BN\n",
    "            no_decay_params.append(x)\n",
    "        else:\n",
    "            decay_params.append(x)\n",
    "\n",
    "    return [{'params': no_decay_params, 'weight_decay': 0.0}, {'params': decay_params, 'weight_decay': weight_decay}]\n",
    "\n",
    "def load_backbone(net, ckpt_path, args):\n",
    "    \"\"\"Load darknet53 backbone checkpoint.\"\"\"\n",
    "    param_dict = load_checkpoint(ckpt_path)\n",
    "    load_param_into_net(net, param_dict)\n",
    "\n",
    "    param_not_load = []\n",
    "    for _, param in net.parameters_and_names():\n",
    "        if param.name in param_dict:\n",
    "            pass\n",
    "        else:\n",
    "            param_not_load.append(param.name)\n",
    "    args.logger.info(\"not loading param is :\", len(param_not_load))\n",
    "    return net\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=':f', tb_writer=None):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "        self.tb_writer = tb_writer\n",
    "        self.cur_step = 1\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        if self.tb_writer is not None:\n",
    "            self.tb_writer.add_scalar(self.name, self.val, self.cur_step)\n",
    "        self.cur_step += 1\n",
    "\n",
    "    def __str__(self):\n",
    "        print(\"loss update----------------------------------------------------------------------\")\n",
    "        fmtstr = '{name}:{avg' + self.fmt + '}'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "def keep_loss_fp32(network):\n",
    "    \"\"\"Keep loss of network with float32\"\"\"\n",
    "    for _, cell in network.cells_and_names():\n",
    "        if isinstance(cell, (YOLOLossCell,)):\n",
    "            cell.to_float(mstype.float32)\n",
    "\n",
    "class EMACallBack(Callback):\n",
    "\n",
    "    def __init__(self, network, steps_per_epoch, cur_steps=0):\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.cur_steps = cur_steps\n",
    "        self.network = network\n",
    "\n",
    "    def on_train_epoch_begin(self, run_context):\n",
    "        if self.network.ema:\n",
    "            if not isinstance(self.network.ema_moving_weight, list):\n",
    "                tmp_moving = []\n",
    "                for weight in self.network.ema_moving_weight:\n",
    "                    tmp_moving.append(weight.asnumpy())\n",
    "                self.network.ema_moving_weight = tmp_moving\n",
    "\n",
    "    def on_train_step_end(self, run_context):\n",
    "        if self.network.ema:\n",
    "            self.network.moving_parameter_update()\n",
    "            self.cur_steps += 1\n",
    "\n",
    "            if self.cur_steps % self.steps_per_epoch == 0:\n",
    "                if isinstance(self.network.ema_moving_weight, list):\n",
    "                    tmp_moving = []\n",
    "                    moving_name = []\n",
    "                    idx = 0\n",
    "                    for key in self.network.moving_name:\n",
    "                        moving_name.append(key)\n",
    "\n",
    "                    for weight in self.network.ema_moving_weight:\n",
    "                        param = Parameter(Tensor(weight), name=moving_name[idx])\n",
    "                        tmp_moving.append(param)\n",
    "                        idx += 1\n",
    "                    self.network.ema_moving_weight = ParameterTuple(tmp_moving)\n",
    "\n",
    "class YOLOXCB(Callback):\n",
    "    \"\"\"\n",
    "    YOLOX Callback.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logger, step_per_epoch, lr, save_ckpt_path, is_modelart=False, per_print_times=1,\n",
    "                 train_url=None):\n",
    "        super(YOLOXCB, self).__init__()\n",
    "        self.train_url = train_url\n",
    "        if not isinstance(per_print_times, int) or per_print_times < 0:\n",
    "            raise ValueError(\"print_step must be int and >= 0.\")\n",
    "        self._per_print_times = per_print_times\n",
    "        self.lr = lr\n",
    "        self.is_modelarts = is_modelart\n",
    "        self.step_per_epoch = step_per_epoch\n",
    "        self.current_step = 0\n",
    "        self.save_ckpt_path = save_ckpt_path\n",
    "        self.iter_time = time.time()\n",
    "        self.epoch_start_time = time.time()\n",
    "        self.average_loss = []\n",
    "        self.logger = logger\n",
    "\n",
    "    def on_train_epoch_begin(self, run_context):\n",
    "        \"\"\"\n",
    "        Called before each epoch beginning.\n",
    "\n",
    "        Args:\n",
    "            run_context (RunContext): Include some information of the model.\n",
    "        \"\"\"\n",
    "        self.epoch_start_time = time.time()\n",
    "        self.iter_time = time.time()\n",
    "\n",
    "    def on_train_epoch_end(self, run_context):\n",
    "        \"\"\"\n",
    "        Called after each epoch finished.\n",
    "\n",
    "        Args:\n",
    "            run_context (RunContext): Include some information of the model.\n",
    "        \"\"\"\n",
    "        cb_params = run_context.original_args()\n",
    "        cur_epoch = cb_params.cur_epoch_num\n",
    "        loss = cb_params.net_outputs\n",
    "        loss = \"loss: %.4f, overflow: %s, scale: %s\" % (float(loss[0].asnumpy()),\n",
    "                                                        bool(loss[1].asnumpy()),\n",
    "                                                        int(loss[2].asnumpy()))\n",
    "        self.logger.info(\n",
    "            \"epoch: %s epoch time %.2fs %s\" % (cur_epoch, time.time() - self.epoch_start_time, loss))\n",
    "\n",
    "        if self.current_step % (self.step_per_epoch * 1) == 0:\n",
    "            if self.is_modelarts:\n",
    "                import moxing as mox\n",
    "                if self.save_ckpt_path and self.train_url:\n",
    "                    mox.file.copy_parallel(src_url=self.save_ckpt_path, dst_url=self.train_url)\n",
    "                    cur_epoch = self.current_step // self.step_per_epoch\n",
    "                    self.logger.info(\n",
    "                        \"[epoch {}]copy ckpt from{} to {}\".format(self.save_ckpt_path, cur_epoch, self.train_url))\n",
    "\n",
    "    def on_train_step_begin(self, run_context):\n",
    "        \"\"\"\n",
    "        Called before each step beginning.\n",
    "\n",
    "        Args:\n",
    "            run_context (RunContext): Include some information of the model.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_train_step_end(self, run_context):\n",
    "        \"\"\"\n",
    "        Called after each step finished.\n",
    "\n",
    "        Args:\n",
    "            run_context (RunContext): Include some information of the model.\n",
    "        \"\"\"\n",
    "\n",
    "        cur_epoch_step = (self.current_step + 1) % self.step_per_epoch\n",
    "        if cur_epoch_step % self._per_print_times == 0 and cur_epoch_step != 0:\n",
    "            cb_params = run_context.original_args()\n",
    "            cur_epoch = cb_params.cur_epoch_num\n",
    "            loss = cb_params.net_outputs\n",
    "            loss = \"loss: %.4f, overflow: %s, scale: %s\" % (float(loss[0].asnumpy()),\n",
    "                                                            bool(loss[1].asnumpy()),\n",
    "                                                            int(loss[2].asnumpy()))\n",
    "            self.logger.info(\"epoch: %s step: [%s/%s], %s, lr: %.6f, avg step time: %.2f ms\" % (\n",
    "                cur_epoch, cur_epoch_step, self.step_per_epoch, loss, self.lr[self.current_step],\n",
    "                (time.time() - self.iter_time) * 1000 / self._per_print_times))\n",
    "            self.iter_time = time.time()\n",
    "        self.current_step += 1\n",
    "\n",
    "    def on_train_end(self, run_context):\n",
    "        \"\"\"\n",
    "        Called once after network training.\n",
    "\n",
    "        Args:\n",
    "            run_context (RunContext): Include some information of the model.\n",
    "        \"\"\"\n",
    "\n",
    "class EvalCallBack(Callback):\n",
    "    def __init__(self, dataset, test_net, train_net, detection, config, start_epoch=0, interval=1):\n",
    "        self.dataset = dataset\n",
    "        self.network = train_net\n",
    "        self.test_network = test_net\n",
    "        self.detection = detection\n",
    "        self.logger = config.logger\n",
    "        self.start_epoch = start_epoch\n",
    "        self.interval = interval\n",
    "        self.max_epoch = config.max_epoch\n",
    "        self.best_result = 0\n",
    "        self.best_epoch = 0\n",
    "        self.rank = config.rank\n",
    "\n",
    "    def load_ema_parameter(self):\n",
    "        param_dict = {}\n",
    "        for name, param in self.network.parameters_and_names():\n",
    "            if name.startswith(\"ema.\"):\n",
    "                new_name = name.split('ema.')[-1]\n",
    "                param_new = param.clone()\n",
    "                param_new.name = new_name\n",
    "                param_dict[new_name] = param_new\n",
    "        load_param_into_net(self.test_network, param_dict)\n",
    "\n",
    "    def load_network_parameter(self):\n",
    "        param_dict = {}\n",
    "        for name, param in self.network.parameters_and_names():\n",
    "            if name.startswith(\"network.\"):\n",
    "                param_new = param.clone()\n",
    "                param_dict[name] = param_new\n",
    "        load_param_into_net(self.test_network, param_dict)\n",
    "\n",
    "    def epoch_end(self, run_context):\n",
    "        cb_param = run_context.original_args()\n",
    "        cur_epoch = cb_param.cur_epoch_num\n",
    "        if cur_epoch >= self.start_epoch:\n",
    "            if (cur_epoch - self.start_epoch) % self.interval == 0 or cur_epoch == self.max_epoch:\n",
    "                self.load_network_parameter()\n",
    "                self.test_network.set_train(False)\n",
    "                eval_print_str, results = self.inference()\n",
    "                if results >= self.best_result:\n",
    "                    self.best_result = results\n",
    "                    self.best_epoch = cur_epoch\n",
    "                    if os.path.exists('best.ckpt'):\n",
    "                        self.remove_ckpoint_file('best.ckpt')\n",
    "                    save_checkpoint(cb_param.train_network, 'best.ckpt')\n",
    "                    self.logger.info(\"Best result %s at %s epoch\" % (self.best_result, self.best_epoch))\n",
    "                self.logger.info(eval_print_str)\n",
    "                self.logger.info('Ending inference...')\n",
    "\n",
    "    def end(self, run_context):\n",
    "        self.logger.info(\"Best result %s at %s epoch\" % (self.best_result, self.best_epoch))\n",
    "\n",
    "    def inference(self):\n",
    "        self.logger.info('Start inference...')\n",
    "        self.logger.info(\"eval dataset size, %s\" % self.dataset.get_dataset_size())\n",
    "        counts = 0\n",
    "        for data in self.dataset.create_dict_iterator(num_epochs=1):\n",
    "            image = data['image']\n",
    "            img_info = data['image_shape']\n",
    "            img_id = data['img_id']\n",
    "            prediction = self.test_network(image)\n",
    "            prediction = prediction.asnumpy()\n",
    "            img_shape = img_info.asnumpy()\n",
    "            img_id = img_id.asnumpy()\n",
    "            counts = counts + 1\n",
    "            self.detection.detection(prediction, img_shape, img_id)\n",
    "            self.logger.info('Calculating mAP...%s' % counts)\n",
    "\n",
    "        self.logger.info('Calculating mAP...%s' % counts)\n",
    "        result_file_path = self.detection.evaluate_prediction()\n",
    "        self.logger.info('result file path: %s', result_file_path)\n",
    "        eval_result, results = self.detection.get_eval_result()\n",
    "        if eval_result is not None and results is not None:\n",
    "            eval_print_str = '\\n=============coco eval result=========\\n' + eval_result\n",
    "            return eval_print_str, results\n",
    "        return None, 0\n",
    "\n",
    "    def remove_ckpoint_file(self, file_name):\n",
    "        \"\"\"Remove the specified checkpoint file from this checkpoint manager and also from the directory.\"\"\"\n",
    "        try:\n",
    "            os.chmod(file_name, stat.S_IWRITE)\n",
    "            os.remove(file_name)\n",
    "        except OSError:\n",
    "            self.logger.info(\"OSError, failed to remove the older ckpt file %s.\", file_name)\n",
    "        except ValueError:\n",
    "            self.logger.info(\"ValueError, failed to remove the older ckpt file %s.\", file_name)\n",
    "\n",
    "class Redirct:\n",
    "    def __init__(self):\n",
    "        self.content = \"\"\n",
    "\n",
    "    def write(self, content):\n",
    "        self.content += content\n",
    "\n",
    "    def flush(self):\n",
    "        self.content = \"\"\n",
    "\n",
    "class DetectionEngine:\n",
    "    \"\"\" Detection engine \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.input_size = self.config.input_size\n",
    "        self.strides = self.config.fpn_strides  # [8, 16, 32]\n",
    "\n",
    "        self.expanded_strides = None\n",
    "        self.grids = None\n",
    "\n",
    "        self.num_classes = config.num_classes\n",
    "\n",
    "        self.conf_thre = config.conf_thre\n",
    "        self.nms_thre = config.nms_thre\n",
    "        self.annFile = os.path.join(config.data_dir, 'annotations/instances_val2017.json')\n",
    "        self._coco = COCO(self.annFile)\n",
    "        self._img_ids = list(sorted(self._coco.imgs.keys()))\n",
    "        self.coco_catIds = self._coco.getCatIds()\n",
    "        self.save_prefix = config.outputs_dir\n",
    "        self.file_path = ''\n",
    "\n",
    "        self.data_list = []\n",
    "\n",
    "    def detection(self, outputs, img_shape, img_ids):\n",
    "        # post process nms\n",
    "        outputs = self.postprocess(outputs, self.num_classes, self.conf_thre, self.nms_thre)\n",
    "        self.data_list.extend(self.convert_to_coco_format(outputs, info_imgs=img_shape, ids=img_ids))\n",
    "\n",
    "    def postprocess(self, prediction, num_classes, conf_thre=0.7, nms_thre=0.45, class_agnostic=False):\n",
    "        \"\"\" nms \"\"\"\n",
    "        box_corner = np.zeros_like(prediction)\n",
    "        box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n",
    "        box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n",
    "        box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n",
    "        box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n",
    "        prediction[:, :, :4] = box_corner[:, :, :4]\n",
    "        output = [None for _ in range(len(prediction))]\n",
    "        for i, image_pred in enumerate(prediction):\n",
    "            if not image_pred.shape[0]:\n",
    "                continue\n",
    "            # Get score and class with highest confidence\n",
    "            class_conf = np.max(image_pred[:, 5:5 + num_classes], axis=-1)  # (8400)\n",
    "            class_pred = np.argmax(image_pred[:, 5:5 + num_classes], axis=-1)  # (8400)\n",
    "            conf_mask = (image_pred[:, 4] * class_conf >= conf_thre).squeeze()  # (8400)\n",
    "            class_conf = np.expand_dims(class_conf, axis=-1)  # (8400, 1)\n",
    "            class_pred = np.expand_dims(class_pred, axis=-1).astype(np.float16)  # (8400, 1)\n",
    "            # Detections ordered as (x1, y1, x2, y2, obj_conf, class_conf, class_pred)\n",
    "            detections = np.concatenate((image_pred[:, :5], class_conf, class_pred), axis=1)\n",
    "            detections = detections[conf_mask]\n",
    "            if not detections.shape[0]:\n",
    "                continue\n",
    "            if class_agnostic:\n",
    "                nms_out_index = self._nms(detections[:, :4], detections[:, 4] * detections[:, 5], nms_thre)\n",
    "            else:\n",
    "                nms_out_index = self._batch_nms(detections[:, :4], detections[:, 4] * detections[:, 5],\n",
    "                                                detections[:, 6], nms_thre)\n",
    "            detections = detections[nms_out_index]\n",
    "            if output[i] is None:\n",
    "                output[i] = detections\n",
    "            else:\n",
    "                output[i] = np.concatenate((output[i], detections))\n",
    "        return output\n",
    "\n",
    "    def _nms(self, xyxys, scores, threshold):\n",
    "        \"\"\"Calculate NMS\"\"\"\n",
    "        x1 = xyxys[:, 0]\n",
    "        y1 = xyxys[:, 1]\n",
    "        x2 = xyxys[:, 2]\n",
    "        y2 = xyxys[:, 3]\n",
    "        scores = scores\n",
    "        areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "        order = scores.argsort()[::-1]\n",
    "        reserved_boxes = []\n",
    "        while order.size > 0:\n",
    "            i = order[0]\n",
    "            reserved_boxes.append(i)\n",
    "            max_x1 = np.maximum(x1[i], x1[order[1:]])\n",
    "            max_y1 = np.maximum(y1[i], y1[order[1:]])\n",
    "            min_x2 = np.minimum(x2[i], x2[order[1:]])\n",
    "            min_y2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "            intersect_w = np.maximum(0.0, min_x2 - max_x1 + 1)\n",
    "            intersect_h = np.maximum(0.0, min_y2 - max_y1 + 1)\n",
    "            intersect_area = intersect_w * intersect_h\n",
    "\n",
    "            ovr = intersect_area / (areas[i] + areas[order[1:]] - intersect_area)\n",
    "            indexes = np.where(ovr <= threshold)[0]\n",
    "            order = order[indexes + 1]\n",
    "        return reserved_boxes\n",
    "\n",
    "    def _batch_nms(self, xyxys, scores, idxs, threshold, use_offset=True):\n",
    "        \"\"\"Calculate Nms based on class info,Each index value correspond to a category,\n",
    "        and NMS will not be applied between elements of different categories.\"\"\"\n",
    "        if use_offset:\n",
    "            max_coordinate = xyxys.max()\n",
    "            offsets = idxs * (max_coordinate + np.array([1]))\n",
    "            boxes_for_nms = xyxys + offsets[:, None]\n",
    "            keep = self._nms(boxes_for_nms, scores, threshold)\n",
    "            return keep\n",
    "        keep_mask = np.zeros_like(scores, dtype=np.bool_)\n",
    "        for class_id in np.unique(idxs):\n",
    "            curr_indices = np.where(idxs == class_id)[0]\n",
    "            curr_keep_indices = self._nms(xyxys[curr_indices], scores[curr_indices], threshold)\n",
    "            keep_mask[curr_indices[curr_keep_indices]] = True\n",
    "        keep_indices = np.where(keep_mask)[0]\n",
    "        return keep_indices[np.argsort(-scores[keep_indices])]\n",
    "\n",
    "    def convert_to_coco_format(self, outputs, info_imgs, ids):\n",
    "        \"\"\" convert to coco format \"\"\"\n",
    "        data_list = []\n",
    "        for (output, img_h, img_w, img_id) in zip(\n",
    "                outputs, info_imgs[:, 0], info_imgs[:, 1], ids\n",
    "        ):\n",
    "            if output is None:\n",
    "                continue\n",
    "            bboxes = output[:, 0:4]\n",
    "            scale = min(\n",
    "                self.input_size[0] / float(img_h), self.input_size[1] / float(img_w)\n",
    "            )\n",
    "\n",
    "            bboxes = bboxes / scale\n",
    "            bboxes[:, [0, 2]] = np.clip(bboxes[:, [0, 2]], 0, img_w)\n",
    "            bboxes[:, [1, 3]] = np.clip(bboxes[:, [1, 3]], 0, img_h)\n",
    "            bboxes = xyxy2xywh(bboxes)\n",
    "\n",
    "            cls = output[:, 6]\n",
    "            scores = output[:, 4] * output[:, 5]\n",
    "            for ind in range(bboxes.shape[0]):\n",
    "                label = self.coco_catIds[int(cls[ind])]\n",
    "                pred_data = {\n",
    "                    \"image_id\": int(img_id),\n",
    "                    \"category_id\": label,\n",
    "                    \"bbox\": bboxes[ind].tolist(),\n",
    "                    \"score\": scores[ind].item(),\n",
    "                    \"segmentation\": [],\n",
    "                }  # COCO json format\n",
    "                data_list.append(pred_data)\n",
    "        return data_list\n",
    "\n",
    "    def evaluate_prediction(self):\n",
    "        \"\"\" generate prediction coco json file \"\"\"\n",
    "        print('Evaluate in main process...')\n",
    "        # write result to coco json format\n",
    "\n",
    "        t = datetime.datetime.now().strftime('_%Y_%m_%d_%H_%M_%S')\n",
    "        try:\n",
    "            self.file_path = self.save_prefix + '/predict' + t + '.json'\n",
    "            f = open(self.file_path, 'w')\n",
    "            json.dump(self.data_list, f)\n",
    "        except IOError as e:\n",
    "            raise RuntimeError(\"Unable to open json file to dump. What():{}\".format(str(e)))\n",
    "        else:\n",
    "            f.close()\n",
    "            if not self.data_list:\n",
    "                self.file_path = ''\n",
    "                return self.file_path\n",
    "\n",
    "            self.data_list.clear()\n",
    "            return self.file_path\n",
    "\n",
    "    def get_eval_result(self):\n",
    "        \"\"\"Get eval result\"\"\"\n",
    "        if not self.file_path:\n",
    "            return None, None\n",
    "\n",
    "        cocoGt = self._coco\n",
    "        cocoDt = cocoGt.loadRes(self.file_path)\n",
    "        cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "        cocoEval.evaluate()\n",
    "        cocoEval.accumulate()\n",
    "        rdct = Redirct()\n",
    "        stdout = sys.stdout\n",
    "        sys.stdout = rdct\n",
    "        cocoEval.summarize()\n",
    "        sys.stdout = stdout\n",
    "        return rdct.content, cocoEval.stats[0]\n",
    "\n",
    "class PredictionEngine:\n",
    "    def __init__(self, config):\n",
    "        self.input_size = config.input_size\n",
    "\n",
    "        self.num_classes = config.num_classes\n",
    "\n",
    "        self.conf_thre = config.pred_conf_thre\n",
    "        self.nms_thre = config.pred_nms_thre\n",
    "        \n",
    "        self.class_names = self.get_classes(config.classes_path)\n",
    "\n",
    "        hsv_tuples = [(x / self.num_classes, 1., 1.) for x in range(self.num_classes)]\n",
    "        self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "        self.colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), self.colors))\n",
    "\n",
    "\n",
    "    def prediction(self, outputs, image_shape):\n",
    "        outputs = self.postprocess(outputs, self.num_classes, self.conf_thre, self.nms_thre)\n",
    "        \n",
    "        if outputs[0] is None:\n",
    "            return None\n",
    "\n",
    "        top_label = outputs[0][:, 6].astype('int32')\n",
    "        top_conf = outputs[0][:, 4] * outputs[0][:, 5]\n",
    "        top_boxes = outputs[0][:, :4]\n",
    "        scale = min(self.input_size[0] / float(image_shape[0]), self.input_size[1] / float(image_shape[1]))\n",
    "        top_boxes = top_boxes / scale\n",
    "        top_boxes[:, [0, 2]] = np.clip(top_boxes[:, [0, 2]], 0, image_shape[1])\n",
    "        top_boxes[:, [1, 3]] = np.clip(top_boxes[:, [1, 3]], 0, image_shape[0])\n",
    "\n",
    "        info_mask = np.zeros((image_shape[0], image_shape[1], 3))\n",
    "        for i, c in list(enumerate(top_label)):\n",
    "            label_name = self.class_names[int(c)-1]#id start with 1\n",
    "            box = top_boxes[i]\n",
    "            score = top_conf[i]\n",
    "\n",
    "            left, top, right, bottom = box\n",
    "            top     = max(0, np.floor(top).astype('int32'))\n",
    "            left    = max(0, np.floor(left).astype('int32'))\n",
    "            bottom  = min(image_shape[1], np.floor(bottom).astype('int32'))\n",
    "            right   = min(image_shape[0], np.floor(right).astype('int32'))\n",
    "            cv2.rectangle(info_mask, (left, top), (right, bottom), self.colors[int(c)-1], 1)\n",
    "            text = \"{}: {:.4f}\".format(label_name, score) \n",
    "            cv2.putText(info_mask, text, (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.colors[int(c)-1], 1)\n",
    "        return info_mask\n",
    "\n",
    "    def postprocess(self, prediction, num_classes, conf_thre=0.7, nms_thre=0.45, class_agnostic=False):\n",
    "        \"\"\" nms \"\"\"\n",
    "        box_corner = np.zeros_like(prediction)\n",
    "        box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n",
    "        box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n",
    "        box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n",
    "        box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n",
    "        prediction[:, :, :4] = box_corner[:, :, :4]\n",
    "        output = [None for _ in range(len(prediction))]\n",
    "        for i, image_pred in enumerate(prediction):\n",
    "            if not image_pred.shape[0]:\n",
    "                continue\n",
    "            # Get score and class with highest confidence\n",
    "            class_conf = np.max(image_pred[:, 5:5 + num_classes], axis=-1)  # (8400)\n",
    "            class_pred = np.argmax(image_pred[:, 5:5 + num_classes], axis=-1)  # (8400)\n",
    "            conf_mask = (image_pred[:, 4] * class_conf >= conf_thre).squeeze()  # (8400)\n",
    "            class_conf = np.expand_dims(class_conf, axis=-1)  # (8400, 1)\n",
    "            class_pred = np.expand_dims(class_pred, axis=-1).astype(np.float16)  # (8400, 1)\n",
    "            # Detections ordered as (x1, y1, x2, y2, obj_conf, class_conf, class_pred)\n",
    "            detections = np.concatenate((image_pred[:, :5], class_conf, class_pred), axis=1)\n",
    "            detections = detections[conf_mask]\n",
    "            if not detections.shape[0]:\n",
    "                continue\n",
    "            if class_agnostic:\n",
    "                nms_out_index = self._nms(detections[:, :4], detections[:, 4] * detections[:, 5], nms_thre)\n",
    "            else:\n",
    "                nms_out_index = self._batch_nms(detections[:, :4], detections[:, 4] * detections[:, 5],\n",
    "                                                detections[:, 6], nms_thre)\n",
    "            detections = detections[nms_out_index]\n",
    "            if output[i] is None:\n",
    "                output[i] = detections\n",
    "            else:\n",
    "                output[i] = np.concatenate((output[i], detections))\n",
    "        return output\n",
    "\n",
    "    def _nms(self, xyxys, scores, threshold):\n",
    "        \"\"\"Calculate NMS\"\"\"\n",
    "        x1 = xyxys[:, 0]\n",
    "        y1 = xyxys[:, 1]\n",
    "        x2 = xyxys[:, 2]\n",
    "        y2 = xyxys[:, 3]\n",
    "        scores = scores\n",
    "        areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "        order = scores.argsort()[::-1]\n",
    "        reserved_boxes = []\n",
    "        while order.size > 0:\n",
    "            i = order[0]\n",
    "            reserved_boxes.append(i)\n",
    "            max_x1 = np.maximum(x1[i], x1[order[1:]])\n",
    "            max_y1 = np.maximum(y1[i], y1[order[1:]])\n",
    "            min_x2 = np.minimum(x2[i], x2[order[1:]])\n",
    "            min_y2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "            intersect_w = np.maximum(0.0, min_x2 - max_x1 + 1)\n",
    "            intersect_h = np.maximum(0.0, min_y2 - max_y1 + 1)\n",
    "            intersect_area = intersect_w * intersect_h\n",
    "\n",
    "            ovr = intersect_area / (areas[i] + areas[order[1:]] - intersect_area)\n",
    "            indexes = np.where(ovr <= threshold)[0]\n",
    "            order = order[indexes + 1]\n",
    "        return reserved_boxes\n",
    "\n",
    "    def _batch_nms(self, xyxys, scores, idxs, threshold, use_offset=True):\n",
    "        \"\"\"Calculate Nms based on class info,Each index value correspond to a category,\n",
    "        and NMS will not be applied between elements of different categories.\"\"\"\n",
    "        if use_offset:\n",
    "            max_coordinate = xyxys.max()\n",
    "            offsets = idxs * (max_coordinate + np.array([1]))\n",
    "            boxes_for_nms = xyxys + offsets[:, None]\n",
    "            keep = self._nms(boxes_for_nms, scores, threshold)\n",
    "            return keep\n",
    "        keep_mask = np.zeros_like(scores, dtype=np.bool_)\n",
    "        for class_id in np.unique(idxs):\n",
    "            curr_indices = np.where(idxs == class_id)[0]\n",
    "            curr_keep_indices = self._nms(xyxys[curr_indices], scores[curr_indices], threshold)\n",
    "            keep_mask[curr_indices[curr_keep_indices]] = True\n",
    "        keep_indices = np.where(keep_mask)[0]\n",
    "        return keep_indices[np.argsort(-scores[keep_indices])]\n",
    "    \n",
    "    def get_classes(self, classes_path):\n",
    "        with open(classes_path, encoding='utf-8') as f:\n",
    "            class_names = f.readlines()\n",
    "        class_names = [c.strip() for c in class_names]\n",
    "        return class_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.11 网络权重初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "# network initialized \n",
    "#------------------------#\n",
    "def calculate_gain(nonlinearity, param=None):\n",
    "    r\"\"\"Return the recommended gain value for the given nonlinearity function.\n",
    "    The values are as follows:\n",
    "\n",
    "    ================= ====================================================\n",
    "    nonlinearity      gain\n",
    "    ================= ====================================================\n",
    "    Linear / Identity :math:`1`\n",
    "    Conv{1,2,3}D      :math:`1`\n",
    "    Sigmoid           :math:`1`\n",
    "    Tanh              :math:`\\frac{5}{3}`\n",
    "    ReLU              :math:`\\sqrt{2}`\n",
    "    Leaky Relu        :math:`\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}`\n",
    "    ================= ====================================================\n",
    "\n",
    "    Args:\n",
    "        nonlinearity: the non-linear function (`nn.functional` name)\n",
    "        param: optional parameter for the non-linear function\n",
    "\n",
    "    Examples:\n",
    "        >>> gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2\n",
    "    \"\"\"\n",
    "    linear_fns = ['linear', 'conv1d', 'conv2d', 'conv3d', 'conv_transpose1d', 'conv_transpose2d', 'conv_transpose3d']\n",
    "    if nonlinearity in linear_fns or nonlinearity == 'sigmoid':\n",
    "        return 1\n",
    "    if nonlinearity == 'tanh':\n",
    "        return 5.0 / 3\n",
    "    if nonlinearity == 'relu':\n",
    "        return math.sqrt(2.0)\n",
    "    if nonlinearity == 'leaky_relu':\n",
    "        if param is None:\n",
    "            negative_slope = 0.01\n",
    "        elif not isinstance(param, bool) and isinstance(param, int) or isinstance(param, float):\n",
    "            # True/False are instances of int, hence check above\n",
    "            negative_slope = param\n",
    "        else:\n",
    "            raise ValueError(\"negative_slope {} not a valid number\".format(param))\n",
    "        return math.sqrt(2.0 / (1 + negative_slope ** 2))\n",
    "\n",
    "    raise ValueError(\"Unsupported nonlinearity {}\".format(nonlinearity))\n",
    "\n",
    "def _assignment(arr, num):\n",
    "    \"\"\"Assign the value of 'num' and 'arr'.\"\"\"\n",
    "    if arr.shape == ():\n",
    "        arr = arr.reshape((1))\n",
    "        arr[:] = num\n",
    "        arr = arr.reshape(())\n",
    "    else:\n",
    "        if isinstance(num, np.ndarray):\n",
    "            arr[:] = num[:]\n",
    "        else:\n",
    "            arr[:] = num\n",
    "    return arr\n",
    "\n",
    "def _calculate_correct_fan(array, mode):\n",
    "    mode = mode.lower()\n",
    "    valid_modes = ['fan_in', 'fan_out']\n",
    "    if mode not in valid_modes:\n",
    "        raise ValueError(\"Mode {} not supported, please use one of {}\".format(mode, valid_modes))\n",
    "\n",
    "    fan_in, fan_out = _calculate_fan_in_and_fan_out(array)\n",
    "    return fan_in if mode == 'fan_in' else fan_out\n",
    "\n",
    "def kaiming_uniform_(arr, a=0, mode='fan_in', nonlinearity='leaky_relu'):\n",
    "    r\"\"\"Fills the input `Tensor` with values according to the method\n",
    "    described in `Delving deep into rectifiers: Surpassing human-level\n",
    "    performance on ImageNet classification` - He, K. et al. (2015), using a\n",
    "    uniform distribution. The resulting tensor will have values sampled from\n",
    "    :math:`\\mathcal{U}(-\\text{bound}, \\text{bound})` where\n",
    "\n",
    "    .. math::\n",
    "        \\text{bound} = \\text{gain} \\times \\sqrt{\\frac{3}{\\text{fan\\_mode}}}\n",
    "\n",
    "    Also known as He initialization.\n",
    "\n",
    "    Args:\n",
    "        tensor: an n-dimensional `Tensor`\n",
    "        a: the negative slope of the rectifier used after this layer (only\n",
    "        used with ``'leaky_relu'``)\n",
    "        mode: either ``'fan_in'`` (default) or ``'fan_out'``. Choosing ``'fan_in'``\n",
    "            preserves the magnitude of the variance of the weights in the\n",
    "            forward pass. Choosing ``'fan_out'`` preserves the magnitudes in the\n",
    "            backwards pass.\n",
    "        nonlinearity: the non-linear function (`nn.functional` name),\n",
    "            recommended to use only with ``'relu'`` or ``'leaky_relu'`` (default).\n",
    "\n",
    "    Examples:\n",
    "        >>> w = np.empty(3, 5)\n",
    "        >>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')\n",
    "    \"\"\"\n",
    "    fan = _calculate_correct_fan(arr, mode)\n",
    "    gain = calculate_gain(nonlinearity, a)\n",
    "    std = gain / math.sqrt(fan)\n",
    "    bound = math.sqrt(3.0) * std  # Calculate uniform bounds from standard deviation\n",
    "    return np.random.uniform(-bound, bound, arr.shape)\n",
    "\n",
    "def _calculate_fan_in_and_fan_out(arr):\n",
    "    \"\"\"Calculate fan in and fan out.\"\"\"\n",
    "    dimensions = len(arr.shape)\n",
    "    if dimensions < 2:\n",
    "        raise ValueError(\"Fan in and fan out can not be computed for array with fewer than 2 dimensions\")\n",
    "\n",
    "    num_input_fmaps = arr.shape[1]\n",
    "    num_output_fmaps = arr.shape[0]\n",
    "    receptive_field_size = 1\n",
    "    if dimensions > 2:\n",
    "        receptive_field_size = reduce(lambda x, y: x * y, arr.shape[2:])\n",
    "    fan_in = num_input_fmaps * receptive_field_size\n",
    "    fan_out = num_output_fmaps * receptive_field_size\n",
    "\n",
    "    return fan_in, fan_out\n",
    "\n",
    "class KaimingUniform(MeInitializer):\n",
    "    \"\"\"Kaiming uniform initializer.\"\"\"\n",
    "\n",
    "    def __init__(self, a=0, mode='fan_in', nonlinearity='leaky_relu'):\n",
    "        super(KaimingUniform, self).__init__()\n",
    "        self.a = a\n",
    "        self.mode = mode\n",
    "        self.nonlinearity = nonlinearity\n",
    "\n",
    "    def _initialize(self, arr):\n",
    "        tmp = kaiming_uniform_(arr, self.a, self.mode, self.nonlinearity)\n",
    "        _assignment(arr, tmp)\n",
    "\n",
    "def default_recurisive_init(custom_cell, prior_prob=1e-2):\n",
    "    \"\"\"Initialize parameter.\"\"\"\n",
    "    for _, cell in custom_cell.cells_and_names():\n",
    "        if isinstance(cell, nn.Conv2d):\n",
    "            cell.weight.set_data(initializer.initializer(KaimingUniform(a=math.sqrt(5)),\n",
    "                                                  cell.weight.shape,\n",
    "                                                  cell.weight.dtype))\n",
    "            if cell.bias is not None:\n",
    "                fan_in, _ = _calculate_fan_in_and_fan_out(cell.weight)\n",
    "                bound = 1 / math.sqrt(fan_in)\n",
    "                cell.bias.set_data(initializer.initializer(initializer.Uniform(bound),\n",
    "                                                    cell.bias.shape,\n",
    "                                                    cell.bias.dtype))\n",
    "                if \"cls_preds\" in cell.bias.name or \"obj_preds\" in cell.bias.name:\n",
    "                    cell.bias.set_data(initializer.initializer(-math.log((1 - prior_prob) / prior_prob), cell.bias.shape,\n",
    "                                                        cell.bias.dtype))\n",
    "        elif isinstance(cell, nn.Dense):\n",
    "            cell.weight.set_data(initializer.initializer(KaimingUniform(a=math.sqrt(5)),\n",
    "                                                  cell.weight.shape,\n",
    "                                                  cell.weight.dtype))\n",
    "            if cell.bias is not None:\n",
    "                fan_in, _ = _calculate_fan_in_and_fan_out(cell.weight)\n",
    "                bound = 1 / math.sqrt(fan_in)\n",
    "                cell.bias.set_data(initializer.initializer(initializer.Uniform(bound),\n",
    "                                                    cell.bias.shape,\n",
    "                                                    cell.bias.dtype))\n",
    "        elif isinstance(cell, (nn.BatchNorm2d, nn.BatchNorm1d, nn.SyncBatchNorm)):\n",
    "            cell.momentum = 0.97\n",
    "            cell.eps = 0.001\n",
    "        else:\n",
    "            pass\n",
    "        initialize_head_biases(custom_cell, prior_prob=0.01)\n",
    "\n",
    "def initialize_head_biases(network, prior_prob):\n",
    "    for name, cell in network.cells_and_names():\n",
    "        if name.endswith(\"cls_preds\") or name.endswith(\"obj_preds\"):\n",
    "            cell.bias.set_data(initializer.initializer(-math.log((1 - prior_prob) / prior_prob), cell.bias.shape,\n",
    "                                                cell.bias.dtype))\n",
    "\n",
    "def load_yolox_params(args, network):\n",
    "    \"\"\"Load yolox darknet parameter from checkpoint.\"\"\"\n",
    "    if args.pretrained_backbone:\n",
    "        network = load_backbone(network, args.pretrained_backbone, args)\n",
    "        args.logger.info('load pre-trained backbone {} into network'.format(args.pretrained_backbone))\n",
    "    else:\n",
    "        args.logger.info('Not load pre-trained backbone, please be careful')\n",
    "\n",
    "def load_resume_params(args, network):\n",
    "    if args.resume_yolox:\n",
    "        args.logger.info('Start to load resume parameters...')\n",
    "        network = load_backbone(network, args.resume_yolox, args)\n",
    "        args.logger.info('resume finished')\n",
    "        args.logger.info('load_model {} success'.format(args.resume_yolox))\n",
    "    else:\n",
    "        args.logger.info('Not load resume!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.12 训练相关函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "# train func\n",
    "#------------------------#\n",
    "set_seed(888)\n",
    "\n",
    "def set_default():\n",
    "    \"\"\" set default \"\"\"\n",
    "    if config.enable_modelarts:\n",
    "        config.data_root = os.path.join(config.data_dir, 'coco2017/train2017')\n",
    "        config.annFile = os.path.join(config.data_dir, 'coco2017/annotations')\n",
    "        outputs_dir = os.path.join(config.outputs_dir, config.ckpt_path)\n",
    "    else:\n",
    "        config.data_root = os.path.join(config.data_dir, 'train2017')\n",
    "        config.annFile = os.path.join(config.data_dir, 'annotations/instances_train2017.json')\n",
    "        outputs_dir = config.ckpt_path\n",
    "\n",
    "    # logger\n",
    "\n",
    "    config.outputs_dir = os.path.join(outputs_dir, datetime.datetime.now().strftime('%Y-%m-%d_time_%H_%M_%S'))\n",
    "    config.logger = get_logger(config.outputs_dir, config.rank)\n",
    "    config.logger.save_args(config)\n",
    "\n",
    "def set_graph_kernel_context():\n",
    "    if context.get_context(\"device_target\") == \"GPU\":\n",
    "        context.set_context(enable_graph_kernel=True)\n",
    "        context.set_context(graph_kernel_flags=\"--enable_parallel_fusion \"\n",
    "                                               \"--enable_trans_op_optimize \"\n",
    "                                               \"--disable_cluster_ops=ReduceMax,Reshape \"\n",
    "                                               \"--enable_expand_ops=Conv2D\")\n",
    "\n",
    "def network_init(cfg):\n",
    "    \"\"\" Network init \"\"\"\n",
    "    device_id = int(os.getenv('DEVICE_ID', '0'))\n",
    "    context.set_context(mode=context.GRAPH_MODE,\n",
    "                        device_target=cfg.device_target, save_graphs=cfg.save_graphs, device_id=device_id,\n",
    "                        save_graphs_path=\"ir_path\")\n",
    "    set_graph_kernel_context()\n",
    "\n",
    "    profiler = None\n",
    "    if cfg.need_profiler:\n",
    "        profiling_dir = os.path.join(cfg.outputs_dir,\n",
    "                                     datetime.datetime.now().strftime('%Y-%m-%d_time_%H_%M_%S'))\n",
    "        profiler = Profiler(output_path=profiling_dir, is_detail=True, is_show_op_path=True)\n",
    "\n",
    "    # init distributed\n",
    "    cfg.use_syc_bn = False\n",
    "    if cfg.is_distributed:\n",
    "        cfg.use_syc_bn = True\n",
    "        init()\n",
    "        cfg.rank = get_rank()\n",
    "        cfg.group_size = get_group_size()\n",
    "        context.reset_auto_parallel_context()\n",
    "        context.set_auto_parallel_context(parallel_mode=ParallelMode.DATA_PARALLEL, gradients_mean=True,\n",
    "                                          device_num=cfg.group_size)\n",
    "\n",
    "    # select for master rank save ckpt or all rank save, compatible for model parallel\n",
    "    cfg.rank_save_ckpt_flag = 0\n",
    "    if cfg.is_save_on_master:\n",
    "        if cfg.rank == 0:\n",
    "            cfg.rank_save_ckpt_flag = 1\n",
    "    else:\n",
    "        cfg.rank_save_ckpt_flag = 1\n",
    "\n",
    "    # logger\n",
    "    cfg.outputs_dir = os.path.join(cfg.ckpt_path,\n",
    "                                   datetime.datetime.now().strftime('%Y-%m-%d_time_%H_%M_%S'))\n",
    "    cfg.logger = get_logger(cfg.outputs_dir, cfg.rank)\n",
    "    cfg.logger.save_args(cfg)\n",
    "    return profiler\n",
    "\n",
    "def parallel_init(args):\n",
    "    context.reset_auto_parallel_context()\n",
    "    parallel_mode = ParallelMode.STAND_ALONE\n",
    "    degree = 1\n",
    "    if args.is_distributed:\n",
    "        parallel_mode = ParallelMode.DATA_PARALLEL\n",
    "        degree = get_group_size()\n",
    "    context.set_auto_parallel_context(parallel_mode=parallel_mode, gradients_mean=True, device_num=degree)\n",
    "\n",
    "def modelarts_pre_process():\n",
    "    '''modelarts pre process function.'''\n",
    "\n",
    "    def unzip(zip_file, save_dir):\n",
    "        import zipfile\n",
    "        s_time = time.time()\n",
    "        if not os.path.exists(os.path.join(save_dir, config.modelarts_dataset_unzip_name)):\n",
    "            zip_isexist = zipfile.is_zipfile(zip_file)\n",
    "            if zip_isexist:\n",
    "                fz = zipfile.ZipFile(zip_file, 'r')\n",
    "                data_num = len(fz.namelist())\n",
    "                print(\"Extract Start...\")\n",
    "                print(\"unzip file num: {}\".format(data_num))\n",
    "                data_print = int(data_num / 100) if data_num > 100 else 1\n",
    "                i = 0\n",
    "                for file in fz.namelist():\n",
    "                    if i % data_print == 0:\n",
    "                        print(\"unzip percent: {}%\".format(int(i * 100 / data_num)), flush=True)\n",
    "                    i += 1\n",
    "                    fz.extract(file, save_dir)\n",
    "                print(\"cost time: {}min:{}s.\".format(int((time.time() - s_time) / 60),\n",
    "                                                     int(int(time.time() - s_time) % 60)))\n",
    "                print(\"Extract Done.\")\n",
    "            else:\n",
    "                print(\"This is not zip.\")\n",
    "        else:\n",
    "            print(\"Zip has been extracted.\")\n",
    "\n",
    "    if config.need_modelarts_dataset_unzip:\n",
    "        zip_file_1 = os.path.join(config.data_path, config.modelarts_dataset_unzip_name + \".zip\")\n",
    "        save_dir_1 = os.path.join(config.data_path)\n",
    "\n",
    "        sync_lock = \"/tmp/unzip_sync.lock\"\n",
    "\n",
    "        # Each server contains 8 devices as most.\n",
    "        if get_device_id() % min(get_device_num(), 8) == 0 and not os.path.exists(sync_lock):\n",
    "            print(\"Zip file path: \", zip_file_1)\n",
    "            print(\"Unzip file save dir: \", save_dir_1)\n",
    "            unzip(zip_file_1, save_dir_1)\n",
    "            print(\"===Finish extract data synchronization===\")\n",
    "            try:\n",
    "                os.mknod(sync_lock)\n",
    "            except IOError:\n",
    "                pass\n",
    "\n",
    "        while True:\n",
    "            if os.path.exists(sync_lock):\n",
    "                break\n",
    "            time.sleep(1)\n",
    "\n",
    "        print(\"Device: {}, Finish sync unzip data from {} to {}.\".format(get_device_id(), zip_file_1, save_dir_1))\n",
    "\n",
    "    config.ckpt_path = os.path.join(config.output_path, config.ckpt_path)\n",
    "\n",
    "def parser_init():\n",
    "    parser = argparse.ArgumentParser(description='Yolox train.')\n",
    "    parser.add_argument('--data_url', required=False, default=None, help='Location of data.')\n",
    "    parser.add_argument('--train_url', required=False, default=None, help='Location of training outputs.')\n",
    "    parser.add_argument('--backbone', required=False, default=\"yolox_darknet53\")\n",
    "    parser.add_argument('--min_lr_ratio', required=False, default=0.05)\n",
    "    parser.add_argument('--data_aug', required=False, default=True)\n",
    "    return parser\n",
    "\n",
    "def get_val_dataset():\n",
    "    val_root = os.path.join(config.data_dir, 'val2017')\n",
    "    ann_file = os.path.join(config.data_dir, 'annotations/instances_val2017.json')\n",
    "    ds_test = create_yolox_dataset(val_root, ann_file, is_training=False, batch_size=config.per_batch_size,\n",
    "                                   device_num=config.group_size,\n",
    "                                   rank=config.rank)\n",
    "    config.logger.info(\"Finish loading the val dataset!\")\n",
    "    return ds_test\n",
    "\n",
    "def get_optimizer(cfg, network, lr):\n",
    "    param_group = get_param_groups(network, cfg.weight_decay)\n",
    "    if cfg.opt == \"SGD\":\n",
    "        from mindspore.nn import SGD\n",
    "        opt = SGD(params=param_group, learning_rate=Tensor(lr), momentum=config.momentum, nesterov=True)\n",
    "        cfg.logger.info(\"Use SGD Optimizer\")\n",
    "    else:\n",
    "        from mindspore.nn import Momentum\n",
    "        opt = Momentum(params=param_group,\n",
    "                       learning_rate=Tensor(lr),\n",
    "                       momentum=cfg.momentum,\n",
    "                       use_nesterov=True)\n",
    "        cfg.logger.info(\"Use Momentum Optimizer\")\n",
    "    return opt\n",
    "\n",
    "def load_resume_checkpoint(cfg, network, ckpt_path):\n",
    "    param_dict = load_checkpoint(ckpt_path)\n",
    "\n",
    "    ema_train_weight = []\n",
    "    ema_moving_weight = []\n",
    "    param_load = {}\n",
    "    for key, param in param_dict.items():\n",
    "        if key.startswith(\"network.\") or key.startswith(\"moments.\"):\n",
    "            param_load[key] = param\n",
    "        elif \"updates\" in key:\n",
    "            cfg.updates = param\n",
    "            network.updates = cfg.updates\n",
    "            config.logger.info(\"network_ema updates:%s\" % network.updates.asnumpy().item())\n",
    "    load_param_into_net(network, param_load)\n",
    "\n",
    "    for key, param in network.parameters_and_names():\n",
    "        if key.startswith(\"ema.\") and \"moving_mean\" not in key and \"moving_variance\" not in key:\n",
    "            ema_train_weight.append(param_dict[key])\n",
    "        elif key.startswith(\"ema.\") and (\"moving_mean\" in key or \"moving_variance\" in key):\n",
    "            ema_moving_weight.append(param_dict[key])\n",
    "\n",
    "    if network.ema:\n",
    "        if ema_train_weight and ema_moving_weight:\n",
    "            network.ema_weight = ParameterTuple(ema_train_weight)\n",
    "            network.ema_moving_weight = ParameterTuple(ema_moving_weight)\n",
    "            config.logger.info(\"successful loading ema weights\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4 运行\n",
    "### 4.1 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@moxing_wrapper(pre_process=modelarts_pre_process)\n",
    "def run_train(train_stage='stage_1', profiler=None):\n",
    "    \"\"\" Launch Train process \"\"\"\n",
    "    parser = parser_init()\n",
    "    args_opt, _ = parser.parse_known_args()\n",
    "    if not config.data_aug:  # Train the last no data augment epochs\n",
    "        config.use_l1 = True  # Add L1 loss\n",
    "        config.max_epoch = config.total_epoch - config.max_epoch\n",
    "        config.lr_scheduler = \"no_aug_lr\"  # fix the min lr for last no data aug epochs\n",
    "    if config.enable_modelarts:\n",
    "        import moxing as mox\n",
    "        local_data_url = os.path.join(config.data_path, str(config.rank))\n",
    "        local_annFile = os.path.join(config.data_path, str(config.rank))\n",
    "        mox.file.copy_parallel(config.data_root, local_data_url)\n",
    "        config.data_dir = os.path.join(config.data_path, 'coco2017')\n",
    "        mox.file.copy_parallel(config.annFile, local_annFile)\n",
    "        config.annFile = os.path.join(local_data_url, 'instances_train2017.json')\n",
    "    if config.backbone == \"yolox_darknet53\":\n",
    "        backbone = \"yolofpn\"\n",
    "    else:\n",
    "        backbone = \"yolopafpn\"\n",
    "    base_network = DetectionBlock(config, backbone=backbone)\n",
    "    if config.pretrained:\n",
    "        base_network = load_backbone(base_network, config.pretrained, config)\n",
    "    config.logger.info('Training backbone is: %s' % config.backbone)\n",
    "    if config.use_syc_bn:\n",
    "        config.logger.info(\"Using Synchronized batch norm layer...\")\n",
    "        use_syc_bn(base_network)\n",
    "    default_recurisive_init(base_network)\n",
    "    config.logger.info(\"Network weights have been initialized...\")\n",
    "    network = YOLOLossCell(base_network, config)\n",
    "    config.logger.info('Finish getting network...')\n",
    "    config.data_root = os.path.join(config.data_dir, 'train2017')\n",
    "    config.annFile = os.path.join(config.data_dir, 'annotations/instances_train2017.json')\n",
    "    ds = create_yolox_dataset(image_dir=config.data_root, anno_path=config.annFile, batch_size=config.per_batch_size,\n",
    "                              device_num=config.group_size, rank=config.rank, data_aug=config.data_aug)\n",
    "    ds_test = get_val_dataset()\n",
    "    config.logger.info('Finish loading training dataset! batch size:%s' % config.per_batch_size)\n",
    "    config.steps_per_epoch = ds.get_dataset_size()\n",
    "    config.logger.info('%s steps for one epoch.' % config.steps_per_epoch)\n",
    "    if config.ckpt_interval <= 0:\n",
    "        config.ckpt_interval = 1\n",
    "    lr = get_lr(config)\n",
    "    config.logger.info(\"Learning rate scheduler:%s, base_lr:%s, min lr ratio:%s\" % (config.lr_scheduler, config.lr,\n",
    "                                                                                    config.min_lr_ratio))\n",
    "    opt = get_optimizer(config, network, lr)\n",
    "    loss_scale_manager = DynamicLossScaleManager(init_loss_scale=2 ** 22)\n",
    "    update_cell = loss_scale_manager.get_update_cell()\n",
    "    network_ema = TrainOneStepWithEMA(network, opt, update_cell,\n",
    "                                      ema=True, decay=0.9998, updates=config.updates).set_train()\n",
    "    if config.resume_yolox:\n",
    "        resume_steps = config.updates.asnumpy().items()\n",
    "        config.resume_epoch = resume_steps // config.steps_per_epoch\n",
    "        lr = lr[resume_steps:]\n",
    "        opt = get_optimizer(config, network, lr)\n",
    "        network_ema = TrainOneStepWithEMA(network, opt, update_cell,\n",
    "                                          ema=True, decay=0.9998, updates=resume_steps).set_train()\n",
    "        load_resume_checkpoint(config, network_ema, config.resume_yolox)\n",
    "    if not config.data_aug:\n",
    "        if os.path.isfile(config.yolox_no_aug_ckpt):  # Loading the resume checkpoint for the last no data aug epochs\n",
    "            load_resume_checkpoint(config, network_ema, config.yolox_no_aug_ckpt)\n",
    "            config.logger.info(\"Finish load the resume checkpoint, begin to train the last...\")\n",
    "        else:\n",
    "            raise FileNotFoundError('{} not exist or not a pre-trained file'.format(config.yolox_no_aug_ckpt))\n",
    "    config.logger.info(\"Add ema model\")\n",
    "    model = Model(network_ema, amp_level=\"O0\")\n",
    "    cb = []\n",
    "    save_ckpt_path = None\n",
    "    if config.rank_save_ckpt_flag:\n",
    "        cb.append(EMACallBack(network_ema, config.steps_per_epoch))\n",
    "        ckpt_config = CheckpointConfig(save_checkpoint_steps=config.steps_per_epoch * config.ckpt_interval,\n",
    "                                       keep_checkpoint_max=config.ckpt_max_num)\n",
    "        save_ckpt_path = os.path.join(config.outputs_dir, 'ckpt_' + str(config.rank) + '/' + train_stage + '/')\n",
    "        cb.append(ModelCheckpoint(config=ckpt_config, directory=save_ckpt_path, prefix='{}'.format(config.backbone)))\n",
    "    cb.append(YOLOXCB(config.logger, config.steps_per_epoch, lr=lr, save_ckpt_path=save_ckpt_path,\n",
    "                      is_modelart=config.enable_modelarts,\n",
    "                      per_print_times=config.log_interval, train_url=args_opt.train_url))\n",
    "    if config.run_eval:\n",
    "        test_block = DetectionBlock(config, backbone=backbone)\n",
    "        cb.append(\n",
    "            EvalCallBack(ds_test, test_block, network_ema, DetectionEngine(config), config,\n",
    "                         interval=config.eval_interval))\n",
    "    if config.need_profiler:\n",
    "        model.train(3, ds, callbacks=cb, dataset_sink_mode=True, sink_size=config.log_interval)\n",
    "        profiler.analyse()\n",
    "    else:\n",
    "        config.logger.info(\"Epoch number:%s\" % config.max_epoch)\n",
    "        config.logger.info(\"All steps number:%s\" % (config.max_epoch * config.steps_per_epoch))\n",
    "        config.logger.info(\"==================Start Training \" + train_stage + \"=========================\")\n",
    "        model.train(config.max_epoch, ds, callbacks=cb, dataset_sink_mode=False, sink_size=-1)\n",
    "    config.logger.info(\"==================Training END \" + train_stage + \"======================\")\n",
    "    mindspore.save_checkpoint(network_ema, os.path.join(config.outputs_dir, 'ckpt_' + str(config.rank) + '/' + train_stage + '/' + train_stage+'_final.ckpt'))\n",
    "    config.yolox_no_aug_ckpt = os.path.join(config.outputs_dir, 'ckpt_' + str(config.rank) + '/' + train_stage + '/' + train_stage+'_final.ckpt')\n",
    "    config.val_ckpt = os.path.join(config.outputs_dir, 'ckpt_' + str(config.rank) + '/' + train_stage + '/' + train_stage+'_final.ckpt')\n",
    "    config.pred_ckpt = os.path.join(config.outputs_dir, 'ckpt_' + str(config.rank) + '/' + train_stage + '/' + train_stage+'_final.ckpt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.2 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "# eval func\n",
    "#------------------------#\n",
    "def run_eval():\n",
    "    \"\"\"The function of eval\"\"\"\n",
    "    config.data_root = os.path.join(config.data_dir, 'val2017')\n",
    "    config.annFile = os.path.join(config.data_dir, 'annotations/instances_val2017.json')\n",
    "\n",
    "    # logger\n",
    "    config.outputs_dir = os.path.join(\n",
    "        config.log_path, datetime.datetime.now().strftime('%Y-%m-%d_time_%H_%M_%S')\n",
    "    )\n",
    "    rank_id = int(os.getenv('RANK_ID', '0'))\n",
    "    config.logger = get_logger(config.outputs_dir, rank_id)\n",
    "\n",
    "    context.reset_auto_parallel_context()\n",
    "    parallel_mode = ParallelMode.STAND_ALONE\n",
    "    context.set_auto_parallel_context(parallel_mode=parallel_mode, gradients_mean=True, device_num=1)\n",
    "    # ------------------network create----------------------------------------------------------------------------\n",
    "    config.logger.info('Begin Creating Network....')\n",
    "    if config.backbone == \"yolox_darknet53\":\n",
    "        backbone = \"yolofpn\"\n",
    "    else:\n",
    "        backbone = \"yolopafpn\"\n",
    "    network = DetectionBlock(config, backbone=backbone)  # default yolo-darknet53\n",
    "    default_recurisive_init(network)\n",
    "    config.logger.info(config.val_ckpt)\n",
    "    if os.path.isfile(config.val_ckpt):\n",
    "        param_dict = load_checkpoint(config.val_ckpt)\n",
    "        ema_param_dict = {}\n",
    "        for param in param_dict:\n",
    "            if param.startswith(\"ema.\"):\n",
    "                new_name = param.split(\"ema.\")[1]\n",
    "                data = param_dict[param]\n",
    "                data.name = new_name\n",
    "                ema_param_dict[new_name] = data\n",
    "\n",
    "        load_param_into_net(network, ema_param_dict)\n",
    "        config.logger.info('load model %s success', config.val_ckpt)\n",
    "    else:\n",
    "        config.logger.info('%s doesn''t exist or is not a pre-trained file', config.val_ckpt)\n",
    "        raise FileNotFoundError('{} not exist or not a pre-trained file'.format(config.val_ckpt))\n",
    "    data_root = config.data_root\n",
    "    anno_file = config.annFile\n",
    "    ds = create_yolox_dataset(data_root, anno_file, is_training=False, batch_size=config.per_batch_size, device_num=1,\n",
    "                              rank=rank_id)\n",
    "    data_size = ds.get_dataset_size()\n",
    "    config.logger.info(\n",
    "        'Finish loading the dataset, totally %s images to eval, iters %s' % (data_size * config.per_batch_size, \\\n",
    "                                                                                 data_size))\n",
    "    network.set_train(False)\n",
    "    # init detection engine\n",
    "    detection = DetectionEngine(config)\n",
    "    config.logger.info('Start inference...')\n",
    "    for _, data in enumerate(\n",
    "            tqdm(ds.create_dict_iterator(num_epochs=1), total=data_size,\n",
    "                 colour=\"GREEN\")):\n",
    "        image = data['image']\n",
    "        img_info = data['image_shape']\n",
    "        img_id = data['img_id']\n",
    "        prediction = network(image)\n",
    "        prediction = prediction.asnumpy()\n",
    "        img_shape = img_info.asnumpy()\n",
    "        img_id = img_id.asnumpy()\n",
    "        detection.detection(prediction, img_shape, img_id)\n",
    "\n",
    "    config.logger.info('Calculating mAP...')\n",
    "    result_file_path = detection.evaluate_prediction()\n",
    "    config.logger.info('result file path: %s', result_file_path)\n",
    "    eval_result, _ = detection.get_eval_result()\n",
    "    eval_print_str = '\\n=============coco eval result=========\\n' + eval_result\n",
    "    config.logger.info(eval_print_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.3 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "# pred func(to be fixed)\n",
    "#------------------------#\n",
    "def run_pred():\n",
    "    if not os.path.exists(config.pred_output):\n",
    "        os.makedirs(config.pred_output)\n",
    "\n",
    "    context.reset_auto_parallel_context()\n",
    "    parallel_mode = ParallelMode.STAND_ALONE\n",
    "    context.set_auto_parallel_context(parallel_mode=parallel_mode, gradients_mean=True, device_num=1)\n",
    "    if config.backbone == \"yolox_darknet53\":\n",
    "        backbone = \"yolofpn\"\n",
    "    else:\n",
    "        backbone = \"yolopafpn\"\n",
    "    network = DetectionBlock(config, backbone=backbone) \n",
    "    default_recurisive_init(network)\n",
    "\n",
    "    if os.path.isfile(config.pred_ckpt):\n",
    "        param_dict = load_checkpoint(config.pred_ckpt)\n",
    "        ema_param_dict = {}\n",
    "        for param in param_dict:\n",
    "            if param.startswith(\"ema.\"):\n",
    "                new_name = param.split(\"ema.\")[1]\n",
    "                data = param_dict[param]\n",
    "                data.name = new_name\n",
    "                ema_param_dict[new_name] = data\n",
    "\n",
    "        load_param_into_net(network, ema_param_dict)\n",
    "    else:\n",
    "        raise FileNotFoundError('{} not exist or not a pre-trained file'.format(config.pred_ckpt))\n",
    "\n",
    "    pred_transform = ValTransform(legacy=False)\n",
    "    \n",
    "    data_list = os.listdir(config.pred_input)\n",
    "    prediction_engine = PredictionEngine(config=config)\n",
    "    network.set_train(False)\n",
    "    for image_name in tqdm(data_list):\n",
    "        image_path = os.path.join(config.pred_input, image_name)\n",
    "        image = np.array(cv2.imread(image_path))\n",
    "        r = min(config.input_size[0] / image.shape[0], config.input_size[1] / image.shape[1])\n",
    "        image_data = cv2.resize(\n",
    "            image,\n",
    "            (int(image.shape[1] * r), int(image.shape[0] * r)),\n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "        ).astype(np.float32)\n",
    "        image_data, _ = pred_transform(image_data, config.input_size)\n",
    "        image_data = np.expand_dims(image_data,0)\n",
    "        image_data = Tensor(image_data)\n",
    "        output = network(image_data).asnumpy()\n",
    "        \n",
    "        mask = prediction_engine.prediction(output, image.shape).astype(image.dtype)\n",
    "\n",
    "        if not mask is None:\n",
    "            pred_image = cv2.addWeighted(image,1,mask,0.3,0)\n",
    "            cv2.imwrite(os.path.join(config.pred_output, image_name), pred_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-10 14:21:38,101:INFO:Args:\n",
      "2022-11-10 14:21:38,102:INFO:--> backbone: yolox_darknet53\n",
      "2022-11-10 14:21:38,103:INFO:--> data_aug: True\n",
      "2022-11-10 14:21:38,104:INFO:--> device_target: Ascend\n",
      "2022-11-10 14:21:38,105:INFO:--> outputs_dir: ./save_weights/2022-11-10_time_14_21_38\n",
      "2022-11-10 14:21:38,105:INFO:--> save_graphs: False\n",
      "2022-11-10 14:21:38,106:INFO:--> lr_scheduler: yolox_warm_cos_lr\n",
      "2022-11-10 14:21:38,107:INFO:--> max_epoch: 10\n",
      "2022-11-10 14:21:38,107:INFO:--> total_epoch: 15\n",
      "2022-11-10 14:21:38,108:INFO:--> data_dir: test_coco\n",
      "2022-11-10 14:21:38,108:INFO:--> yolox_no_aug_ckpt: \n",
      "2022-11-10 14:21:38,109:INFO:--> need_profiler: 0\n",
      "2022-11-10 14:21:38,110:INFO:--> pretrained: None\n",
      "2022-11-10 14:21:38,110:INFO:--> resume_yolox: None\n",
      "2022-11-10 14:21:38,111:INFO:--> flip_prob: 0.5\n",
      "2022-11-10 14:21:38,112:INFO:--> hsv_prob: 1.0\n",
      "2022-11-10 14:21:38,112:INFO:--> per_batch_size: 2\n",
      "2022-11-10 14:21:38,113:INFO:--> depth_wise: False\n",
      "2022-11-10 14:21:38,114:INFO:--> max_gt: 120\n",
      "2022-11-10 14:21:38,114:INFO:--> num_classes: 3\n",
      "2022-11-10 14:21:38,115:INFO:--> input_size: [640, 640]\n",
      "2022-11-10 14:21:38,116:INFO:--> fpn_strides: [8, 16, 32]\n",
      "2022-11-10 14:21:38,116:INFO:--> use_l1: False\n",
      "2022-11-10 14:21:38,117:INFO:--> use_syc_bn: True\n",
      "2022-11-10 14:21:38,118:INFO:--> updates: 0.0\n",
      "2022-11-10 14:21:38,118:INFO:--> n_candidate_k: 10\n",
      "2022-11-10 14:21:38,119:INFO:--> lr: 0.01\n",
      "2022-11-10 14:21:38,120:INFO:--> min_lr_ratio: 0.001\n",
      "2022-11-10 14:21:38,120:INFO:--> warmup_epochs: 5\n",
      "2022-11-10 14:21:38,121:INFO:--> weight_decay: 0.0005\n",
      "2022-11-10 14:21:38,122:INFO:--> momentum: 0.9\n",
      "2022-11-10 14:21:38,122:INFO:--> no_aug_epochs: 5\n",
      "2022-11-10 14:21:38,123:INFO:--> log_interval: 30\n",
      "2022-11-10 14:21:38,124:INFO:--> ckpt_interval: 10\n",
      "2022-11-10 14:21:38,124:INFO:--> is_save_on_master: 1\n",
      "2022-11-10 14:21:38,125:INFO:--> ckpt_max_num: 60\n",
      "2022-11-10 14:21:38,126:INFO:--> opt: Momentum\n",
      "2022-11-10 14:21:38,126:INFO:--> is_distributed: 0\n",
      "2022-11-10 14:21:38,127:INFO:--> rank: 0\n",
      "2022-11-10 14:21:38,128:INFO:--> group_size: 1\n",
      "2022-11-10 14:21:38,128:INFO:--> bind_cpu: True\n",
      "2022-11-10 14:21:38,129:INFO:--> device_num: 1\n",
      "2022-11-10 14:21:38,130:INFO:--> is_modelArts: 0\n",
      "2022-11-10 14:21:38,130:INFO:--> enable_modelarts: False\n",
      "2022-11-10 14:21:38,131:INFO:--> need_modelarts_dataset_unzip: False\n",
      "2022-11-10 14:21:38,132:INFO:--> modelarts_dataset_unzip_name: coco2017\n",
      "2022-11-10 14:21:38,132:INFO:--> data_url: \n",
      "2022-11-10 14:21:38,133:INFO:--> train_url: \n",
      "2022-11-10 14:21:38,134:INFO:--> checkpoint_url: \n",
      "2022-11-10 14:21:38,134:INFO:--> data_path: \n",
      "2022-11-10 14:21:38,135:INFO:--> output_path: ./\n",
      "2022-11-10 14:21:38,136:INFO:--> load_path: \n",
      "2022-11-10 14:21:38,136:INFO:--> ckpt_path: ./save_weights\n",
      "2022-11-10 14:21:38,137:INFO:--> log_path: ./eval_logs\n",
      "2022-11-10 14:21:38,138:INFO:--> val_ckpt: \n",
      "2022-11-10 14:21:38,138:INFO:--> conf_thre: 0.001\n",
      "2022-11-10 14:21:38,139:INFO:--> nms_thre: 0.65\n",
      "2022-11-10 14:21:38,140:INFO:--> eval_interval: 10\n",
      "2022-11-10 14:21:38,140:INFO:--> run_eval: False\n",
      "2022-11-10 14:21:38,141:INFO:--> pred_ckpt: \n",
      "2022-11-10 14:21:38,141:INFO:--> pred_conf_thre: 0.01\n",
      "2022-11-10 14:21:38,142:INFO:--> pred_nms_thre: 0.5\n",
      "2022-11-10 14:21:38,143:INFO:--> classes_path: test_coco/classes.txt\n",
      "2022-11-10 14:21:38,143:INFO:--> pred_input: test_coco/val2017\n",
      "2022-11-10 14:21:38,144:INFO:--> pred_output: ./pred_output\n",
      "2022-11-10 14:21:38,145:INFO:--> is_modelart: False\n",
      "2022-11-10 14:21:38,145:INFO:--> result_path: \n",
      "2022-11-10 14:21:38,146:INFO:--> file_format: MINDIR\n",
      "2022-11-10 14:21:38,147:INFO:--> export_bs: 1\n",
      "2022-11-10 14:21:38,148:INFO:--> data_root: test_coco/train2017\n",
      "2022-11-10 14:21:38,148:INFO:--> annFile: test_coco/annotations/instances_train2017.json\n",
      "2022-11-10 14:21:38,149:INFO:--> logger: <LOGGER yolox (NOTSET)>\n",
      "2022-11-10 14:21:38,150:INFO:\n",
      "2022-11-10 14:21:38,163:INFO:Args:\n",
      "2022-11-10 14:21:38,164:INFO:--> backbone: yolox_darknet53\n",
      "2022-11-10 14:21:38,164:INFO:--> data_aug: True\n",
      "2022-11-10 14:21:38,165:INFO:--> device_target: Ascend\n",
      "2022-11-10 14:21:38,166:INFO:--> outputs_dir: ./save_weights/2022-11-10_time_14_21_38\n",
      "2022-11-10 14:21:38,166:INFO:--> save_graphs: False\n",
      "2022-11-10 14:21:38,167:INFO:--> lr_scheduler: yolox_warm_cos_lr\n",
      "2022-11-10 14:21:38,168:INFO:--> max_epoch: 10\n",
      "2022-11-10 14:21:38,168:INFO:--> total_epoch: 15\n",
      "2022-11-10 14:21:38,169:INFO:--> data_dir: test_coco\n",
      "2022-11-10 14:21:38,170:INFO:--> yolox_no_aug_ckpt: \n",
      "2022-11-10 14:21:38,170:INFO:--> need_profiler: 0\n",
      "2022-11-10 14:21:38,171:INFO:--> pretrained: None\n",
      "2022-11-10 14:21:38,172:INFO:--> resume_yolox: None\n",
      "2022-11-10 14:21:38,172:INFO:--> flip_prob: 0.5\n",
      "2022-11-10 14:21:38,173:INFO:--> hsv_prob: 1.0\n",
      "2022-11-10 14:21:38,174:INFO:--> per_batch_size: 2\n",
      "2022-11-10 14:21:38,174:INFO:--> depth_wise: False\n",
      "2022-11-10 14:21:38,175:INFO:--> max_gt: 120\n",
      "2022-11-10 14:21:38,176:INFO:--> num_classes: 3\n",
      "2022-11-10 14:21:38,176:INFO:--> input_size: [640, 640]\n",
      "2022-11-10 14:21:38,177:INFO:--> fpn_strides: [8, 16, 32]\n",
      "2022-11-10 14:21:38,178:INFO:--> use_l1: False\n",
      "2022-11-10 14:21:38,178:INFO:--> use_syc_bn: False\n",
      "2022-11-10 14:21:38,179:INFO:--> updates: 0.0\n",
      "2022-11-10 14:21:38,180:INFO:--> n_candidate_k: 10\n",
      "2022-11-10 14:21:38,180:INFO:--> lr: 0.01\n",
      "2022-11-10 14:21:38,181:INFO:--> min_lr_ratio: 0.001\n",
      "2022-11-10 14:21:38,182:INFO:--> warmup_epochs: 5\n",
      "2022-11-10 14:21:38,182:INFO:--> weight_decay: 0.0005\n",
      "2022-11-10 14:21:38,183:INFO:--> momentum: 0.9\n",
      "2022-11-10 14:21:38,184:INFO:--> no_aug_epochs: 5\n",
      "2022-11-10 14:21:38,184:INFO:--> log_interval: 30\n",
      "2022-11-10 14:21:38,185:INFO:--> ckpt_interval: 10\n",
      "2022-11-10 14:21:38,186:INFO:--> is_save_on_master: 1\n",
      "2022-11-10 14:21:38,187:INFO:--> ckpt_max_num: 60\n",
      "2022-11-10 14:21:38,187:INFO:--> opt: Momentum\n",
      "2022-11-10 14:21:38,188:INFO:--> is_distributed: 0\n",
      "2022-11-10 14:21:38,189:INFO:--> rank: 0\n",
      "2022-11-10 14:21:38,189:INFO:--> group_size: 1\n",
      "2022-11-10 14:21:38,190:INFO:--> bind_cpu: True\n",
      "2022-11-10 14:21:38,191:INFO:--> device_num: 1\n",
      "2022-11-10 14:21:38,191:INFO:--> is_modelArts: 0\n",
      "2022-11-10 14:21:38,192:INFO:--> enable_modelarts: False\n",
      "2022-11-10 14:21:38,193:INFO:--> need_modelarts_dataset_unzip: False\n",
      "2022-11-10 14:21:38,193:INFO:--> modelarts_dataset_unzip_name: coco2017\n",
      "2022-11-10 14:21:38,194:INFO:--> data_url: \n",
      "2022-11-10 14:21:38,195:INFO:--> train_url: \n",
      "2022-11-10 14:21:38,195:INFO:--> checkpoint_url: \n",
      "2022-11-10 14:21:38,196:INFO:--> data_path: \n",
      "2022-11-10 14:21:38,197:INFO:--> output_path: ./\n",
      "2022-11-10 14:21:38,197:INFO:--> load_path: \n",
      "2022-11-10 14:21:38,198:INFO:--> ckpt_path: ./save_weights\n",
      "2022-11-10 14:21:38,199:INFO:--> log_path: ./eval_logs\n",
      "2022-11-10 14:21:38,199:INFO:--> val_ckpt: \n",
      "2022-11-10 14:21:38,200:INFO:--> conf_thre: 0.001\n",
      "2022-11-10 14:21:38,201:INFO:--> nms_thre: 0.65\n",
      "2022-11-10 14:21:38,201:INFO:--> eval_interval: 10\n",
      "2022-11-10 14:21:38,202:INFO:--> run_eval: False\n",
      "2022-11-10 14:21:38,203:INFO:--> pred_ckpt: \n",
      "2022-11-10 14:21:38,203:INFO:--> pred_conf_thre: 0.01\n",
      "2022-11-10 14:21:38,204:INFO:--> pred_nms_thre: 0.5\n",
      "2022-11-10 14:21:38,205:INFO:--> classes_path: test_coco/classes.txt\n",
      "2022-11-10 14:21:38,205:INFO:--> pred_input: test_coco/val2017\n",
      "2022-11-10 14:21:38,206:INFO:--> pred_output: ./pred_output\n",
      "2022-11-10 14:21:38,207:INFO:--> is_modelart: False\n",
      "2022-11-10 14:21:38,207:INFO:--> result_path: \n",
      "2022-11-10 14:21:38,208:INFO:--> file_format: MINDIR\n",
      "2022-11-10 14:21:38,209:INFO:--> export_bs: 1\n",
      "2022-11-10 14:21:38,209:INFO:--> data_root: test_coco/train2017\n",
      "2022-11-10 14:21:38,210:INFO:--> annFile: test_coco/annotations/instances_train2017.json\n",
      "2022-11-10 14:21:38,211:INFO:--> logger: <LOGGER yolox (NOTSET)>\n",
      "2022-11-10 14:21:38,211:INFO:--> rank_save_ckpt_flag: 1\n",
      "2022-11-10 14:21:38,212:INFO:\n",
      "2022-11-10 14:21:43,415:INFO:Training backbone is: yolox_darknet53\n",
      "2022-11-10 14:21:45,718:INFO:Network weights have been initialized...\n",
      "2022-11-10 14:21:45,731:INFO:Finish getting network...\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "2022-11-10 14:21:45,747:INFO:Finish loading the val dataset!\n",
      "2022-11-10 14:21:45,747:INFO:Finish loading training dataset! batch size:2\n",
      "2022-11-10 14:21:47,338:INFO:5 steps for one epoch.\n",
      "2022-11-10 14:21:47,339:INFO:Learning rate scheduler:yolox_warm_cos_lr, base_lr:0.01, min lr ratio:0.001\n",
      "2022-11-10 14:21:47,732:INFO:Use Momentum Optimizer\n",
      "2022-11-10 14:21:48,067:INFO:Add ema model\n",
      "2022-11-10 14:21:48,070:INFO:Epoch number:10\n",
      "2022-11-10 14:21:48,070:INFO:All steps number:50\n",
      "2022-11-10 14:21:48,071:INFO:==================Start Training stage_1=========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(13817:281473179576192,_GeneratorWorkerMp-16):2022-11-10-14:21:50.537.649 [mindspore/dataset/engine/queue.py:125] Using shared memory queue, but rowsize is larger than allocated memory max_rowsize 6291456 current rowsize 6931200\n",
      "[WARNING] ME(13814:281473179576192,_GeneratorWorkerMp-13):2022-11-10-14:21:50.517.937 [mindspore/dataset/engine/queue.py:125] Using shared memory queue, but rowsize is larger than allocated memory max_rowsize 6291456 current rowsize 6931200\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:22:50.134.654 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-YOLOLossCell/TopK-op7823] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:22:50.178.079 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-YOLOLossCell/TopK-op7828] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:22:50.428.946 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op7856] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:22:50.436.058 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op7857] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:22:50.816.860 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op7860] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:22:50.818.526 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op7861] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:22:50.823.701 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op7862] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:22:50.825.302 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op7863] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:22:50.835.196 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op7865] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:22:50.837.923 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op7866] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:22:50.839.592 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op7867] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:22:50.842.928 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op7868] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:22:50.848.815 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op7870] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:22:50.881.458 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op7884] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:22:50.888.338 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op7885] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:22:50.895.140 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op7886] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:22:50.901.832 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op7887] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:22:50.907.584 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op7888] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:22:50.913.320 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op7889] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:22:50.919.210 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op7890] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:22:50.920.925 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op7891] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:22:50.922.649 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op7892] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:24:30.379.822 [mindspore/ccsrc/plugin/device/ascend/hal/device/ascend_stream_assign.cc:1944] InsertEventForCallCommSubGraph] Cannot find comm group for sub comm graph label id 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-10 14:24:35,334:INFO:epoch: 1 epoch time 165.28s loss: 24.1893, overflow: False, scale: 262144\n",
      "2022-11-10 14:24:35,983:INFO:epoch: 2 epoch time 0.64s loss: 18.0759, overflow: False, scale: 262144\n",
      "2022-11-10 14:24:36,622:INFO:epoch: 3 epoch time 0.63s loss: 14.9537, overflow: True, scale: 131072\n",
      "2022-11-10 14:24:37,296:INFO:epoch: 4 epoch time 0.67s loss: 10.8070, overflow: False, scale: 131072\n",
      "2022-11-10 14:24:38,041:INFO:epoch: 5 epoch time 0.74s loss: 11.5410, overflow: False, scale: 131072\n",
      "2022-11-10 14:24:38,889:INFO:epoch: 6 epoch time 0.84s loss: 11.0122, overflow: False, scale: 131072\n",
      "2022-11-10 14:24:39,801:INFO:epoch: 7 epoch time 0.90s loss: 11.9497, overflow: False, scale: 65536\n",
      "2022-11-10 14:24:40,714:INFO:epoch: 8 epoch time 0.91s loss: 11.1984, overflow: False, scale: 65536\n",
      "2022-11-10 14:24:41,639:INFO:epoch: 9 epoch time 0.92s loss: 10.3653, overflow: False, scale: 65536\n",
      "2022-11-10 14:24:46,788:INFO:epoch: 10 epoch time 5.14s loss: 22.4534, overflow: False, scale: 65536\n",
      "2022-11-10 14:24:46,791:INFO:==================Training END stage_1======================\n",
      "2022-11-10 14:24:53,847:INFO:Training backbone is: yolox_darknet53\n",
      "2022-11-10 14:24:56,335:INFO:Network weights have been initialized...\n",
      "2022-11-10 14:24:56,378:INFO:Finish getting network...\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "2022-11-10 14:24:56,396:INFO:Finish loading the val dataset!\n",
      "2022-11-10 14:24:56,397:INFO:Finish loading training dataset! batch size:2\n",
      "2022-11-10 14:24:58,521:INFO:5 steps for one epoch.\n",
      "2022-11-10 14:24:58,524:INFO:Learning rate scheduler:no_aug_lr, base_lr:0.01, min lr ratio:0.001\n",
      "2022-11-10 14:24:59,092:INFO:Use Momentum Optimizer\n",
      "2022-11-10 14:25:01,790:INFO:network_ema updates:50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.461.148 [mindspore/train/serialization.py:712] For 'load_param_into_net', 450 parameters in the 'net' are not loaded, because they are not in the 'parameter_dict', please check whether the network structure is consistent when training and loading checkpoint.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.463.243 [mindspore/train/serialization.py:714] scale_sense is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.464.488 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.stem.0.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.465.689 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.stem.0.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.466.910 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.stem.1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.468.087 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.stem.1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.469.249 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.stem.2.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.470.414 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.stem.2.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.471.555 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.stem.2.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.472.711 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.stem.2.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.473.910 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.0.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.475.096 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.0.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.476.236 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.1.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.477.393 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.1.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.478.601 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.1.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.479.752 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.1.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.480.977 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.2.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.482.144 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.2.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.483.336 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.2.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.484.486 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.2.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.485.667 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.0.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.486.818 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.0.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.487.964 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.1.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.489.120 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.1.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.490.290 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.1.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.491.446 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.1.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.492.596 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.2.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.493.757 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.2.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.494.948 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.2.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.496.132 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.2.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.497.300 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.3.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.498.206 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.3.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.499.015 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.3.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.500.435 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.3.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.501.598 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.4.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.502.766 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.4.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.503.923 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.4.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.505.064 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.4.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.506.243 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.5.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.507.389 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.5.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.508.522 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.5.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.509.823 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.5.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.510.628 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.6.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.511.525 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.6.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.512.924 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.6.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.514.103 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.6.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.515.257 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.7.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.516.412 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.7.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.517.554 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.7.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.518.751 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.7.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.519.913 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.8.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.521.044 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.8.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.522.200 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.8.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.523.301 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.8.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.524.527 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.0.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.525.689 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.0.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.526.858 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.1.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.528.003 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.1.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.529.188 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.1.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.530.394 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.1.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.531.578 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.2.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.532.727 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.2.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.533.892 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.2.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.535.060 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.2.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.536.191 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.3.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.537.333 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.3.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.538.516 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.3.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.539.654 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.3.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.540.788 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.4.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.541.718 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.4.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.542.885 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.4.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.544.017 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.4.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.545.145 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.5.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.546.333 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.5.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.547.497 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.5.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.548.615 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.5.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.549.757 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.6.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.550.911 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.6.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.552.093 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.6.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.553.219 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.6.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.554.400 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.7.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.555.537 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.7.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.556.669 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.7.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.557.837 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.7.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.558.970 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.8.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.560.100 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.8.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.561.237 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.8.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.562.418 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.8.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.563.560 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.0.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.564.693 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.0.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.565.859 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.1.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.567.002 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.1.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.568.133 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.1.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.569.257 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.1.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.570.458 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.2.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.571.598 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.2.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.572.762 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.2.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.573.910 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.2.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.575.089 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.3.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.576.225 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.3.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.577.376 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.3.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.578.557 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.3.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.579.706 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.4.layer1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.580.855 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.4.layer1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.582.009 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.4.layer2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.583.144 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.4.layer2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.584.277 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.5.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.585.409 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.5.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.586.598 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.6.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.587.731 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.6.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.588.868 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.7.conv1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.590.035 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.7.conv1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.591.187 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.7.conv2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.592.319 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.7.conv2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.593.487 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.8.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.594.649 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.8.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.595.775 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.9.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.596.926 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.9.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.598.090 [mindspore/train/serialization.py:714] ema.network.backbone.out1_cbl.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.599.251 [mindspore/train/serialization.py:714] ema.network.backbone.out1_cbl.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.600.388 [mindspore/train/serialization.py:714] ema.network.backbone.out1.0.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.601.532 [mindspore/train/serialization.py:714] ema.network.backbone.out1.0.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.602.686 [mindspore/train/serialization.py:714] ema.network.backbone.out1.1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.603.848 [mindspore/train/serialization.py:714] ema.network.backbone.out1.1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.604.982 [mindspore/train/serialization.py:714] ema.network.backbone.out1.2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.606.142 [mindspore/train/serialization.py:714] ema.network.backbone.out1.2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.607.304 [mindspore/train/serialization.py:714] ema.network.backbone.out1.3.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.608.455 [mindspore/train/serialization.py:714] ema.network.backbone.out1.3.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.609.654 [mindspore/train/serialization.py:714] ema.network.backbone.out1.4.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.610.805 [mindspore/train/serialization.py:714] ema.network.backbone.out1.4.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.611.931 [mindspore/train/serialization.py:714] ema.network.backbone.out2_cbl.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.613.073 [mindspore/train/serialization.py:714] ema.network.backbone.out2_cbl.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.614.281 [mindspore/train/serialization.py:714] ema.network.backbone.out2.0.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.615.417 [mindspore/train/serialization.py:714] ema.network.backbone.out2.0.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.616.601 [mindspore/train/serialization.py:714] ema.network.backbone.out2.1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.617.780 [mindspore/train/serialization.py:714] ema.network.backbone.out2.1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.618.961 [mindspore/train/serialization.py:714] ema.network.backbone.out2.2.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.620.121 [mindspore/train/serialization.py:714] ema.network.backbone.out2.2.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.621.273 [mindspore/train/serialization.py:714] ema.network.backbone.out2.3.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.622.432 [mindspore/train/serialization.py:714] ema.network.backbone.out2.3.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.623.588 [mindspore/train/serialization.py:714] ema.network.backbone.out2.4.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.624.723 [mindspore/train/serialization.py:714] ema.network.backbone.out2.4.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.625.884 [mindspore/train/serialization.py:714] ema.network.head_l.stem.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.627.078 [mindspore/train/serialization.py:714] ema.network.head_l.stem.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.628.205 [mindspore/train/serialization.py:714] ema.network.head_l.cls_convs.0.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.629.364 [mindspore/train/serialization.py:714] ema.network.head_l.cls_convs.0.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.630.746 [mindspore/train/serialization.py:714] ema.network.head_l.cls_convs.1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.632.113 [mindspore/train/serialization.py:714] ema.network.head_l.cls_convs.1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.633.023 [mindspore/train/serialization.py:714] ema.network.head_l.reg_convs.0.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.633.855 [mindspore/train/serialization.py:714] ema.network.head_l.reg_convs.0.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.634.693 [mindspore/train/serialization.py:714] ema.network.head_l.reg_convs.1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.635.599 [mindspore/train/serialization.py:714] ema.network.head_l.reg_convs.1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.637.572 [mindspore/train/serialization.py:714] ema.network.head_l.cls_preds.bias is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.638.954 [mindspore/train/serialization.py:714] ema.network.head_l.reg_preds.bias is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.640.140 [mindspore/train/serialization.py:714] ema.network.head_l.obj_preds.bias is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.641.262 [mindspore/train/serialization.py:714] ema.network.head_m.stem.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.642.442 [mindspore/train/serialization.py:714] ema.network.head_m.stem.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.643.623 [mindspore/train/serialization.py:714] ema.network.head_m.cls_convs.0.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.644.758 [mindspore/train/serialization.py:714] ema.network.head_m.cls_convs.0.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.645.931 [mindspore/train/serialization.py:714] ema.network.head_m.cls_convs.1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.647.086 [mindspore/train/serialization.py:714] ema.network.head_m.cls_convs.1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.648.193 [mindspore/train/serialization.py:714] ema.network.head_m.reg_convs.0.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.649.331 [mindspore/train/serialization.py:714] ema.network.head_m.reg_convs.0.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.650.490 [mindspore/train/serialization.py:714] ema.network.head_m.reg_convs.1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.651.639 [mindspore/train/serialization.py:714] ema.network.head_m.reg_convs.1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.652.808 [mindspore/train/serialization.py:714] ema.network.head_m.cls_preds.bias is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.653.969 [mindspore/train/serialization.py:714] ema.network.head_m.reg_preds.bias is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.655.118 [mindspore/train/serialization.py:714] ema.network.head_m.obj_preds.bias is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.656.265 [mindspore/train/serialization.py:714] ema.network.head_s.stem.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.657.457 [mindspore/train/serialization.py:714] ema.network.head_s.stem.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.658.607 [mindspore/train/serialization.py:714] ema.network.head_s.cls_convs.0.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.659.713 [mindspore/train/serialization.py:714] ema.network.head_s.cls_convs.0.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.660.861 [mindspore/train/serialization.py:714] ema.network.head_s.cls_convs.1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.662.060 [mindspore/train/serialization.py:714] ema.network.head_s.cls_convs.1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.663.236 [mindspore/train/serialization.py:714] ema.network.head_s.reg_convs.0.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.664.393 [mindspore/train/serialization.py:714] ema.network.head_s.reg_convs.0.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.665.560 [mindspore/train/serialization.py:714] ema.network.head_s.reg_convs.1.bn.gamma is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.666.733 [mindspore/train/serialization.py:714] ema.network.head_s.reg_convs.1.bn.beta is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.668.013 [mindspore/train/serialization.py:714] ema.network.head_s.cls_preds.bias is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.669.176 [mindspore/train/serialization.py:714] ema.network.head_s.reg_preds.bias is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.670.366 [mindspore/train/serialization.py:714] ema.network.head_s.obj_preds.bias is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.671.308 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.stem.0.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.672.049 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.stem.1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.673.479 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.stem.2.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.674.666 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.stem.2.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.675.843 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.0.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.676.996 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.1.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.678.171 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.1.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.679.328 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.2.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.680.542 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.2.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.681.696 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.0.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.682.878 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.1.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.684.063 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.1.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.685.220 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.2.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.686.477 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.2.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.687.665 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.3.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.688.826 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.3.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.690.002 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.4.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.691.191 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.4.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.692.413 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.5.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.693.562 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.5.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.694.786 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.6.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.695.962 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.6.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.697.133 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.7.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.698.364 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.7.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.699.500 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.8.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.700.657 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.8.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.701.452 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.0.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.702.192 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.1.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.703.682 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.1.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.704.832 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.2.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.706.015 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.2.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.707.210 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.3.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.708.189 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.3.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.708.929 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.4.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.709.812 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.4.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.711.389 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.5.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.712.454 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.5.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.713.259 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.6.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.714.548 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.6.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.715.710 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.7.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.716.912 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.7.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.718.098 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.8.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.719.306 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.8.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.720.497 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.0.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.721.693 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.1.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.722.906 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.1.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.724.133 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.2.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.725.341 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.2.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.726.574 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.3.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.727.788 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.3.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.728.982 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.4.layer1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.730.183 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.4.layer2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.731.390 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.5.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.732.542 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.6.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.733.741 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.7.conv1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.734.947 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.7.conv2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.736.149 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.8.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.737.335 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.9.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.738.528 [mindspore/train/serialization.py:714] ema.network.backbone.out1_cbl.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.739.774 [mindspore/train/serialization.py:714] ema.network.backbone.out1.0.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.740.882 [mindspore/train/serialization.py:714] ema.network.backbone.out1.1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.742.114 [mindspore/train/serialization.py:714] ema.network.backbone.out1.2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.743.290 [mindspore/train/serialization.py:714] ema.network.backbone.out1.3.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.744.493 [mindspore/train/serialization.py:714] ema.network.backbone.out1.4.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.745.730 [mindspore/train/serialization.py:714] ema.network.backbone.out2_cbl.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.746.925 [mindspore/train/serialization.py:714] ema.network.backbone.out2.0.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.748.087 [mindspore/train/serialization.py:714] ema.network.backbone.out2.1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.749.267 [mindspore/train/serialization.py:714] ema.network.backbone.out2.2.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.750.483 [mindspore/train/serialization.py:714] ema.network.backbone.out2.3.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.751.642 [mindspore/train/serialization.py:714] ema.network.backbone.out2.4.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.752.828 [mindspore/train/serialization.py:714] ema.network.head_l.stem.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.753.996 [mindspore/train/serialization.py:714] ema.network.head_l.cls_convs.0.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.755.195 [mindspore/train/serialization.py:714] ema.network.head_l.cls_convs.1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.756.402 [mindspore/train/serialization.py:714] ema.network.head_l.reg_convs.0.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.757.571 [mindspore/train/serialization.py:714] ema.network.head_l.reg_convs.1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.758.754 [mindspore/train/serialization.py:714] ema.network.head_l.cls_preds.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.759.677 [mindspore/train/serialization.py:714] ema.network.head_l.reg_preds.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.760.495 [mindspore/train/serialization.py:714] ema.network.head_l.obj_preds.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.761.288 [mindspore/train/serialization.py:714] ema.network.head_m.stem.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.762.079 [mindspore/train/serialization.py:714] ema.network.head_m.cls_convs.0.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.762.882 [mindspore/train/serialization.py:714] ema.network.head_m.cls_convs.1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.764.723 [mindspore/train/serialization.py:714] ema.network.head_m.reg_convs.0.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.765.921 [mindspore/train/serialization.py:714] ema.network.head_m.reg_convs.1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.766.861 [mindspore/train/serialization.py:714] ema.network.head_m.cls_preds.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.768.054 [mindspore/train/serialization.py:714] ema.network.head_m.reg_preds.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.769.219 [mindspore/train/serialization.py:714] ema.network.head_m.obj_preds.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.770.404 [mindspore/train/serialization.py:714] ema.network.head_s.stem.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.771.556 [mindspore/train/serialization.py:714] ema.network.head_s.cls_convs.0.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.772.773 [mindspore/train/serialization.py:714] ema.network.head_s.cls_convs.1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.773.956 [mindspore/train/serialization.py:714] ema.network.head_s.reg_convs.0.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.775.128 [mindspore/train/serialization.py:714] ema.network.head_s.reg_convs.1.conv.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.776.308 [mindspore/train/serialization.py:714] ema.network.head_s.cls_preds.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.777.491 [mindspore/train/serialization.py:714] ema.network.head_s.reg_preds.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.778.772 [mindspore/train/serialization.py:714] ema.network.head_s.obj_preds.weight is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.779.837 [mindspore/train/serialization.py:714] updates is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.780.962 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.stem.0.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.782.121 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.stem.0.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.783.244 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.stem.1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.784.402 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.stem.1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.785.495 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.stem.2.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.786.667 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.stem.2.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.787.796 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.stem.2.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.788.959 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.stem.2.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.790.129 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.0.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.791.271 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.0.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.792.393 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.1.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.793.544 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.1.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.794.707 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.1.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.795.871 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.1.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.797.021 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.2.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.798.255 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.2.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.799.376 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.2.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.800.472 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark2.2.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.801.632 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.0.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.802.808 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.0.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.803.946 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.1.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.805.095 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.1.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.806.222 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.1.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.807.439 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.1.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.808.653 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.2.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.809.759 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.2.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.810.910 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.2.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.812.045 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.2.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.813.215 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.3.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.814.353 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.3.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.815.349 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.3.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.816.498 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.3.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.817.670 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.4.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.818.835 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.4.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.820.000 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.4.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.821.164 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.4.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.822.373 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.5.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.823.557 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.5.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.824.735 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.5.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.825.860 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.5.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.827.060 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.6.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.828.231 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.6.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.829.378 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.6.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.830.589 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.6.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.831.765 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.7.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.832.922 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.7.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.834.099 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.7.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.835.226 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.7.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.836.413 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.8.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.837.556 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.8.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.838.735 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.8.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.839.954 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark3.8.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.841.154 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.0.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.842.341 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.0.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.843.535 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.1.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.844.723 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.1.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.845.980 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.1.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.847.166 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.1.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.848.318 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.2.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.849.422 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.2.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.850.603 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.2.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.851.717 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.2.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.852.840 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.3.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.854.027 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.3.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.855.164 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.3.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.856.310 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.3.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.857.525 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.4.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.858.685 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.4.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.859.856 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.4.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.860.947 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.4.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.862.115 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.5.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.863.267 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.5.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.864.285 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.5.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.864.941 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.5.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.865.800 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.6.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.866.632 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.6.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.867.471 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.6.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.869.311 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.6.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.870.580 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.7.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.871.829 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.7.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.872.988 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.7.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.874.190 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.7.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.875.379 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.8.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.876.538 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.8.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.877.748 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.8.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.878.918 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark4.8.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.880.125 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.0.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.881.305 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.0.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.882.481 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.1.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.883.657 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.1.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.884.837 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.1.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.886.046 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.1.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.887.231 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.2.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.888.392 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.2.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.889.565 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.2.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.890.749 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.2.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.891.915 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.3.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.893.085 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.3.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.894.335 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.3.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.895.515 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.3.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.896.689 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.4.layer1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.897.875 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.4.layer1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.899.036 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.4.layer2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.900.180 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.4.layer2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.901.357 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.5.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.902.544 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.5.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.903.623 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.6.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.904.774 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.6.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.906.013 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.7.conv1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.907.171 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.7.conv1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.908.347 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.7.conv2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.909.489 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.7.conv2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.910.701 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.8.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.911.883 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.8.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.913.049 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.9.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.914.231 [mindspore/train/serialization.py:714] ema.network.backbone.backbone.dark5.9.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.915.427 [mindspore/train/serialization.py:714] ema.network.backbone.out1_cbl.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.916.595 [mindspore/train/serialization.py:714] ema.network.backbone.out1_cbl.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.917.845 [mindspore/train/serialization.py:714] ema.network.backbone.out1.0.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.919.005 [mindspore/train/serialization.py:714] ema.network.backbone.out1.0.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.920.178 [mindspore/train/serialization.py:714] ema.network.backbone.out1.1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.921.361 [mindspore/train/serialization.py:714] ema.network.backbone.out1.1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.922.544 [mindspore/train/serialization.py:714] ema.network.backbone.out1.2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.923.696 [mindspore/train/serialization.py:714] ema.network.backbone.out1.2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.924.868 [mindspore/train/serialization.py:714] ema.network.backbone.out1.3.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.926.038 [mindspore/train/serialization.py:714] ema.network.backbone.out1.3.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.927.190 [mindspore/train/serialization.py:714] ema.network.backbone.out1.4.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.928.368 [mindspore/train/serialization.py:714] ema.network.backbone.out1.4.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.929.557 [mindspore/train/serialization.py:714] ema.network.backbone.out2_cbl.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.930.721 [mindspore/train/serialization.py:714] ema.network.backbone.out2_cbl.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.931.907 [mindspore/train/serialization.py:714] ema.network.backbone.out2.0.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.933.093 [mindspore/train/serialization.py:714] ema.network.backbone.out2.0.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.934.266 [mindspore/train/serialization.py:714] ema.network.backbone.out2.1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.935.412 [mindspore/train/serialization.py:714] ema.network.backbone.out2.1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.936.571 [mindspore/train/serialization.py:714] ema.network.backbone.out2.2.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.937.744 [mindspore/train/serialization.py:714] ema.network.backbone.out2.2.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.938.933 [mindspore/train/serialization.py:714] ema.network.backbone.out2.3.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.940.081 [mindspore/train/serialization.py:714] ema.network.backbone.out2.3.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.941.187 [mindspore/train/serialization.py:714] ema.network.backbone.out2.4.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.942.314 [mindspore/train/serialization.py:714] ema.network.backbone.out2.4.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.943.417 [mindspore/train/serialization.py:714] ema.network.head_l.stem.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.944.538 [mindspore/train/serialization.py:714] ema.network.head_l.stem.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.945.816 [mindspore/train/serialization.py:714] ema.network.head_l.cls_convs.0.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.946.964 [mindspore/train/serialization.py:714] ema.network.head_l.cls_convs.0.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.948.119 [mindspore/train/serialization.py:714] ema.network.head_l.cls_convs.1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.949.296 [mindspore/train/serialization.py:714] ema.network.head_l.cls_convs.1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.950.514 [mindspore/train/serialization.py:714] ema.network.head_l.reg_convs.0.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.951.763 [mindspore/train/serialization.py:714] ema.network.head_l.reg_convs.0.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.952.973 [mindspore/train/serialization.py:714] ema.network.head_l.reg_convs.1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.954.201 [mindspore/train/serialization.py:714] ema.network.head_l.reg_convs.1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.955.288 [mindspore/train/serialization.py:714] ema.network.head_m.stem.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.956.434 [mindspore/train/serialization.py:714] ema.network.head_m.stem.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.957.596 [mindspore/train/serialization.py:714] ema.network.head_m.cls_convs.0.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.958.830 [mindspore/train/serialization.py:714] ema.network.head_m.cls_convs.0.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.959.995 [mindspore/train/serialization.py:714] ema.network.head_m.cls_convs.1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.961.118 [mindspore/train/serialization.py:714] ema.network.head_m.cls_convs.1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.962.315 [mindspore/train/serialization.py:714] ema.network.head_m.reg_convs.0.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.963.427 [mindspore/train/serialization.py:714] ema.network.head_m.reg_convs.0.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.964.570 [mindspore/train/serialization.py:714] ema.network.head_m.reg_convs.1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.965.759 [mindspore/train/serialization.py:714] ema.network.head_m.reg_convs.1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.966.926 [mindspore/train/serialization.py:714] ema.network.head_s.stem.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.968.086 [mindspore/train/serialization.py:714] ema.network.head_s.stem.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.969.247 [mindspore/train/serialization.py:714] ema.network.head_s.cls_convs.0.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.970.488 [mindspore/train/serialization.py:714] ema.network.head_s.cls_convs.0.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.971.638 [mindspore/train/serialization.py:714] ema.network.head_s.cls_convs.1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.972.755 [mindspore/train/serialization.py:714] ema.network.head_s.cls_convs.1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.973.941 [mindspore/train/serialization.py:714] ema.network.head_s.reg_convs.0.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.975.152 [mindspore/train/serialization.py:714] ema.network.head_s.reg_convs.0.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.976.315 [mindspore/train/serialization.py:714] ema.network.head_s.reg_convs.1.bn.moving_mean is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.977.476 [mindspore/train/serialization.py:714] ema.network.head_s.reg_convs.1.bn.moving_variance is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.978.642 [mindspore/train/serialization.py:714] global_step is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.979.804 [mindspore/train/serialization.py:714] momentum is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.980.963 [mindspore/train/serialization.py:714] learning_rate is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.982.147 [mindspore/train/serialization.py:714] current_iterator_step is not loaded.\n",
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:25:02.983.310 [mindspore/train/serialization.py:714] last_overflow_iterator_step is not loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-10 14:25:03,012:INFO:successful loading ema weights\n",
      "2022-11-10 14:25:03,018:INFO:Finish load the resume checkpoint, begin to train the last...\n",
      "2022-11-10 14:25:03,020:INFO:Add ema model\n",
      "2022-11-10 14:25:03,023:INFO:Epoch number:5\n",
      "2022-11-10 14:25:03,024:INFO:All steps number:25\n",
      "2022-11-10 14:25:03,025:INFO:==================Start Training stage_2=========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(21896:281473179576192,_GeneratorWorkerMp-38):2022-11-10-14:25:08.813.026 [mindspore/dataset/engine/queue.py:125] Using shared memory queue, but rowsize is larger than allocated memory max_rowsize 6291456 current rowsize 6931200\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:58.616.938 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-YOLOLossCell/TopK-op20921] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:58.660.276 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-YOLOLossCell/TopK-op20926] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:58.889.328 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op20970] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:58.906.706 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op20974] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:58.922.008 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op20976] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:58.925.382 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op20977] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:58.928.749 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op20978] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.225.594 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op20979] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.227.505 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op20980] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.229.348 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op20981] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.245.822 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op20986] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.247.677 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op20987] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.249.511 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op20988] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.251.306 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op20989] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.253.048 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op20990] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.258.678 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op20991] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.260.469 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op20992] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.269.740 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op20994] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.272.715 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op20995] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.274.561 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op20996] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.278.311 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op20997] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.287.337 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op21000] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.301.334 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op21014] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.305.528 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op21015] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.309.714 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op21016] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.313.912 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op21017] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.317.043 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op21018] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.320.159 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op21019] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.323.248 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op21020] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.325.022 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op21021] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:25:59.326.798 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/network-YOLOLossCell/network-DetectionBlock/gradStridedSlice/StridedSliceGrad-op21022] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(11567,ffff94e1e780,python):2022-11-10-14:26:44.312.097 [mindspore/ccsrc/plugin/device/ascend/hal/device/ascend_stream_assign.cc:1944] InsertEventForCallCommSubGraph] Cannot find comm group for sub comm graph label id 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-10 14:26:51,196:INFO:epoch: 1 epoch time 102.87s loss: 16.3883, overflow: True, scale: 131072\n",
      "2022-11-10 14:26:51,981:INFO:epoch: 2 epoch time 0.78s loss: 14.2845, overflow: False, scale: 32768\n",
      "2022-11-10 14:26:52,773:INFO:epoch: 3 epoch time 0.79s loss: 241.6574, overflow: False, scale: 32768\n",
      "2022-11-10 14:26:53,613:INFO:epoch: 4 epoch time 0.83s loss: 16.4360, overflow: False, scale: 32768\n",
      "2022-11-10 14:26:54,407:INFO:epoch: 5 epoch time 0.79s loss: 14.1322, overflow: False, scale: 32768\n",
      "2022-11-10 14:26:58,514:INFO:==================Training END stage_2======================\n",
      "2022-11-10 14:27:01,909:INFO:Begin Creating Network....\n",
      "2022-11-10 14:27:06,822:INFO:./save_weights/2022-11-10_time_14_21_38/ckpt_0/stage_2/stage_2_final.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(11567:281473179576192,MainProcess):2022-11-10-14:27:09.582.839 [mindspore/train/serialization.py:734] For 'load_param_into_net', remove parameter prefix name: network., continue to load.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-10 14:27:09,663:INFO:load model ./save_weights/2022-11-10_time_14_21_38/ckpt_0/stage_2/stage_2_final.ckpt success\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "2022-11-10 14:27:12,777:INFO:Finish loading the dataset, totally 10 images to eval, iters 5\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "2022-11-10 14:27:12,793:INFO:Start inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[32m██████████\u001b[0m| 5/5 [00:21<00:00,  4.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-10 14:27:40,798:INFO:Calculating mAP...\n",
      "Evaluate in main process...\n",
      "2022-11-10 14:27:40,862:INFO:result file path: ./eval_logs/2022-11-10_time_14_27_01/predict_2022_11_10_14_27_40.json\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.03s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      "2022-11-10 14:27:40,947:INFO:\n",
      "=============coco eval result=========\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.002\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.039\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.039\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#------------------------#\n",
    "# process train\n",
    "#------------------------#\n",
    "def run():\n",
    "    set_default()\n",
    "    profiler = network_init(config)\n",
    "    parallel_init(config)\n",
    "    config.data_aug = True\n",
    "    run_train('stage_1', profiler)\n",
    "    config.data_aug = False\n",
    "    run_train('stage_2', profiler)\n",
    "    \n",
    "    run_eval()\n",
    "\n",
    "    #run_pred()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "54f77735b2c11183a1887b438d203b99bd94157dba0a765801d5b399d06610b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
